{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f38f6c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41f17b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = DocumentConverter()\n",
    "result = converter.convert(\"https://blazorise.com/docs\")\n",
    "# print(result.document.export_to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bd23579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<urlset xmlns:x=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/blazorise-license-changes</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/blazorise-commercial-going-live</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/094-1</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/094-2</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/094-3</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/094-4</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/094-5</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/094-7</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/094-8</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/095</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/094-9</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/095-1</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/095-2</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/095-3</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/announcing-2022-blazorise-plans-and-pricing-updates</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/095-4</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/095-5</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/095-6</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/100</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/095-7</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/102</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/103</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/104</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/101</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/105</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/106</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/110</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/111</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/107</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/112</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/113</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/114</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/115</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/116</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/120</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/117</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/121</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/122</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/123</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/2023-changes-to-blazorise-plans-and-licensing</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/124</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/125</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/130</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/131</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/132</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/133</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/140</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/134</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/141</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/142</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/143</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/150</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/151</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/152</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/153</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/160</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/161</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/162</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/170</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/171</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/172</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/173</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/174</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news/release-notes/175</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/news</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/publishing-blazor-app-azure-container-app-github-container-registry</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/blazorise-2-plans-and-vision-for-the-future</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/blazor-and-tailwind-quick-setup-without-npm</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/how-to-create-social-media-share-buttons</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/how-to-implement-validation-with-captcha</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/how-to-override-fluent-design-tokens</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/how-to-handle-localization-in-blazorise-validation</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/how-to-enhance-the-new-datagrid-menu-filter</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/the-importance-of-javascript-in-blazorise</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/what-is-blazorise-all-about</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/2022-blazorise-goals</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/introducing-the-blazorise-mvc-award-program</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/create-a-blazor-application</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/async-await-what-happens-under-the-hood</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/practical-css-tips-for-developers</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/what-to-expect-in-net7</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/reactive-ui-blazorise-fluent-validation</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/create-a-tabbed-login-form-with-blazorise-components</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/a-beginners-guide-to-maui</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/what-is-blazor-wasm</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/exploring-webassembly-the-underlying-technology-behind-blazor-wasm</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/how-to-handle-select-with-primitive-and-complex-types</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/blazor-form-validation-with-data-annotations</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/blog/how-to-create-a-blazorise-application-beginners-guide</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/book-a-demo</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/contact</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/help-us-improve</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/about</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/affiliate-terms</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/credits</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/privacy</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/resellers</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/terms</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/faq</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/pwa</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/start</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/templates</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/testing</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/usage</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/specifications/autocomplete</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/specifications</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/utilities</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/utilities/position</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/utilities/object-fit</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/utilities/object-fit/responsive-example-iframe</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/utilities/grid</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/grid</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/utilities/css-grid</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/sizes</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/localization</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/bar</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/button</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/chart</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/common</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/datagrid</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/date</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/divider</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/dropdown</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/heading</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/icon</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/listgroup</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/snackbar</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/spinkit</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/table</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/tabs</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/text</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/tooltip</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/enums/validation</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/helpers/colors</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/usage/tailwind</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/usage/material</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/usage/licensing</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/usage/licensing/register-product-token</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/usage/fluent2</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/usage/bulma</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/usage/bootstrap4</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/usage/bootstrap5</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/usage/ant-design</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/validation</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/typography</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/tooltip</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/toast</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/time</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/time-picker</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/theming/api/colors</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/theming/api/options</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/theming/api</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/theming/api/palettes</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/theming</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/text</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/tab</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/table</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/table/mobile-mode-iframe</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/switch</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/step</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/slider</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/skeleton/api</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/skeleton</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/select</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/repeater</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/rating</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/radio</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/progress</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/pagination</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/offcanvas</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/numeric</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/numeric-picker</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/modal</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/memo</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/list-group</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/link</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/layout</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/jumbotron</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/input-mask</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/image</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/highlighter</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/focus-trap</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/file</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/file-picker</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/figure</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/field</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/dropdown</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/dragdrop</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/divider</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/date</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/date-picker</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/color</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/color-picker</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/close-button</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/check</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/carousel</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/card</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/button</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/breadcrumb</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/bar</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/badge</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/alert</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/addon</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/components/accordion</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/services</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/services/toast-provider</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/services/notification</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/services/notification-provider</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/services/modal-provider</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/services/message</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/services/message-provider</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/services/page-progress</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/services/page-progress-provider</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/video/api</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/video</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/treeview</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/transferlist</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/splitter</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/spinkit</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/snackbar</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/signaturepad</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/sidebar</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/selectlist</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/routertabs</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/routertabs2</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/routertabs3</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/richtextedit/api</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/richtextedit</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/qrcode</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/pdfviewer/api</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/pdfviewer</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/markdown</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/lottie-animation</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/loadingindicator</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/list-view</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/icons-available</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/icons</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/dropdownlist</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/aggregates</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/columns</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/api</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/getting-started</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/validations</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/templates/button-row</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/templates/commands</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/templates/detail-row</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/templates/display</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/templates/edit</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/templates</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/templates/loading</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/templates/row-overlay</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/selection/cell</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/selection/custom-row-colors</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/selection</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/selection/multiple</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/selection/single</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/auto-generate-columns</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/column-chooser</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/context-menu</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/editing</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/filtering</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/fixed-columns</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/fixed-header</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/grouping</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/header-group</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/mobile-mode-iframe</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/mobile-mode</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/paging</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/resizing</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/sorting</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/features/state-management</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/binding-data/dynamic</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/binding-data</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/binding-data/in-memory</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/binding-data/large-data</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/binding-data/observable</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/datagrid/binding-data/virtualize</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/cropper</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/chart</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/chart-trendline</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/chart-zoom</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/chart-live</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/chart-annotation</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/chart-datalabels</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/captcha</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/autocomplete</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/animate</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/docs/extensions/fluent-validation</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/pricing</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/purchase-order</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/purchase-order/{ProductName}</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/affiliates</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/commercial</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/community</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/customers</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/enterprise-plus</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/features</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/license</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/themes</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "  <url>\n",
      "    <loc>https://blazorise.com/themes/blazestack</loc>\n",
      "    <lastmod>2025-04-11T11:32:08Z</lastmod>\n",
      "  </url>\n",
      "</urlset>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "xml = requests.get(\"https://blazorise.com/sitemap.xml\").text\n",
    "print(xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f504a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('urlset', {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "root = ET.fromstring(xml)\n",
    "root.tag, root.attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d1df08",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = [url.find('loc').text for url in root.findall('url')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52eb2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = \"/home/babos/chatbot/app/docling_docs\"\n",
    "for i, url in enumerate(url_list):\n",
    "    if 'news' not in url and 'blog' not in url:\n",
    "        try:\n",
    "            doc = converter.convert(url)\n",
    "            doc.document.save_as_markdown(f\"{path_to_save}/{i+1}_{url.split('/')[-1]}.md\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b50dc9",
   "metadata": {},
   "source": [
    "# Database - SqlAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2106324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker, AsyncSession, AsyncAttrs\n",
    "\n",
    "# Example connection string\n",
    "DATABASE_URL=\"postgresql+asyncpg://rag_user:rag_pass@localhost:5432/rag_db\"\n",
    "\n",
    "engine = create_async_engine(\n",
    "    DATABASE_URL,\n",
    "    pool_size=10,\n",
    "    max_overflow=20,\n",
    "    pool_timeout=30,\n",
    "    echo=True\n",
    ")\n",
    "\n",
    "async_session = async_sessionmaker(\n",
    "    bind=engine,\n",
    "    expire_on_commit=False,\n",
    "    class_=AsyncSession\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548b90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.orm import Mapped, mapped_column, DeclarativeBase\n",
    "from datetime import datetime\n",
    "from sqlalchemy import LargeBinary\n",
    "\n",
    "class Base(AsyncAttrs,DeclarativeBase):\n",
    "    pass\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = \"users\"\n",
    "\n",
    "    id: Mapped[int] = mapped_column(primary_key=True)\n",
    "    firstname: Mapped[str]\n",
    "    lastname: Mapped[str]\n",
    "    email: Mapped[str]\n",
    "\n",
    "class Message(Base):\n",
    "    __tablename__ = \"messages\"\n",
    "\n",
    "    id: Mapped[int] = mapped_column(primary_key=True, autoincrement=True)\n",
    "    user_id: Mapped[int]\n",
    "    conversation_id: Mapped[int] = mapped_column(nullable=True)\n",
    "    timestamp: Mapped[datetime]\n",
    "    content: Mapped[bytes] = mapped_column(LargeBinary)\n",
    "\n",
    "class Conversation(Base):\n",
    "    __tablename__ = \"conversations\"\n",
    "\n",
    "    id: Mapped[int] = mapped_column(primary_key=True) # Same as conversation_id\n",
    "    title: Mapped[str]\n",
    "    last_updated: Mapped[datetime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "457716ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-30 16:26:26,015 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-04-30 16:26:26,016 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = $1::VARCHAR AND pg_catalog.pg_class.relkind = ANY (ARRAY[$2::VARCHAR, $3::VARCHAR, $4::VARCHAR, $5::VARCHAR, $6::VARCHAR]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != $7::VARCHAR\n",
      "2025-04-30 16:26:26,017 INFO sqlalchemy.engine.Engine [cached since 7574s ago] ('users', 'r', 'p', 'f', 'v', 'm', 'pg_catalog')\n",
      "2025-04-30 16:26:26,022 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = $1::VARCHAR AND pg_catalog.pg_class.relkind = ANY (ARRAY[$2::VARCHAR, $3::VARCHAR, $4::VARCHAR, $5::VARCHAR, $6::VARCHAR]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != $7::VARCHAR\n",
      "2025-04-30 16:26:26,023 INFO sqlalchemy.engine.Engine [cached since 7574s ago] ('messages', 'r', 'p', 'f', 'v', 'm', 'pg_catalog')\n",
      "2025-04-30 16:26:26,026 INFO sqlalchemy.engine.Engine \n",
      "DROP TABLE messages\n",
      "2025-04-30 16:26:26,027 INFO sqlalchemy.engine.Engine [no key 0.00111s] ()\n",
      "2025-04-30 16:26:26,032 INFO sqlalchemy.engine.Engine \n",
      "DROP TABLE users\n",
      "2025-04-30 16:26:26,033 INFO sqlalchemy.engine.Engine [no key 0.00134s] ()\n",
      "2025-04-30 16:26:26,037 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "async with engine.begin() as conn:\n",
    "    await conn.run_sync(Base.metadata.drop_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c06956f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-30 16:26:26,860 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-04-30 16:26:26,862 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = $1::VARCHAR AND pg_catalog.pg_class.relkind = ANY (ARRAY[$2::VARCHAR, $3::VARCHAR, $4::VARCHAR, $5::VARCHAR, $6::VARCHAR]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != $7::VARCHAR\n",
      "2025-04-30 16:26:26,863 INFO sqlalchemy.engine.Engine [cached since 7575s ago] ('users', 'r', 'p', 'f', 'v', 'm', 'pg_catalog')\n",
      "2025-04-30 16:26:26,868 INFO sqlalchemy.engine.Engine SELECT pg_catalog.pg_class.relname \n",
      "FROM pg_catalog.pg_class JOIN pg_catalog.pg_namespace ON pg_catalog.pg_namespace.oid = pg_catalog.pg_class.relnamespace \n",
      "WHERE pg_catalog.pg_class.relname = $1::VARCHAR AND pg_catalog.pg_class.relkind = ANY (ARRAY[$2::VARCHAR, $3::VARCHAR, $4::VARCHAR, $5::VARCHAR, $6::VARCHAR]) AND pg_catalog.pg_table_is_visible(pg_catalog.pg_class.oid) AND pg_catalog.pg_namespace.nspname != $7::VARCHAR\n",
      "2025-04-30 16:26:26,869 INFO sqlalchemy.engine.Engine [cached since 7575s ago] ('messages', 'r', 'p', 'f', 'v', 'm', 'pg_catalog')\n",
      "2025-04-30 16:26:26,873 INFO sqlalchemy.engine.Engine \n",
      "CREATE TABLE users (\n",
      "\tid SERIAL NOT NULL, \n",
      "\tfirstname VARCHAR NOT NULL, \n",
      "\tlastname VARCHAR NOT NULL, \n",
      "\temail VARCHAR NOT NULL, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "\n",
      "2025-04-30 16:26:26,875 INFO sqlalchemy.engine.Engine [no key 0.00166s] ()\n",
      "2025-04-30 16:26:26,880 INFO sqlalchemy.engine.Engine \n",
      "CREATE TABLE messages (\n",
      "\tid SERIAL NOT NULL, \n",
      "\tuser_id INTEGER NOT NULL, \n",
      "\tconversation_id INTEGER, \n",
      "\ttimestamp TIMESTAMP WITHOUT TIME ZONE NOT NULL, \n",
      "\tcontent BYTEA NOT NULL, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "\n",
      "2025-04-30 16:26:26,881 INFO sqlalchemy.engine.Engine [no key 0.00086s] ()\n",
      "2025-04-30 16:26:26,886 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "async with engine.begin() as conn:\n",
    "    await conn.run_sync(Base.metadata.create_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f520815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-01 14:24:46,423 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-05-01 14:24:46,427 INFO sqlalchemy.engine.Engine SELECT anon_1.id, anon_1.user_id, anon_1.conversation_id, anon_1.timestamp, anon_1.content \n",
      "FROM (SELECT messages.id AS id, messages.user_id AS user_id, messages.conversation_id AS conversation_id, messages.timestamp AS timestamp, messages.content AS content, row_number() OVER (PARTITION BY messages.conversation_id ORDER BY messages.timestamp) AS row_num \n",
      "FROM messages \n",
      "WHERE messages.user_id = $1::INTEGER) AS anon_1 \n",
      "WHERE anon_1.row_num = $2::INTEGER ORDER BY anon_1.id ASC\n",
      "2025-05-01 14:24:46,428 INFO sqlalchemy.engine.Engine [cached since 88.55s ago] (1, 1)\n",
      "1 2025-04-30 16:27:23.046220 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T14:27:22.261542Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"Hi, I am here to test something\",\"timestamp\":\"2025-04-30T14:27:22.261547Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Hello! I\\'m here to help. What would you like to test? Let me know if you have any questions or need assistance with something specific.\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T14:27:22Z\",\"kind\":\"response\"}]'\n",
      "10 2025-04-30 16:35:43.193490 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T14:35:41.711585Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"did we have any conversation today?\",\"timestamp\":\"2025-04-30T14:35:41.711599Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"I don\\'t have the ability to browse or recall previous interactions once the session is over. Each conversation with me is independent of past interactions. How can I assist you today?\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T14:35:41Z\",\"kind\":\"response\"}]'\n",
      "17 2025-04-30 18:07:36.061111 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T16:07:35.409330Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"hello\",\"timestamp\":\"2025-04-30T16:07:35.409337Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Hello! How can I assist you today? Let\\'s have a friendly conversation.  How are you doing?\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T16:07:35Z\",\"kind\":\"response\"}]'\n",
      "29 2025-04-30 19:52:16.485483 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T17:52:16.030774Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"hi\",\"timestamp\":\"2025-04-30T17:52:16.030779Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Hello! How can I assist you today? Let\\'s have a friendly conversation.  How are you doing?\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T17:52:15Z\",\"kind\":\"response\"}]'\n",
      "30 2025-04-30 19:58:42.170514 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T17:58:36.581394Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"i am plannignto go to the cinema\",\"timestamp\":\"2025-04-30T17:58:36.581399Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"That sounds like a fun plan! Here are a few steps to help you prepare for your cinema trip:\\\\n\\\\n1. **Choose a Movie**: Decide on the movie you want to watch. You can check your local cinema\\'s website or a movie ticket booking app for showtimes and available seats.\\\\n\\\\n2. **Check Showtimes**: Make sure the showtime fits your schedule.\\\\n\\\\n3. **Buy Tickets**: Book your tickets in advance to ensure you get a seat. You can usually do this online or at the cinema.\\\\n\\\\n4. **Location**: Make sure you know the location of the cinema and how to get there. Check the address and plan your route if it\\'s your first time going.\\\\n\\\\n5. **Snacks**: Cinemas usually have snacks and drinks available for purchase. If you prefer, you can also bring your own, but check the cinema\\'s policy on outside food first.\\\\n\\\\n6. **Arrival Time**: Plan to arrive about 15-20 minutes before the showtime. This will give you enough time to collect your tickets, buy snacks, and find your seat.\\\\n\\\\n7. **Comfort**: Dress comfortably and bring a jacket or sweater, as cinemas can be air-conditioned and cool.\\\\n\\\\n8. **Etiquette**: Remember to silence your phone and refrain from talking during the movie to ensure a pleasant experience for everyone.\\\\n\\\\n9. **Enjoy the Movie**: Sit back, relax, and enjoy the show!\\\\n\\\\nHere are some popular movie ticket booking websites and apps:\\\\n\\\\n- Fandango\\\\n- Atom Tickets\\\\n- MovieTickets.com\\\\n- AMC Theatres\\\\n- Regal Cinemas\\\\n- Cinemark Theatres\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T17:58:36Z\",\"kind\":\"response\"}]'\n",
      "31 2025-05-01 00:55:44.524048 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T22:55:43.848148Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"hi\",\"timestamp\":\"2025-04-30T22:55:43.848153Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Hello! How can I assist you today? Let\\'s have a friendly conversation.  How are you doing?\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T22:55:43Z\",\"kind\":\"response\"}]'\n",
      "32 2025-05-01 00:58:32.416686 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T22:58:31.854644Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"hello\",\"timestamp\":\"2025-04-30T22:58:31.854649Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Hello! How can I assist you today? Let\\'s have a friendly conversation.  How are you doing?\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T22:58:31Z\",\"kind\":\"response\"}]'\n",
      "33 2025-05-01 01:00:59.959968 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T23:00:59.369778Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"hello\",\"timestamp\":\"2025-04-30T23:00:59.369782Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Hello! How can I assist you today? Let\\'s have a friendly conversation.  How are you doing?\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T23:00:58Z\",\"kind\":\"response\"}]'\n",
      "39 2025-05-01 08:58:47.094414 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-05-01T06:58:46.340074Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"Hi\",\"timestamp\":\"2025-05-01T06:58:46.340079Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Hello! How can I assist you today? Let\\'s have a friendly and engaging conversation.  How are you doing?\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-05-01T06:58:45Z\",\"kind\":\"response\"}]'\n",
      "2025-05-01 14:24:46,434 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import select, func\n",
    "from sqlalchemy.orm import aliased\n",
    "\n",
    "async with async_session() as session:\n",
    "    row_number = func.row_number().over(\n",
    "            partition_by=Message.conversation_id,\n",
    "            order_by=Message.timestamp\n",
    "        ).label(\"row_num\")\n",
    "\n",
    "    # Build base query with the row number\n",
    "    stmt = (\n",
    "        select(Message, row_number)\n",
    "        .where(Message.user_id == 1)\n",
    "        .subquery()\n",
    "    )\n",
    "    # Alias the subquery for filtering\n",
    "    sub_alias = aliased(Message, stmt)\n",
    "\n",
    "    # Select only the first message per conversation (row_num == 1)\n",
    "    query = (\n",
    "        select(sub_alias)\n",
    "        .where(stmt.c.row_num == 1)\n",
    "        .order_by(sub_alias.id.asc())\n",
    "    )\n",
    "\n",
    "    result = await session.execute(query)\n",
    "    for line in result.scalars().all():\n",
    "        print(line.id, line.timestamp, line.user_id, line.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9894a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-01 14:57:22,367 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-05-01 14:57:22,371 INFO sqlalchemy.engine.Engine SELECT messages.id, messages.user_id, messages.conversation_id, messages.timestamp, messages.content \n",
      "FROM messages \n",
      "WHERE messages.user_id = $1::INTEGER AND messages.id = messages.conversation_id\n",
      "2025-05-01 14:57:22,372 INFO sqlalchemy.engine.Engine [generated in 0.00122s] (1,)\n",
      "1 2025-04-30 16:27:23.046220 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T14:27:22.261542Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"Hi, I am here to test something\",\"timestamp\":\"2025-04-30T14:27:22.261547Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Hello! I\\'m here to help. What would you like to test? Let me know if you have any questions or need assistance with something specific.\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T14:27:22Z\",\"kind\":\"response\"}]'\n",
      "10 2025-04-30 16:35:43.193490 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T14:35:41.711585Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"did we have any conversation today?\",\"timestamp\":\"2025-04-30T14:35:41.711599Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"I don\\'t have the ability to browse or recall previous interactions once the session is over. Each conversation with me is independent of past interactions. How can I assist you today?\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T14:35:41Z\",\"kind\":\"response\"}]'\n",
      "17 2025-04-30 18:07:36.061111 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T16:07:35.409330Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"hello\",\"timestamp\":\"2025-04-30T16:07:35.409337Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Hello! How can I assist you today? Let\\'s have a friendly conversation.  How are you doing?\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T16:07:35Z\",\"kind\":\"response\"}]'\n",
      "29 2025-04-30 19:52:16.485483 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T17:52:16.030774Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"hi\",\"timestamp\":\"2025-04-30T17:52:16.030779Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Hello! How can I assist you today? Let\\'s have a friendly conversation.  How are you doing?\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T17:52:15Z\",\"kind\":\"response\"}]'\n",
      "30 2025-04-30 19:58:42.170514 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T17:58:36.581394Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"i am plannignto go to the cinema\",\"timestamp\":\"2025-04-30T17:58:36.581399Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"That sounds like a fun plan! Here are a few steps to help you prepare for your cinema trip:\\\\n\\\\n1. **Choose a Movie**: Decide on the movie you want to watch. You can check your local cinema\\'s website or a movie ticket booking app for showtimes and available seats.\\\\n\\\\n2. **Check Showtimes**: Make sure the showtime fits your schedule.\\\\n\\\\n3. **Buy Tickets**: Book your tickets in advance to ensure you get a seat. You can usually do this online or at the cinema.\\\\n\\\\n4. **Location**: Make sure you know the location of the cinema and how to get there. Check the address and plan your route if it\\'s your first time going.\\\\n\\\\n5. **Snacks**: Cinemas usually have snacks and drinks available for purchase. If you prefer, you can also bring your own, but check the cinema\\'s policy on outside food first.\\\\n\\\\n6. **Arrival Time**: Plan to arrive about 15-20 minutes before the showtime. This will give you enough time to collect your tickets, buy snacks, and find your seat.\\\\n\\\\n7. **Comfort**: Dress comfortably and bring a jacket or sweater, as cinemas can be air-conditioned and cool.\\\\n\\\\n8. **Etiquette**: Remember to silence your phone and refrain from talking during the movie to ensure a pleasant experience for everyone.\\\\n\\\\n9. **Enjoy the Movie**: Sit back, relax, and enjoy the show!\\\\n\\\\nHere are some popular movie ticket booking websites and apps:\\\\n\\\\n- Fandango\\\\n- Atom Tickets\\\\n- MovieTickets.com\\\\n- AMC Theatres\\\\n- Regal Cinemas\\\\n- Cinemark Theatres\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T17:58:36Z\",\"kind\":\"response\"}]'\n",
      "31 2025-05-01 00:55:44.524048 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T22:55:43.848148Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"hi\",\"timestamp\":\"2025-04-30T22:55:43.848153Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Hello! How can I assist you today? Let\\'s have a friendly conversation.  How are you doing?\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T22:55:43Z\",\"kind\":\"response\"}]'\n",
      "32 2025-05-01 00:58:32.416686 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T22:58:31.854644Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"hello\",\"timestamp\":\"2025-04-30T22:58:31.854649Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Hello! How can I assist you today? Let\\'s have a friendly conversation.  How are you doing?\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T22:58:31Z\",\"kind\":\"response\"}]'\n",
      "33 2025-05-01 01:00:59.959968 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-04-30T23:00:59.369778Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"hello\",\"timestamp\":\"2025-04-30T23:00:59.369782Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Hello! How can I assist you today? Let\\'s have a friendly conversation.  How are you doing?\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T23:00:58Z\",\"kind\":\"response\"}]'\n",
      "39 2025-05-01 08:58:47.094414 1 b'[{\"parts\":[{\"content\":\"You are a helpful assistant.\",\"timestamp\":\"2025-05-01T06:58:46.340074Z\",\"dynamic_ref\":null,\"part_kind\":\"system-prompt\"},{\"content\":\"Hi\",\"timestamp\":\"2025-05-01T06:58:46.340079Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Hello! How can I assist you today? Let\\'s have a friendly and engaging conversation.  How are you doing?\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-05-01T06:58:45Z\",\"kind\":\"response\"}]'\n",
      "2025-05-01 14:57:22,378 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "async with async_session() as session:\n",
    "    stmt = select(Message).where(Message.user_id == 1, Message.id == Message.conversation_id)\n",
    "    result = await session.execute(stmt)\n",
    "    for line in result.scalars().all():\n",
    "        print(line.id, line.timestamp, line.user_id, line.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d05a1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-01 15:14:40,327 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-05-01 15:14:40,329 INFO sqlalchemy.engine.Engine SELECT messages.id, messages.user_id, messages.conversation_id, messages.timestamp, messages.content \n",
      "FROM messages \n",
      "WHERE messages.conversation_id = $1::INTEGER ORDER BY messages.id DESC \n",
      " LIMIT $2::INTEGER\n",
      "2025-05-01 15:14:40,330 INFO sqlalchemy.engine.Engine [cached since 46.97s ago] (10, 3)\n",
      "14 2025-04-30 16:38:50.311156 1 b'[{\"parts\":[{\"content\":\"do you know about 1st of may?\",\"timestamp\":\"2025-04-30T14:38:46.802488Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Yes, May 1st is a significant date in Hungary, as it is Labor Day, also known as May Day. This is a national holiday in Hungary, as it is in many other countries around the world. Labor Day is a celebration of workers\\' achievements and is promoted by the international labor movement.\\\\n\\\\nIn Hungary, May 1st is a public holiday, which means that schools, businesses, and government offices are typically closed. Various events and demonstrations may be organized to commemorate the day and highlight workers\\' rights.\\\\n\\\\nHere are a few more details about May Day in Hungary:\\\\n\\\\n* Local name: Munka \\xc3\\x9cnnepe\\\\n* Date: May 1st\\\\n* Type of holiday: National holiday, public holiday\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T14:38:46Z\",\"kind\":\"response\"}]'\n",
      "15 2025-04-30 16:39:24.264171 1 b'[{\"parts\":[{\"content\":\"how about 20th of August?\",\"timestamp\":\"2025-04-30T14:39:19.053992Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"August 20 is a significant national holiday in Hungary known as St. Stephen\\'s Day (Szent Istv\\xc3\\xa1n nap). This day commemorates the founding of the Hungarian state and honors St. Stephen, the first king of Hungary.\\\\n\\\\n### Key Points About St. Stephen\\'s Day:\\\\n\\\\n- **Historical Significance**: St. Stephen is considered the founder of the Hungarian state. He was crowned the first king of Hungary on Christmas Day in the year 1000 or 1001.\\\\n- **Celebrations**: The day is marked by various celebrations, including fireworks, parades, and cultural events. There are also religious ceremonies and public speeches.\\\\n- **Public Holiday**: August 20 is a public holiday in Hungary, meaning that schools, businesses, and government offices are typically closed.\\\\n- **Bread Festival**: One of the notable traditions is the \\\\\"Bread Festival,\\\\\" where a large loaf of bread is blessed and then shared among the people.\\\\n- **Air and Water Parade**: There is often an air and water parade on the Danube River, which includes performances by the military and other organizations.\\\\n\\\\nSt. Stephen\\'s Day is a time for Hungarians to celebrate their national identity, history, and cultural heritage.\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T14:39:18Z\",\"kind\":\"response\"}]'\n",
      "16 2025-04-30 16:40:05.925907 1 b'[{\"parts\":[{\"content\":\"can you translate this to hungarian?\",\"timestamp\":\"2025-04-30T14:39:58.232488Z\",\"part_kind\":\"user-prompt\"}],\"kind\":\"request\"},{\"parts\":[{\"content\":\"Of course! Here is the translation of the information about August 20 into Hungarian:\\\\n\\\\nAugusztus 20-a Magyarorsz\\xc3\\xa1gon jelent\\xc5\\x91s nemzeti \\xc3\\xbcnnep, Szent Istv\\xc3\\xa1n napja (Szent Istv\\xc3\\xa1n nap). Ezen a napon \\xc3\\xbcnneplik az magyar \\xc3\\xa1llam alap\\xc3\\xadt\\xc3\\xa1s\\xc3\\xa1t \\xc3\\xa9s tisztelegnek Szent Istv\\xc3\\xa1n el\\xc5\\x91tt, Magyarorsz\\xc3\\xa1g els\\xc5\\x91 kir\\xc3\\xa1lya el\\xc5\\x91tt.\\\\n\\\\n### Szent Istv\\xc3\\xa1n napj\\xc3\\xa1nak fontos pontjai:\\\\n\\\\n- **T\\xc3\\xb6rt\\xc3\\xa9neti jelent\\xc5\\x91s\\xc3\\xa9g**: Szent Istv\\xc3\\xa1nt Magyarorsz\\xc3\\xa1g els\\xc5\\x91 kir\\xc3\\xa1lyak\\xc3\\xa9nt \\xc3\\xa9s az \\xc3\\xa1llam alap\\xc3\\xadt\\xc3\\xb3jak\\xc3\\xa9nt tartj\\xc3\\xa1k sz\\xc3\\xa1mon. 1000. vagy 1001. kar\\xc3\\xa1csony\\xc3\\xa1n koron\\xc3\\xa1zt\\xc3\\xa1k meg \\xc5\\x91t Magyarorsz\\xc3\\xa1g els\\xc5\\x91 kir\\xc3\\xa1ly\\xc3\\xa1v\\xc3\\xa1.\\\\n- **\\xc3\\x9cnnepl\\xc3\\xa9sek**: A napot k\\xc3\\xbcl\\xc3\\xb6nb\\xc3\\xb6z\\xc5\\x91 \\xc3\\xbcnnepl\\xc3\\xa9sek jellemzik, bele\\xc3\\xa9rtve a t\\xc5\\xb1zij\\xc3\\xa1t\\xc3\\xa9kot, a felvonul\\xc3\\xa1sokat \\xc3\\xa9s a kultur\\xc3\\xa1lis esem\\xc3\\xa9nyeket. Vannak vall\\xc3\\xa1si szertart\\xc3\\xa1sok \\xc3\\xa9s nyilv\\xc3\\xa1nos besz\\xc3\\xa9dek is.\\\\n- **Munkasz\\xc3\\xbcneti nap**: Augusztus 20. munkasz\\xc3\\xbcneti nap Magyarorsz\\xc3\\xa1gon, ami azt jelenti, hogy az iskol\\xc3\\xa1k, v\\xc3\\xa1llalkoz\\xc3\\xa1sok \\xc3\\xa9s korm\\xc3\\xa1nyzati irod\\xc3\\xa1k \\xc3\\xa1ltal\\xc3\\xa1ban z\\xc3\\xa1rva vannak.\\\\n- **Keny\\xc3\\xa9r\\xc3\\xbcnnep**: Az egyik jellegzetes hagyom\\xc3\\xa1ny a \\\\\"Keny\\xc3\\xa9r\\xc3\\xbcnnep,\\\\\" ahol egy nagy kenyeret \\xc3\\xa1ldanak meg, majd megosztanak az emberek k\\xc3\\xb6z\\xc3\\xb6tt.\\\\n- **L\\xc3\\xa9gi \\xc3\\xa9s v\\xc3\\xadzi felvonul\\xc3\\xa1s**: Gyakran rendeznek l\\xc3\\xa9gi \\xc3\\xa9s v\\xc3\\xadzi felvonul\\xc3\\xa1st a Duna foly\\xc3\\xb3n, amely katonai \\xc3\\xa9s m\\xc3\\xa1s szervezetek el\\xc5\\x91ad\\xc3\\xa1sait is mag\\xc3\\xa1ban foglalja.\\\\n\\\\nSzent Istv\\xc3\\xa1n napja alkalom arra, hogy a magyarok \\xc3\\xbcnnepelj\\xc3\\xa9k nemzeti identit\\xc3\\xa1sukat, t\\xc3\\xb6rt\\xc3\\xa9nelm\\xc3\\xbcket \\xc3\\xa9s kultur\\xc3\\xa1lis \\xc3\\xb6r\\xc3\\xb6ks\\xc3\\xa9g\\xc3\\xbcket.\",\"part_kind\":\"text\"}],\"model_name\":\"mistral-large-latest\",\"timestamp\":\"2025-04-30T14:39:58Z\",\"kind\":\"response\"}]'\n",
      "2025-05-01 15:14:40,335 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "async with async_session() as session:\n",
    "    stmt = select(Message).where(Message.conversation_id == 10)\n",
    "\n",
    "    stmt = stmt.order_by(Message.id.desc()).limit(3)\n",
    "    result = await session.execute(stmt)\n",
    "    final_list = list(reversed(result.scalars().all()))\n",
    "    for msg in final_list:\n",
    "        print(msg.id, msg.timestamp, msg.user_id, msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56061102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 20:07:16,839 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-04-16 20:07:16,842 INFO sqlalchemy.engine.Engine SELECT users.id, users.firstname, users.lastname, users.email \n",
      "FROM users \n",
      "WHERE users.id = $1::INTEGER\n",
      "2025-04-16 20:07:16,843 INFO sqlalchemy.engine.Engine [cached since 7.403e+04s ago] (1,)\n",
      "John Wick\n",
      "2025-04-16 20:07:16,847 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import select\n",
    "\n",
    "async with async_session() as session:\n",
    "    stmt = select(User).where(User.id == 1)\n",
    "    result = await session.execute(stmt)\n",
    "    current_user = result.scalars().first()\n",
    "    print(current_user.firstname + \" \" + current_user.lastname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c69a0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "john_message = Message(user_id=1, conversation_id=1, timestamp=datetime.now(), content=\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec1e977a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 20:11:55,824 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-04-16 20:11:55,827 INFO sqlalchemy.engine.Engine INSERT INTO messages (user_id, conversation_id, timestamp, content) VALUES ($1::INTEGER, $2::INTEGER, $3::TIMESTAMP WITHOUT TIME ZONE, $4::VARCHAR) RETURNING messages.id\n",
      "2025-04-16 20:11:55,828 INFO sqlalchemy.engine.Engine [generated in 0.00094s] (1, 1, datetime.datetime(2025, 4, 16, 20, 11, 49, 718225), 'Hello, how are you?')\n",
      "2025-04-16 20:11:55,834 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "async with async_session() as session:\n",
    "    session.add(john_message)\n",
    "    await session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e455a85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 20:36:26,745 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-04-16 20:36:26,748 INFO sqlalchemy.engine.Engine SELECT messages.id, messages.user_id, messages.conversation_id, messages.timestamp, messages.content \n",
      "FROM messages \n",
      "WHERE messages.user_id = $1::INTEGER\n",
      "2025-04-16 20:36:26,750 INFO sqlalchemy.engine.Engine [cached since 1395s ago] (1,)\n",
      "Hello, how are you?\n",
      "2025-04-16 20:36:26,754 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "async with async_session() as session:\n",
    "    stmt = select(Message).where(Message.user_id == 1)\n",
    "    result = await session.execute(stmt)\n",
    "    user_message = result.scalars().first()\n",
    "    print(user_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f23ff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 20:37:48,380 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-04-16 20:37:48,382 INFO sqlalchemy.engine.Engine SELECT users.firstname, users.lastname, messages.content \n",
      "FROM users JOIN messages ON users.id = messages.user_id \n",
      "WHERE users.id = $1::INTEGER\n",
      "2025-04-16 20:37:48,383 INFO sqlalchemy.engine.Engine [cached since 192.6s ago] (1,)\n",
      "John Wick: Hello, how are you?\n",
      "2025-04-16 20:37:48,387 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "async with async_session() as session:\n",
    "    stmt = select(User.firstname, User.lastname, Message.content).join_from(User, Message).where(User.id == 1)\n",
    "    result = await session.execute(stmt)\n",
    "    firstname, lastname, message = result.all()[0]\n",
    "    print(f\"{firstname} {lastname}: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a01a611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 21:19:21,646 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-04-16 21:19:21,648 INFO sqlalchemy.engine.Engine SELECT count(users.id) AS user_count, users.firstname || $1::VARCHAR || users.lastname AS full_name \n",
      "FROM users GROUP BY users.firstname, users.lastname\n",
      "2025-04-16 21:19:21,649 INFO sqlalchemy.engine.Engine [cached since 1235s ago] (' ',)\n",
      "[(1, 'John Doe')]\n",
      "2025-04-16 21:19:21,653 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import func, literal\n",
    "\n",
    "async with async_session() as session:\n",
    "    stmt = select(func.count(User.id).label(\"user_count\"), (User.firstname + ' ' + User.lastname).label(\"full_name\")).group_by(User.firstname, User.lastname)\n",
    "    result = await session.execute(stmt)\n",
    "    print(result.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "952b3d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPDATE users SET lastname=:lastname WHERE users.id = :id_1\n",
      "2025-04-16 21:19:14,690 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-04-16 21:19:14,693 INFO sqlalchemy.engine.Engine UPDATE users SET lastname=$1::VARCHAR WHERE users.id = $2::INTEGER\n",
      "2025-04-16 21:19:14,694 INFO sqlalchemy.engine.Engine [generated in 0.00102s] ('Doe', 1)\n",
      "2025-04-16 21:19:14,698 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import update\n",
    "\n",
    "async with async_session() as session:\n",
    "    stmt = update(User).where(User.id == 1).values(lastname=\"Doe\")\n",
    "    print(stmt)\n",
    "    await session.execute(stmt)\n",
    "    await session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f5329e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELETE FROM users WHERE users.id = :id_1\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import delete\n",
    "\n",
    "async with async_session() as session:\n",
    "    stmt = delete(User).where(User.id == 1)\n",
    "    print(stmt)\n",
    "    # await session.execute(stmt)\n",
    "    # await session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a6c5462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 21:28:37,533 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-04-16 21:28:37,534 INFO sqlalchemy.engine.Engine SELECT users.id AS users_id, users.firstname AS users_firstname, users.lastname AS users_lastname, users.email AS users_email \n",
      "FROM users \n",
      "WHERE users.id = $1::INTEGER\n",
      "2025-04-16 21:28:37,535 INFO sqlalchemy.engine.Engine [cached since 60.64s ago] (1,)\n",
      "John\n",
      "2025-04-16 21:28:37,539 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async with async_session() as session:\n",
    "    result = await session.get(User, 1)\n",
    "    print(result.firstname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d23069a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-16 22:07:04,761 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-04-16 22:07:04,764 INFO sqlalchemy.engine.Engine INSERT INTO messages (user_id, conversation_id, timestamp, content) VALUES ($1::INTEGER, $2::INTEGER, $3::TIMESTAMP WITHOUT TIME ZONE, $4::VARCHAR) RETURNING messages.id\n",
      "2025-04-16 22:07:04,764 INFO sqlalchemy.engine.Engine [cached since 6909s ago] (1, 0, datetime.datetime(2025, 4, 16, 22, 7, 4, 760712), 'Hello, how are you?')\n",
      "2025-04-16 22:07:04,770 INFO sqlalchemy.engine.Engine UPDATE messages SET conversation_id=$1::INTEGER WHERE messages.id = $2::INTEGER\n",
      "2025-04-16 22:07:04,771 INFO sqlalchemy.engine.Engine [generated in 0.00123s] (3, 3)\n",
      "2025-04-16 22:07:04,774 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    }
   ],
   "source": [
    "new_message = Message(user_id=1, conversation_id=0, timestamp=datetime.now(), content=\"Hello, how are you?\")\n",
    "\n",
    "async with async_session() as session:\n",
    "    session.add(new_message)\n",
    "    await session.flush()\n",
    "    new_message.conversation_id = new_message.id\n",
    "    await session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0932148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-17 16:10:01,211 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n",
      "2025-04-17 16:10:01,212 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2025-04-17 16:10:01,216 INFO sqlalchemy.engine.Engine select current_schema()\n",
      "2025-04-17 16:10:01,217 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2025-04-17 16:10:01,220 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n",
      "2025-04-17 16:10:01,221 INFO sqlalchemy.engine.Engine [raw sql] ()\n",
      "2025-04-17 16:10:01,224 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n",
      "2025-04-17 16:10:01,228 INFO sqlalchemy.engine.Engine SELECT messages.id, messages.user_id, messages.conversation_id, messages.timestamp, messages.content \n",
      "FROM messages \n",
      "WHERE messages.conversation_id = $1::INTEGER\n",
      "2025-04-17 16:10:01,229 INFO sqlalchemy.engine.Engine [generated in 0.00104s] (1,)\n",
      "Hello, how are you?\n",
      "Hello, how are you?\n",
      "2025-04-17 16:10:01,234 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import select\n",
    "\n",
    "async with async_session() as session:\n",
    "    stmt = select(Message).where(Message.conversation_id == 1)\n",
    "    result = await session.execute(stmt)\n",
    "    current_user = result.scalars().all()\n",
    "\n",
    "    for message in current_user:\n",
    "        print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e3d0c6",
   "metadata": {},
   "source": [
    "# Mistral & pydantic ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc6cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.mistral import MistralModel\n",
    "from pydantic_ai.providers.mistral import MistralProvider\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "model = MistralModel(\n",
    "    'mistral-large-latest', provider=MistralProvider(api_key='Ua6lkNDZiSdPYKEN2bCoEFoS4H3xBoQO')\n",
    ")\n",
    "agent = Agent(model, system_prompt=\"You are a helpful assistant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8631aec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelRequest(parts=[SystemPromptPart(content='You are a helpful assistant.', timestamp=datetime.datetime(2025, 4, 30, 11, 25, 41, 403342, tzinfo=datetime.timezone.utc), dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='Hi', timestamp=datetime.datetime(2025, 4, 30, 11, 25, 41, 403349, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'), ModelResponse(parts=[TextPart(content=\"Hello! How can I assist you today? Let's have a friendly conversation.  How are you doing?\", part_kind='text')], model_name='mistral-large-latest', timestamp=datetime.datetime(2025, 4, 30, 11, 25, 41, tzinfo=datetime.timezone.utc), kind='response')]\n"
     ]
    }
   ],
   "source": [
    "response = agent.run_sync(\"Hi\")\n",
    "print(response.new_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216d2b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemPromptPart(content='You are a helpful assistant.', timestamp=datetime.datetime(2025, 4, 30, 11, 25, 41, 403342, tzinfo=datetime.timezone.utc), dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='Hi', timestamp=datetime.datetime(2025, 4, 30, 11, 25, 41, 403349, tzinfo=datetime.timezone.utc), part_kind='user-prompt')]\n",
      "[TextPart(content=\"Hello! How can I assist you today? Let's have a friendly conversation.  How are you doing?\", part_kind='text')]\n"
     ]
    }
   ],
   "source": [
    "for message in response.new_messages():\n",
    "    print(message.parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f41461c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A company could benefit from implementing a helpful assistant like me in several ways. Here are some expected results:\n",
      "\n",
      "1. **Improved Customer Service:**\n",
      "   - **24/7 Availability:** Provide round-the-clock support to customers, answering queries and resolving issues promptly.\n",
      "   - **Consistency:** Ensure consistent and standardized responses to customer inquiries.\n",
      "   - **Scalability:** Handle multiple customers simultaneously without additional strain on resources.\n",
      "\n",
      "2. **Increased Efficiency:**\n",
      "   - **Time-Saving:** Automate routine tasks and FAQs, freeing up human agents to focus on more complex issues.\n",
      "   - **Quick Resolutions:** Provide instant responses, reducing customer wait times and improving satisfaction.\n",
      "\n",
      "3. **Cost Savings:**\n",
      "   - **Reduced Labor Costs:** Lower the need for human agents, particularly for handling simple, repetitive tasks.\n",
      "   - **Efficient Training:** Once set up, the assistant can be easily updated with new information, reducing training costs.\n",
      "\n",
      "4. **Enhanced Customer Experience:**\n",
      "   - **Personalization:** Tailor responses based on customer data and preferences.\n",
      "   - **Multilingual Support:** Offer support in multiple languages, breaking down language barriers.\n",
      "\n",
      "5. **Data Collection and Analysis:**\n",
      "   - **Customer Insights:** Gather valuable data on customer interactions, preferences, and common issues.\n",
      "   - **Feedback Loop:** Use customer feedback to improve products, services, and the assistant itself.\n",
      "\n",
      "6. **Lead Generation and Sales:**\n",
      "   - **Qualification:** Engage potential customers and qualify leads through automated conversations.\n",
      "   - **Upselling/Cross-selling:** Provide personalized recommendations and offers based on customer interactions.\n",
      "\n",
      "7. **Branding:**\n",
      "   - **Consistent Voice:** Maintain a consistent brand voice and tone in all customer interactions.\n",
      "   - **Innovative Image:** Project an innovative and tech-savvy image to customers.\n",
      "\n",
      "8. **Employee Satisfaction:**\n",
      "   - **Reduced Workload:** Alleviate the workload of human agents, improving their job satisfaction.\n",
      "   - **Training and Support:** Provide constant training and support to employees.\n"
     ]
    }
   ],
   "source": [
    "msg = \"And what do you think how could a company benefit from it? What are the expected results?\"\n",
    "\n",
    "response = agent.run_sync(\n",
    "    user_prompt=msg,\n",
    "    message_history=response.all_messages()\n",
    ")\n",
    "\n",
    "print(response.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2ae18ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[SystemPromptPart(content='You are a helpful assistant.', timestamp=datetime.datetime(2025, 4, 30, 11, 25, 41, 403342, tzinfo=datetime.timezone.utc), dynamic_ref=None, part_kind='system-prompt'), UserPromptPart(content='Hi', timestamp=datetime.datetime(2025, 4, 30, 11, 25, 41, 403349, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'),\n",
       " ModelResponse(parts=[TextPart(content=\"Hello! How can I assist you today? Let's have a friendly conversation.  How are you doing?\", part_kind='text')], model_name='mistral-large-latest', timestamp=datetime.datetime(2025, 4, 30, 11, 25, 41, tzinfo=datetime.timezone.utc), kind='response'),\n",
       " ModelRequest(parts=[UserPromptPart(content='And what do you think how could a company benefit from it? What are the expected results?', timestamp=datetime.datetime(2025, 4, 30, 11, 25, 45, 169040, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'),\n",
       " ModelResponse(parts=[TextPart(content='A company could benefit from implementing a helpful assistant like me in several ways. Here are some expected results:\\n\\n1. **Improved Customer Service:**\\n   - **24/7 Availability:** Provide round-the-clock support to customers, answering queries and resolving issues promptly.\\n   - **Consistency:** Ensure consistent and standardized responses to customer inquiries.\\n   - **Scalability:** Handle multiple customers simultaneously without additional strain on resources.\\n\\n2. **Increased Efficiency:**\\n   - **Time-Saving:** Automate routine tasks and FAQs, freeing up human agents to focus on more complex issues.\\n   - **Quick Resolutions:** Provide instant responses, reducing customer wait times and improving satisfaction.\\n\\n3. **Cost Savings:**\\n   - **Reduced Labor Costs:** Lower the need for human agents, particularly for handling simple, repetitive tasks.\\n   - **Efficient Training:** Once set up, the assistant can be easily updated with new information, reducing training costs.\\n\\n4. **Enhanced Customer Experience:**\\n   - **Personalization:** Tailor responses based on customer data and preferences.\\n   - **Multilingual Support:** Offer support in multiple languages, breaking down language barriers.\\n\\n5. **Data Collection and Analysis:**\\n   - **Customer Insights:** Gather valuable data on customer interactions, preferences, and common issues.\\n   - **Feedback Loop:** Use customer feedback to improve products, services, and the assistant itself.\\n\\n6. **Lead Generation and Sales:**\\n   - **Qualification:** Engage potential customers and qualify leads through automated conversations.\\n   - **Upselling/Cross-selling:** Provide personalized recommendations and offers based on customer interactions.\\n\\n7. **Branding:**\\n   - **Consistent Voice:** Maintain a consistent brand voice and tone in all customer interactions.\\n   - **Innovative Image:** Project an innovative and tech-savvy image to customers.\\n\\n8. **Employee Satisfaction:**\\n   - **Reduced Workload:** Alleviate the workload of human agents, improving their job satisfaction.\\n   - **Training and Support:** Provide constant training and support to employees.', part_kind='text')], model_name='mistral-large-latest', timestamp=datetime.datetime(2025, 4, 30, 11, 25, 45, tzinfo=datetime.timezone.utc), kind='response')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.all_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e217b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: user-prompt\n",
      "Kind: request\n",
      "Content: Hi\n",
      "Timestamp: 2025-04-28T18:55:11.148279+00:00\n",
      "\n",
      "Role: text\n",
      "Kind: response\n",
      "Content: Hello! How can I assist you today? Let's have a friendly conversation.  How are you doing?\n",
      "Timestamp: None\n",
      "\n",
      "Role: user-prompt\n",
      "Kind: request\n",
      "Content: I am looking for a good conversation starter. Can you help me?\n",
      "Timestamp: 2025-04-28T19:07:21.549595+00:00\n",
      "\n",
      "Role: text\n",
      "Kind: response\n",
      "Content: Of course! Here are a few conversation starters that can help spark an engaging discussion:\n",
      "\n",
      "1. **Shared Interests:** \"What are some of your hobbies or interests? I'm always looking for new things to try.\"\n",
      "2. **Books, Movies, or TV Shows:** \"Have you read or watched anything interesting lately?\"\n",
      "3. **Travel:** \"If you could travel anywhere in the world, where would it be and why?\"\n",
      "4. **Food:** \"What's the best meal you've ever had? I'm always looking for new recipes to try.\"\n",
      "5. **Current Events:** \"What do you think about the recent developments in [insert topic]?\"\n",
      "6. **Goals:** \"What's one thing you're really passionate about and working towards?\"\n",
      "7. **Unique Experiences:** \"What's the most unusual or memorable experience you've ever had?\"\n",
      "8. **Dreams:** \"If you could have any superpower, what would it be and why?\"\n",
      "9. **Music:** \"What kind of music do you like? Have you been to any live concerts lately?\"\n",
      "10. **Learning:** \"Is there a skill or subject you've always wanted to learn more about?\"\n",
      "Timestamp: None\n",
      "\n",
      "Role: user-prompt\n",
      "Kind: request\n",
      "Content: What do you think about the recent developments in the tech industry?\n",
      "Timestamp: 2025-04-28T19:08:23.324644+00:00\n",
      "\n",
      "Role: text\n",
      "Kind: response\n",
      "Content: The tech industry is always evolving rapidly, so there are several recent developments worth discussing. Here are a few key areas:\n",
      "\n",
      "1. **Artificial Intelligence (AI) and Machine Learning (ML):** AI and ML continue to advance significantly. We're seeing more sophisticated applications in various fields, from healthcare to finance and even creative industries. The development of large language models, like the one I'm based on, is a testament to this progress. However, there are also ongoing conversations about the ethical implications and potential job displacements due to AI.\n",
      "\n",
      "2. **Remote Work and Virtual Reality (VR):** The shift towards remote work has accelerated the development of collaborative tools and platforms. VR is also making strides in creating immersive virtual workspaces and meeting rooms.\n",
      "\n",
      "3. **5G Technology:** The rollout of 5G networks is promising faster internet speeds and more reliable connections. This is expected to benefit technologies like the Internet of Things (IoT), autonomous vehicles, and smart cities.\n",
      "\n",
      "4. **Cybersecurity:** With the increase in digital services, cybersecurity has become more critical than ever. There's a growing focus on protecting user data and ensuring the security of digital infrastructure.\n",
      "\n",
      "5. **Space Exploration:** The tech industry is also looking to the stars, with companies like SpaceX, Blue Origin, and NASA working on projects to make space tourism a reality and establish bases on the Moon and Mars.\n",
      "\n",
      "6. **Quantum Computing:** Although still in its early stages, quantum computing has the potential to revolutionize industries by solving complex problems much faster than classical computers.\n",
      "\n",
      "7. **Regulatory Scrutiny:** Many tech giants are facing increased regulatory scrutiny due to concerns about market dominance, data privacy, and content moderation. This is leading to discussions about potential reforms and regulations.\n",
      "\n",
      "What are your thoughts on these developments? Is there a specific area in the tech industry that you're particularly interested in or concerned about? I'd be happy to discuss further!\n",
      "Timestamp: None\n",
      "\n",
      "Role: user-prompt\n",
      "Kind: request\n",
      "Content: What do you think about the development if AI systems?\n",
      "Timestamp: 2025-04-28T19:39:36.574875+00:00\n",
      "\n",
      "Role: text\n",
      "Kind: response\n",
      "Content: The development of AI systems is a complex and multifaceted topic with numerous implications. Here are some of my thoughts on the matter:\n",
      "\n",
      "**Positive Aspects:**\n",
      "\n",
      "1. **Efficiency and Productivity:** AI can automate repetitive tasks, freeing up human time for more creative and strategic work. This can lead to increased productivity and efficiency in various industries.\n",
      "2. **Decision Making and Predictions:** AI can analyze vast amounts of data and identify patterns that humans might miss. This can aid in better decision-making, predictive analytics, and problem-solving.\n",
      "3. **Innovation:** AI is enabling the development of new products and services, from improved speech recognition and image processing to advanced drug discovery and personalized learning platforms.\n",
      "4. **Accessibility:** AI can help make technology more accessible to people with disabilities, for example, through voice-controlled devices and smart assistants.\n",
      "\n",
      "**Challenges and Concerns:**\n",
      "\n",
      "1. **Job Displacement:** There are valid concerns about AI leading to job displacement in sectors where tasks can be easily automated. However, AI also creates new jobs and augments many existing ones.\n",
      "2. **Bias and Fairness:** AI systems are trained on data created by humans, and they can inadvertently perpetuate or even amplify existing biases. Ensuring fairness and unbiased outcomes is an ongoing challenge.\n",
      "3. **Privacy:** AI often relies on large amounts of data, which can raise privacy concerns. Balancing the benefits of AI with user privacy is a critical issue that needs to be addressed.\n",
      "4. **Accountability:** As AI systems become more complex, it can be challenging to understand how they make decisions. Ensuring the accountability and explainability of AI systems is essential.\n",
      "5. **Autonomous Weapons and Malicious Use:** The potential misuse of AI, such as in autonomous weapons or cyberattacks, poses significant ethical and security challenges.\n",
      "\n",
      "**Ethical Considerations:**\n",
      "\n",
      "The development of AI should be guided by ethical principles that prioritize human well-being, fairness, accountability, and transparency. This includes involving diverse stakeholders in AI development, being mindful of AI's potential consequences, and promoting beneficial and responsible AI use.\n",
      "\n",
      "In conclusion, AI has the potential to significantly improve our lives and society, but it also presents challenges that need to be addressed carefully and thoughtfully. It's essential to strike a balance between embracing the benefits of AI and mitigating its potential risks.\n",
      "\n",
      "What are your thoughts on AI development? Are there any specific aspects of AI that you're particularly interested in or concerned about? I'd love to hear your perspective!\n",
      "Timestamp: None\n",
      "\n",
      "Role: user-prompt\n",
      "Kind: request\n",
      "Content: What are the befenits of using AI?\n",
      "Timestamp: 2025-04-28T19:40:54.854962+00:00\n",
      "\n",
      "Role: text\n",
      "Kind: response\n",
      "Content: Using AI offers numerous benefits across various industries and aspects of life. Here are some key advantages:\n",
      "\n",
      "1. **Increased Efficiency and Productivity:**\n",
      "\t* Automation of repetitive tasks, freeing up human time for more creative and strategic work.\n",
      "\t* Streamlined operations and improved workflows.\n",
      "\t* 24/7 availability, allowing for continuous production and support.\n",
      "2. **Improved Decision Making:**\n",
      "\t* Analysis of large and complex datasets, identifying patterns and insights that humans might miss.\n",
      "\t* Data-driven predictions and recommendations for better decision-making.\n",
      "\t* Real-time analytics and monitoring, enabling quick responses to changing situations.\n",
      "3. **Personalization:**\n",
      "\t* Personalized recommendations and experiences based on user preferences and behavior.\n",
      "\t* Targeted marketing and advertising, improving customer engagement and satisfaction.\n",
      "\t* Adaptive learning platforms that tailor educational content to individual students' needs.\n",
      "4. **Cost Savings:**\n",
      "\t* Reduced labor costs through automation.\n",
      "\t* Improved resource allocation and optimization.\n",
      "\t* Fraud detection and prevention, minimizing financial losses.\n",
      "5. **Enhanced Customer Service:**\n",
      "\t* AI-powered chatbots and virtual assistants for round-the-clock customer support.\n",
      "\t* Quick and accurate responses to customer inquiries.\n",
      "\t* Sentiment analysis to understand customer feedback and improve services.\n",
      "6. **Innovation:**\n",
      "\t* Development of new products and services, such as smart home devices, autonomous vehicles, and AI-driven healthcare solutions.\n",
      "\t* Advancements in research and development, like drug discovery and materials science.\n",
      "\t* Creation of new art and entertainment forms, such as AI-generated music, poetry, and visual arts.\n",
      "7. **Accessibility:**\n",
      "\t* Voice-controlled devices and smart assistants for people with disabilities.\n",
      "\t* AI-powered tools for speech recognition, text-to-speech, and real-time captioning.\n",
      "\t* Enhanced user interfaces and experiences tailored to individual needs.\n",
      "8. **Predictive Maintenance:**\n",
      "\t* Monitoring and predicting equipment failures before they occur, minimizing downtime and repair costs.\n",
      "\t* Improved asset management and resource planning.\n",
      "9. **Environmental Monitoring and Protection:**\n",
      "\t* AI-driven models for climate change prediction and mitigation.\n",
      "\t* Wildlife tracking and protection using AI and computer vision.\n",
      "\t* Smart agriculture and precision farming for improved crop yields and reduced environmental impact.\n",
      "10. **Healthcare Improvements:**\n",
      "* AI-assisted diagnostics and early detection of diseases.\n",
      "* Personalized treatment plans based on individual patient data.\n",
      "* Drug discovery and development, accelerating the creation of new medications.\n",
      "\n",
      "These benefits demonstrate the potential of AI to transform industries and improve our lives. However, it's essential to ensure that AI is developed and deployed responsibly, ethically, and with a focus on human well-being.\n",
      "Timestamp: None\n",
      "\n",
      "Role: user-prompt\n",
      "Kind: request\n",
      "Content: And what do you think how could a company benefit from it? What are the expected results?\n",
      "Timestamp: 2025-04-28T19:42:15.336549+00:00\n",
      "\n",
      "Role: text\n",
      "Kind: response\n",
      "Content: A company can benefit from integrating AI in numerous ways, and the expected results can vary depending on the industry, size, and specific use cases. Here are some potential benefits and expected results:\n",
      "\n",
      "1. **Operational Efficiency:**\n",
      "\t* **Expected Results:**\n",
      "\t\t+ Streamlined processes and workflows.\n",
      "\t\t+ Reduced human error and increased accuracy.\n",
      "\t\t+ Faster completion of tasks.\n",
      "\t\t+ Improved resource allocation and optimization.\n",
      "\t\t+ Cost savings through automation and better decision-making.\n",
      "2. **Customer Service and Experience:**\n",
      "\t* **Expected Results:**\n",
      "\t\t+ Faster and more accurate responses to customer inquiries.\n",
      "\t\t+ 24/7 customer support through AI-powered chatbots and virtual assistants.\n",
      "\t\t+ Personalized recommendations and experiences for customers.\n",
      "\t\t+ Improved customer satisfaction and loyalty.\n",
      "\t\t+ Increased sales and revenue through targeted marketing and upselling.\n",
      "3. **Data Analysis and Insights:**\n",
      "\t* **Expected Results:**\n",
      "\t\t+ Better understanding of customer behavior and preferences.\n",
      "\t\t+ Identification of market trends and opportunities.\n",
      "\t\t+ Data-driven decision-making and strategic planning.\n",
      "\t\t+ Improved forecasting and demand prediction.\n",
      "\t\t+ Early detection of anomalies, fraud, or potential issues.\n",
      "4. **Predictive Maintenance:**\n",
      "\t* **Expected Results:**\n",
      "\t\t+ Reduced equipment downtime and repair costs.\n",
      "\t\t+ Extended lifespan of assets and machinery.\n",
      "\t\t+ Improved overall equipment effectiveness (OEE).\n",
      "\t\t+ Better inventory management and parts ordering.\n",
      "\t\t+ Enhanced workplace safety by preventing equipment failures.\n",
      "5. **Employee Productivity and Engagement:**\n",
      "\t* **Expected Results:**\n",
      "\t\t+ Automation of repetitive tasks, allowing employees to focus on more creative and strategic work.\n",
      "\t\t+ Improved employee skills and knowledge through AI-driven training and development.\n",
      "\t\t+ Better employee collaboration and communication through AI-powered tools.\n",
      "\t\t+ Increased employee engagement and job satisfaction.\n",
      "\t\t+ Enhanced talent acquisition and retention through AI-driven HR processes.\n",
      "6. **Innovation and Competitive Advantage:**\n",
      "\t* **Expected Results:**\n",
      "\t\t+ Development of new AI-driven products and services.\n",
      "\t\t+ Improved time-to-market for new offerings.\n",
      "\t\t+ Better adaptation to market changes and disruptions.\n",
      "\t\t+ Enhanced brand reputation as an innovative and forward-thinking company.\n",
      "\t\t+ Increased market share and competitive differentiation.\n",
      "7. **Risk Management and Compliance:**\n",
      "\t* **Expected Results:**\n",
      "\t\t+ Improved identification and mitigation of risks.\n",
      "\t\t+ Better compliance with regulations and industry standards.\n",
      "\t\t+ Enhanced detection and prevention of fraudulent activities.\n",
      "\t\t+ More effective auditing and reporting processes.\n",
      "\t\t+ Reduced legal and financial exposure.\n",
      "\n",
      "To maximize the benefits of AI, a company should:\n",
      "\n",
      "* Identify and prioritize specific use cases and applications tailored to its needs.\n",
      "* Invest in the necessary infrastructure, data, and talent to support AI initiatives.\n",
      "* Foster a culture of innovation and experimentation.\n",
      "* Monitor and evaluate the performance of AI implementations.\n",
      "* Ensure responsible and ethical use of AI.\n",
      "\n",
      "By doing so, a company can successfully integrate AI into its operations and achieve the expected results.\n",
      "Timestamp: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in response.all_messages():\n",
    "    message_parts = message.parts[-1]\n",
    "    request_time = None\n",
    "    if message.kind == \"request\":\n",
    "        request_time = message_parts.timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"Role: {message_parts.part_kind}\\nKind: {message.kind}\\nContent: {message_parts.content}\\nTimestamp: {request_time}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e608532f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A company can benefit from integrating AI in numerous ways, and the expected results can vary depending on the industry, size, and specific use cases. Here are some potential benefits and expected results:\\n\\n1. **Operational Efficiency:**\\n\\t* **Expected Results:**\\n\\t\\t+ Streamlined processes and workflows.\\n\\t\\t+ Reduced human error and increased accuracy.\\n\\t\\t+ Faster completion of tasks.\\n\\t\\t+ Improved resource allocation and optimization.\\n\\t\\t+ Cost savings through automation and better decision-making.\\n2. **Customer Service and Experience:**\\n\\t* **Expected Results:**\\n\\t\\t+ Faster and more accurate responses to customer inquiries.\\n\\t\\t+ 24/7 customer support through AI-powered chatbots and virtual assistants.\\n\\t\\t+ Personalized recommendations and experiences for customers.\\n\\t\\t+ Improved customer satisfaction and loyalty.\\n\\t\\t+ Increased sales and revenue through targeted marketing and upselling.\\n3. **Data Analysis and Insights:**\\n\\t* **Expected Results:**\\n\\t\\t+ Better understanding of customer behavior and preferences.\\n\\t\\t+ Identification of market trends and opportunities.\\n\\t\\t+ Data-driven decision-making and strategic planning.\\n\\t\\t+ Improved forecasting and demand prediction.\\n\\t\\t+ Early detection of anomalies, fraud, or potential issues.\\n4. **Predictive Maintenance:**\\n\\t* **Expected Results:**\\n\\t\\t+ Reduced equipment downtime and repair costs.\\n\\t\\t+ Extended lifespan of assets and machinery.\\n\\t\\t+ Improved overall equipment effectiveness (OEE).\\n\\t\\t+ Better inventory management and parts ordering.\\n\\t\\t+ Enhanced workplace safety by preventing equipment failures.\\n5. **Employee Productivity and Engagement:**\\n\\t* **Expected Results:**\\n\\t\\t+ Automation of repetitive tasks, allowing employees to focus on more creative and strategic work.\\n\\t\\t+ Improved employee skills and knowledge through AI-driven training and development.\\n\\t\\t+ Better employee collaboration and communication through AI-powered tools.\\n\\t\\t+ Increased employee engagement and job satisfaction.\\n\\t\\t+ Enhanced talent acquisition and retention through AI-driven HR processes.\\n6. **Innovation and Competitive Advantage:**\\n\\t* **Expected Results:**\\n\\t\\t+ Development of new AI-driven products and services.\\n\\t\\t+ Improved time-to-market for new offerings.\\n\\t\\t+ Better adaptation to market changes and disruptions.\\n\\t\\t+ Enhanced brand reputation as an innovative and forward-thinking company.\\n\\t\\t+ Increased market share and competitive differentiation.\\n7. **Risk Management and Compliance:**\\n\\t* **Expected Results:**\\n\\t\\t+ Improved identification and mitigation of risks.\\n\\t\\t+ Better compliance with regulations and industry standards.\\n\\t\\t+ Enhanced detection and prevention of fraudulent activities.\\n\\t\\t+ More effective auditing and reporting processes.\\n\\t\\t+ Reduced legal and financial exposure.\\n\\nTo maximize the benefits of AI, a company should:\\n\\n* Identify and prioritize specific use cases and applications tailored to its needs.\\n* Invest in the necessary infrastructure, data, and talent to support AI initiatives.\\n* Foster a culture of innovation and experimentation.\\n* Monitor and evaluate the performance of AI implementations.\\n* Ensure responsible and ethical use of AI.\\n\\nBy doing so, a company can successfully integrate AI into its operations and achieve the expected results.'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.new_messages()[1].parts[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f4118",
   "metadata": {},
   "source": [
    "# Docling & Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1efe258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/babos/chatbot/chat_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- image -->\n",
      "\n",
      "## Docling Technical Report\n",
      "\n",
      "Version 1.0\n",
      "\n",
      "Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\n",
      "\n",
      "AI4K Group, IBM Research R¨ uschlikon, Switzerland\n",
      "\n",
      "## Abstract\n",
      "\n",
      "This technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\n",
      "\n",
      "## 1 Introduction\n",
      "\n",
      "Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\n",
      "\n",
      "With Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\n",
      "\n",
      "Here is what Docling delivers today:\n",
      "\n",
      "- · Converts PDF documents to JSON or Markdown format, stable and lightning fast\n",
      "- · Understands detailed page layout, reading order, locates figures and recovers table structures\n",
      "- · Extracts metadata from the document, such as title, authors, references and language\n",
      "- · Optionally applies OCR, e.g. for scanned PDFs\n",
      "- · Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\n",
      "- · Can leverage different accelerators (GPU, MPS, etc).\n",
      "\n",
      "## 2 Getting Started\n",
      "\n",
      "To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\n",
      "\n",
      "Docling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\n",
      "\n",
      "```\n",
      "from docling.document_converter import DocumentConverter Large\n",
      "```\n",
      "\n",
      "```\n",
      "source = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Human -Annotated Dataset for Document -Layout Analysis [...]\"\n",
      "```\n",
      "\n",
      "Optionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\n",
      "\n",
      "## 3 Processing pipeline\n",
      "\n",
      "Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\n",
      "\n",
      "## 3.1 PDF backends\n",
      "\n",
      "Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\n",
      "\n",
      "1 see huggingface.co/ds4sd/docling-models/\n",
      "\n",
      "Figure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "licensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\n",
      "\n",
      "We therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\n",
      "\n",
      "## 3.2 AI models\n",
      "\n",
      "As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\n",
      "\n",
      "## Layout Analysis Model\n",
      "\n",
      "Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\n",
      "\n",
      "The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\n",
      "\n",
      "## Table Structure Recognition\n",
      "\n",
      "The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\n",
      "\n",
      "The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\n",
      "\n",
      "## OCR\n",
      "\n",
      "Docling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\n",
      "\n",
      "We are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\n",
      "\n",
      "## 3.3 Assembly\n",
      "\n",
      "In the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\n",
      "\n",
      "## 3.4 Extensibility\n",
      "\n",
      "Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\n",
      "\n",
      "Implementations of model classes must satisfy the python Callable interface. The \\_\\_call\\_\\_ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\n",
      "\n",
      "## 4 Performance\n",
      "\n",
      "In this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\n",
      "\n",
      "If you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\n",
      "\n",
      "Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\n",
      "\n",
      "torch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\n",
      "\n",
      "Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\n",
      "\n",
      "| CPU                              | Thread budget   | native backend   | native backend   | native backend   | pypdfium backend   | pypdfium backend   | pypdfium backend   |\n",
      "|----------------------------------|-----------------|------------------|------------------|------------------|--------------------|--------------------|--------------------|\n",
      "|                                  |                 | TTS              | Pages/s          | Mem              | TTS                | Pages/s            | Mem                |\n",
      "| Apple M3 Max                     | 4               | 177 s 167 s      | 1.27 1.34        | 6.20 GB          | 103 s 92 s         | 2.18 2.45          | 2.56 GB            |\n",
      "| (16 cores) Intel(R) Xeon E5-2690 | 16 4 16         | 375 s 244 s      | 0.60 0.92        | 6.16 GB          | 239 s 143 s        | 0.94 1.57          | 2.42 GB            |\n",
      "\n",
      "## 5 Applications\n",
      "\n",
      "Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\n",
      "\n",
      "## 6 Future work and contributions\n",
      "\n",
      "Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\n",
      "\n",
      "We encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\n",
      "\n",
      "## References\n",
      "\n",
      "- [1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\n",
      "- [2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\n",
      "\n",
      "machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\n",
      "\n",
      "- [3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\n",
      "- [4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\n",
      "- [5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\n",
      "- [6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\n",
      "- [7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\n",
      "- [8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama\\_index .\n",
      "- [9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8\\_3 .\n",
      "- [10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\n",
      "- [11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\n",
      "- [12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\n",
      "- [13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.\n",
      "- [14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\n",
      "- [15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\n",
      "- [16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.\n",
      "\n",
      "## Appendix\n",
      "\n",
      "In this section, we illustrate a few examples of Docling's output in Markdown and JSON.\n",
      "\n",
      "## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\n",
      "\n",
      "## DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\n",
      "\n",
      "Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\n",
      "\n",
      "Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\n",
      "\n",
      "Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\n",
      "\n",
      "Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\n",
      "\n",
      "Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\n",
      "\n",
      "## CCS CONCEPTS\n",
      "\n",
      "· Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ;\n",
      "\n",
      "Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD '22, August 14-18, 2022, Washington, DC, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\n",
      "\n",
      "Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\n",
      "\n",
      "Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\n",
      "\n",
      "Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\n",
      "\n",
      "Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\n",
      "\n",
      "Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\n",
      "\n",
      "## ABSTRACT\n",
      "\n",
      "Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large groundtruth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\n",
      "\n",
      "## CCS CONCEPTS\n",
      "\n",
      "Æ Information systems → Document structure ; Æ Applied computing → Document analysis ; Æ Computing methodologies → Machine learning ; Computer vision ; Object detection ;\n",
      "\n",
      "Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n",
      "\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA ' 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\n",
      "\n",
      "Figure 1: Four examples of complex page layouts across different document categories\n",
      "\n",
      "## KEYWORDS\n",
      "\n",
      "PDF document conversion, layout segmentation, object-detection, data set, Machine Learning\n",
      "\n",
      "## ACM Reference Format:\n",
      "\n",
      "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "AGL Energy Limited  ABN 74 1\n",
      "\n",
      "5 061 375\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 1: Four examples of complex page layouts across different document categories\n",
      "\n",
      "## KEYWORDS\n",
      "\n",
      "PDF document conversion, layout segmentation, object-detection, data set, Machine Learning\n",
      "\n",
      "## ACMReference Format:\n",
      "\n",
      "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).\n",
      "\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "\n",
      "Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\n",
      "\n",
      "|                                                                                                        | human                                                                   | MRCNN R50 R101                                                                                                          | FRCNN R101                                                  | YOLO v5x6                                                   |\n",
      "|--------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|-------------------------------------------------------------|\n",
      "| Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All | 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83 | 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5 | 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4 | 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8 |\n",
      "\n",
      "to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s, depending on its complexity.\n",
      "\n",
      "## 5 EXPERIMENTS\n",
      "\n",
      "The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Third, Ccs label qu Oolines achienec Exanole\n",
      "\n",
      "## EXPERIMENTS\n",
      "\n",
      "chalenongayouls ground-vuth dawa such WC\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\n",
      "\n",
      "paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\n",
      "\n",
      "In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\n",
      "\n",
      "## Baselines for Object Detection\n",
      "\n",
      "In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a document.\n",
      "\n",
      "## Baselines for Object Detection\n",
      "\n",
      "mak enbrel dacuont\n",
      "\n",
      "Figure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\n",
      "\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA\n",
      "\n",
      "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "\n",
      "Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\n",
      "\n",
      "between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.\n",
      "\n",
      "of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "| class label    | Count   | % of Total   | % of Total   | % of Total   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   | triple inter-annotator mAP @0.5-0.95 (%)   |\n",
      "|----------------|---------|--------------|--------------|--------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|--------------------------------------------|\n",
      "| class label    | Count   | Train        | Test         | Val          | All                                        | Fin                                        | Man                                        | Sci                                        | Law                                        | Pat                                        | Ten                                        |\n",
      "| Caption        | 22524   | 2.04         | 1.77         | 2.32         | 84-89                                      | 40-61                                      | 86-92                                      | 94-99                                      | 95-99                                      | 69-78                                      | n/a                                        |\n",
      "| Footnote       | 6318    | 0.60         | 0.31         | 0.58         | 83-91                                      | n/a                                        | 100                                        | 62-88                                      | 85-94                                      | n/a                                        | 82-97                                      |\n",
      "| Formula        | 25027   | 2.25         | 1.90         | 2.96         | 83-85                                      | n/a                                        | n/a                                        | 84-87                                      | 86-96                                      | n/a                                        | n/a                                        |\n",
      "| List-item      | 185660  | 17.19        | 13.34        | 15.82        | 87-88                                      | 74-83                                      | 90-92                                      | 97-97                                      | 81-85                                      | 75-88                                      | 93-95                                      |\n",
      "| Page-footer    | 70878   | 6.51         | 5.58         | 6.00         | 93-94                                      | 88-90                                      | 95-96                                      | 100                                        | 92-97                                      | 100                                        | 96-98                                      |\n",
      "| Page-header    | 58022   | 5.10         | 6.70         | 5.06         | 85-89                                      | 66-76                                      | 90-94                                      | 98-100                                     | 91-92                                      | 97-99                                      | 81-86                                      |\n",
      "| Picture        | 45976   | 4.21         | 2.78         | 5.31         | 69-71                                      | 56-59                                      | 82-86                                      | 69-82                                      | 80-95                                      | 66-71                                      | 59-76                                      |\n",
      "| Section-header | 142884  | 12.60        | 15.77        | 12.85        | 83-84                                      | 76-81                                      | 90-92                                      | 94-95                                      | 87-94                                      | 69-73                                      | 78-86                                      |\n",
      "| Table          | 34733   | 3.20         | 2.27         | 3.60         | 77-81                                      | 75-80                                      | 83-86                                      | 98-99                                      | 58-80                                      | 79-84                                      | 70-85                                      |\n",
      "| Text           | 510377  | 45.82        | 49.28        | 45.00        | 84-86                                      | 81-86                                      | 88-93                                      | 89-93                                      | 87-92                                      | 71-79                                      | 87-95                                      |\n",
      "| Title          | 5071    | 0.47         | 0.30         | 0.50         | 60-72                                      | 24-63                                      | 50-63                                      | 94-100                                     | 82-96                                      | 68-79                                      | 24-56                                      |\n",
      "| Total          | 1107470 | 941123       | 99816        | 66531        | 82-83                                      | 71-74                                      | 79-81                                      | 89-94                                      | 86-91                                      | 71-76                                      | 68-85                                      |\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "include publication repositories such as arXiv\n",
      "\n",
      "Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row \"Total\") in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-\n",
      "\n",
      "annotated pages, from which we obtain accuracy ranges.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "|                       |         | %of Total   | %of Total   | %of Total   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   | triple inter- annotator mAP @ 0.5-0.95 (%)   |\n",
      "|-----------------------|---------|-------------|-------------|-------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|----------------------------------------------|\n",
      "| class label           | Count   | Train       | Test        | Val         | All                                          | Fin                                          | Man                                          | Sci                                          | Law                                          | Pat                                          | Ten                                          |\n",
      "| Caption               | 22524   | 2.04        | 1.77        | 2.32        | 84-89                                        | 40-61                                        | 86-92                                        | 94-99                                        | 95-99                                        | 69-78                                        | n/a                                          |\n",
      "| Footnote              | 6318    | 0.60        | 0.31        | 0.58        | 83-91                                        | n/a                                          | 100                                          | 62-88                                        | 85-94                                        | n/a                                          | 82-97                                        |\n",
      "| Formula               | 25027   | 2.25        | 1.90        | 2.96        | 83-85                                        | n/a                                          | n/a                                          | 84-87                                        | 86-96                                        | n/a                                          | n/a                                          |\n",
      "| List-item             | 185660  | 17.19       | 13.34       | 15.82       | 87-88                                        | 74-83                                        | 90-92                                        | 97-97                                        | 81-85                                        | 75-88                                        | 93-95                                        |\n",
      "| Page- footer          | 70878   | 6.51        | 5.58        | 6.00        | 93-94                                        | 88-90                                        | 95-96                                        | 100                                          | 92-97                                        | 100                                          | 96-98                                        |\n",
      "| Page- header offices, | 58022   | 5.10        | 6.70        | 5.06        | 85-89                                        | 66-76                                        | 90-94                                        | 98-100                                       | 91-92                                        | 97-99                                        | 81-86                                        |\n",
      "| Picture               | 45976   | 4.21        | 2.78        | 5.31        | 69-71                                        | 56-59                                        | 82-86                                        | 69-82                                        | 80-95                                        | 66-71                                        | 59-76                                        |\n",
      "| Section- header not   | 142884  | 12.60       | 15.77       | 12.85       | 83-84                                        | 76-81                                        | 90-92                                        | 94-95                                        | 87-94                                        | 69-73                                        | 78-86                                        |\n",
      "| Table                 | 34733   | 3.20        | 2.27        | 3.60        | 77-81                                        | 75-80                                        | 83-86                                        | 98-99                                        | 58-80                                        | 79-84                                        | 70-85                                        |\n",
      "| Text                  | 510377  | 45.82       | 49.28       | 45.00       | 84-86                                        | 81-86                                        | 88-93                                        | 89-93                                        | 87-92                                        | 71-79                                        | 87-95                                        |\n",
      "| Title [22], a         | 5071    | 0.47        | 0.30        | 0.50        | 60-72                                        | 24-63                                        | 50-63                                        | 94-100                                       | 82-96                                        | 68-79                                        | 24-56                                        |\n",
      "| Total in-             | 1107470 | 941123      | 99816       | 66531       | 82-83                                        | 71-74                                        | 79-81                                        | 89-94                                        | 86-91                                        | 71-76                                        | 68-85                                        |\n",
      "\n",
      "3\n",
      "\n",
      ",\n",
      "\n",
      "government offices,\n",
      "\n",
      "We reviewed the col-\n",
      "\n",
      ",\n",
      "\n",
      "Page-\n",
      "\n",
      "Title and\n",
      "\n",
      ".\n",
      "\n",
      "page. Specificity ensures that the choice of label is not ambiguous,\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "we distributed the annotation workload and performed continuous be annotated. We refrained from class labels that are very specific\n",
      "\n",
      "only. For phases three and four, a group of 40 dedicated annotators while coverage ensures that all meaningful items on a page can\n",
      "\n",
      "quality controls. Phase one and two required a small team of experts to a document category, such as\n",
      "\n",
      "Abstract in the\n",
      "\n",
      "Scientific Articles were assembled and supervised.\n",
      "\n",
      "category. We also avoided class labels that are tightly linked to the\n",
      "\n",
      "Phase 1: Data selection and preparation.\n",
      "\n",
      "Our inclusion cri-\n",
      "\n",
      "Author\n",
      "\n",
      "Affiliation\n",
      "\n",
      "teria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\n",
      "\n",
      "semantics of the text. Labels such as and\n",
      "\n",
      ",\n",
      "\n",
      "as seen\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "source = \"https://arxiv.org/pdf/2408.09869\"  # PDF path or URL\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source)\n",
    "print(result.document.export_to_markdown())  # output: \"### Docling Technical Report[...]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b37689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2938 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MAX_TOKENS = 256  # set to a small number for illustrative purposes\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL_ID)\n",
    "\n",
    "chunker = HybridChunker(\n",
    "    tokenizer=tokenizer,  # instance or model name, defaults to \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    max_tokens=MAX_TOKENS,  # optional, by default derived from `tokenizer`\n",
    "    merge_peers=True,  # optional, defaults to True\n",
    ")\n",
    "chunk_iter = chunker.chunk(dl_doc=result.document)\n",
    "chunks = list(chunk_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f252b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 0 ===\n",
      "chunk.text (105 tokens):\n",
      "'Version 1.0\\nChristoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\\nAI4K Group, IBM Research R¨ uschlikon, Switzerland'\n",
      "chunker.serialize(chunk) (109 tokens):\n",
      "'Docling Technical Report\\nVersion 1.0\\nChristoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\\nAI4K Group, IBM Research R¨ uschlikon, Switzerland'\n",
      "\n",
      "=== 1 ===\n",
      "chunk.text (89 tokens):\n",
      "'This technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.'\n",
      "chunker.serialize(chunk) (90 tokens):\n",
      "'Abstract\\nThis technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.'\n",
      "\n",
      "=== 2 ===\n",
      "chunk.text (250 tokens):\n",
      "'Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\\nWith Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\\nHere is what Docling delivers today:'\n",
      "chunker.serialize(chunk) (252 tokens):\n",
      "'1 Introduction\\nConverting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\\nWith Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\\nHere is what Docling delivers today:'\n",
      "\n",
      "=== 3 ===\n",
      "chunk.text (120 tokens):\n",
      "'· Converts PDF documents to JSON or Markdown format, stable and lightning fast\\n· Understands detailed page layout, reading order, locates figures and recovers table structures\\n· Extracts metadata from the document, such as title, authors, references and language\\n· Optionally applies OCR, e.g. for scanned PDFs\\n· Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\\n· Can leverage different accelerators (GPU, MPS, etc).'\n",
      "chunker.serialize(chunk) (122 tokens):\n",
      "'1 Introduction\\n· Converts PDF documents to JSON or Markdown format, stable and lightning fast\\n· Understands detailed page layout, reading order, locates figures and recovers table structures\\n· Extracts metadata from the document, such as title, authors, references and language\\n· Optionally applies OCR, e.g. for scanned PDFs\\n· Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\\n· Can leverage different accelerators (GPU, MPS, etc).'\n",
      "\n",
      "=== 4 ===\n",
      "chunk.text (167 tokens):\n",
      "'To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\\nDocling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\\n```\\nfrom docling.document_converter import DocumentConverter Large\\n```'\n",
      "chunker.serialize(chunk) (170 tokens):\n",
      "'2 Getting Started\\nTo use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\\nDocling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\\n```\\nfrom docling.document_converter import DocumentConverter Large\\n```'\n",
      "\n",
      "=== 5 ===\n",
      "chunk.text (184 tokens):\n",
      "'```\\nsource = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Human -Annotated Dataset for Document -Layout Analysis [...]\"\\n```\\nOptionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.'\n",
      "chunker.serialize(chunk) (187 tokens):\n",
      "'2 Getting Started\\n```\\nsource = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Human -Annotated Dataset for Document -Layout Analysis [...]\"\\n```\\nOptionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.'\n",
      "\n",
      "=== 6 ===\n",
      "chunk.text (160 tokens):\n",
      "'Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.'\n",
      "chunker.serialize(chunk) (163 tokens):\n",
      "'3 Processing pipeline\\nDocling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.'\n",
      "\n",
      "=== 7 ===\n",
      "chunk.text (197 tokens):\n",
      "\"Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\\n1 see huggingface.co/ds4sd/docling-models/\\nFigure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\\nlicensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\"\n",
      "chunker.serialize(chunk) (204 tokens):\n",
      "\"3.1 PDF backends\\nTwo basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\\n1 see huggingface.co/ds4sd/docling-models/\\nFigure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\\nlicensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\"\n",
      "\n",
      "=== 8 ===\n",
      "chunk.text (107 tokens):\n",
      "'We therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.'\n",
      "chunker.serialize(chunk) (114 tokens):\n",
      "'3.1 PDF backends\\nWe therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.'\n",
      "\n",
      "=== 9 ===\n",
      "chunk.text (134 tokens):\n",
      "'As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.'\n",
      "chunker.serialize(chunk) (139 tokens):\n",
      "'3.2 AI models\\nAs part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.'\n",
      "\n",
      "=== 10 ===\n",
      "chunk.text (182 tokens):\n",
      "'Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\\nThe Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.'\n",
      "chunker.serialize(chunk) (185 tokens):\n",
      "'Layout Analysis Model\\nOur layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\\nThe Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.'\n",
      "\n",
      "=== 11 ===\n",
      "chunk.text (237 tokens):\n",
      "'The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\\nThe Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.'\n",
      "chunker.serialize(chunk) (240 tokens):\n",
      "'Table Structure Recognition\\nThe TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\\nThe Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.'\n",
      "\n",
      "=== 12 ===\n",
      "chunk.text (144 tokens):\n",
      "'Docling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\\nWe are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.'\n",
      "chunker.serialize(chunk) (146 tokens):\n",
      "'OCR\\nDocling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\\nWe are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.'\n",
      "\n",
      "=== 13 ===\n",
      "chunk.text (119 tokens):\n",
      "'In the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.'\n",
      "chunker.serialize(chunk) (123 tokens):\n",
      "'3.3 Assembly\\nIn the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.'\n",
      "\n",
      "=== 14 ===\n",
      "chunk.text (198 tokens):\n",
      "'Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\\nImplementations of model classes must satisfy the python Callable interface. The __call__ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.'\n",
      "chunker.serialize(chunk) (204 tokens):\n",
      "'3.4 Extensibility\\nDocling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\\nImplementations of model classes must satisfy the python Callable interface. The __call__ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.'\n",
      "\n",
      "=== 15 ===\n",
      "chunk.text (234 tokens):\n",
      "'In this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\\nIf you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.'\n",
      "chunker.serialize(chunk) (236 tokens):\n",
      "'4 Performance\\nIn this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\\nIf you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.'\n",
      "\n",
      "=== 16 ===\n",
      "chunk.text (160 tokens):\n",
      "'Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\\ntorch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\\n\\nTable 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.'\n",
      "chunker.serialize(chunk) (162 tokens):\n",
      "'4 Performance\\nEstablishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\\ntorch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\\n\\nTable 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.'\n",
      "\n",
      "=== 17 ===\n",
      "chunk.text (252 tokens):\n",
      "'Apple M3 Max, Thread budget. = 4. Apple M3 Max, native backend.TTS = 177 s 167 s. Apple M3 Max, native backend.Pages/s = 1.27 1.34. Apple M3 Max, native backend.Mem = 6.20 GB. Apple M3 Max, pypdfium backend.TTS = 103 s 92 s. Apple M3 Max, pypdfium backend.Pages/s = 2.18 2.45. Apple M3 Max, pypdfium backend.Mem = 2.56 GB. (16 cores) Intel(R) Xeon E5-2690, Thread budget. = 16 4 16. (16 cores) Intel(R) Xeon E5-2690, native backend.TTS = 375 s 244 s. (16 cores) Intel(R) Xeon E5-2690, native backend.Pages/s = 0.60 0.92. (16 cores) Intel(R) Xeon E5-2690, native backend.Mem = 6.16 GB. (16 cores) Intel(R) Xeon E5-2690, pypdfium'\n",
      "chunker.serialize(chunk) (254 tokens):\n",
      "'4 Performance\\nApple M3 Max, Thread budget. = 4. Apple M3 Max, native backend.TTS = 177 s 167 s. Apple M3 Max, native backend.Pages/s = 1.27 1.34. Apple M3 Max, native backend.Mem = 6.20 GB. Apple M3 Max, pypdfium backend.TTS = 103 s 92 s. Apple M3 Max, pypdfium backend.Pages/s = 2.18 2.45. Apple M3 Max, pypdfium backend.Mem = 2.56 GB. (16 cores) Intel(R) Xeon E5-2690, Thread budget. = 16 4 16. (16 cores) Intel(R) Xeon E5-2690, native backend.TTS = 375 s 244 s. (16 cores) Intel(R) Xeon E5-2690, native backend.Pages/s = 0.60 0.92. (16 cores) Intel(R) Xeon E5-2690, native backend.Mem = 6.16 GB. (16 cores) Intel(R) Xeon E5-2690, pypdfium'\n",
      "\n",
      "=== 18 ===\n",
      "chunk.text (75 tokens):\n",
      "'backend.TTS = 239 s 143 s. (16 cores) Intel(R) Xeon E5-2690, pypdfium backend.Pages/s = 0.94 1.57. (16 cores) Intel(R) Xeon E5-2690, pypdfium backend.Mem = 2.42 GB'\n",
      "chunker.serialize(chunk) (77 tokens):\n",
      "'4 Performance\\nbackend.TTS = 239 s 143 s. (16 cores) Intel(R) Xeon E5-2690, pypdfium backend.Pages/s = 0.94 1.57. (16 cores) Intel(R) Xeon E5-2690, pypdfium backend.Mem = 2.42 GB'\n",
      "\n",
      "=== 19 ===\n",
      "chunk.text (243 tokens):\n",
      "\"Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\"\n",
      "chunker.serialize(chunk) (245 tokens):\n",
      "\"5 Applications\\nThanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\"\n",
      "\n",
      "=== 20 ===\n",
      "chunk.text (185 tokens):\n",
      "'Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\\nWe encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.'\n",
      "chunker.serialize(chunk) (190 tokens):\n",
      "'6 Future work and contributions\\nDocling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\\nWe encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.'\n",
      "\n",
      "=== 21 ===\n",
      "chunk.text (53 tokens):\n",
      "'[1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.'\n",
      "chunker.serialize(chunk) (54 tokens):\n",
      "'References\\n[1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.'\n",
      "\n",
      "=== 22 ===\n",
      "chunk.text (255 tokens):\n",
      "'[2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster'\n",
      "chunker.serialize(chunk) (256 tokens):\n",
      "'References\\n[2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster'\n",
      "\n",
      "=== 23 ===\n",
      "chunk.text (90 tokens):\n",
      "\"machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\"\n",
      "chunker.serialize(chunk) (91 tokens):\n",
      "\"References\\nmachine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\"\n",
      "\n",
      "=== 24 ===\n",
      "chunk.text (250 tokens):\n",
      "'[3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\\n[4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\\n[5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\\n[6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\\n[7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .'\n",
      "chunker.serialize(chunk) (251 tokens):\n",
      "'References\\n[3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\\n[4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\\n[5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\\n[6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\\n[7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .'\n",
      "\n",
      "=== 25 ===\n",
      "chunk.text (204 tokens):\n",
      "'[8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index .\\n[9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8_3 .'\n",
      "chunker.serialize(chunk) (205 tokens):\n",
      "'References\\n[8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index .\\n[9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8_3 .'\n",
      "\n",
      "=== 26 ===\n",
      "chunk.text (193 tokens):\n",
      "'[10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .'\n",
      "chunker.serialize(chunk) (194 tokens):\n",
      "'References\\n[10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .'\n",
      "\n",
      "=== 27 ===\n",
      "chunk.text (248 tokens):\n",
      "'[11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\\n[12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\\n[13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.'\n",
      "chunker.serialize(chunk) (249 tokens):\n",
      "'References\\n[11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\\n[12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\\n[13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.'\n",
      "\n",
      "=== 28 ===\n",
      "chunk.text (146 tokens):\n",
      "'[14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\\n[15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\\n[16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.'\n",
      "chunker.serialize(chunk) (147 tokens):\n",
      "'References\\n[14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\\n[15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\\n[16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.'\n",
      "\n",
      "=== 29 ===\n",
      "chunk.text (22 tokens):\n",
      "\"In this section, we illustrate a few examples of Docling's output in Markdown and JSON.\"\n",
      "chunker.serialize(chunk) (23 tokens):\n",
      "\"Appendix\\nIn this section, we illustrate a few examples of Docling's output in Markdown and JSON.\"\n",
      "\n",
      "=== 30 ===\n",
      "chunk.text (102 tokens):\n",
      "'Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\\nChristoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\\nMichele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\\nAhmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\\nPeter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com'\n",
      "chunker.serialize(chunk) (120 tokens):\n",
      "'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\\nBirgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\\nChristoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\\nMichele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\\nAhmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\\nPeter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com'\n",
      "\n",
      "=== 31 ===\n",
      "chunk.text (255 tokens):\n",
      "'Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10%'\n",
      "chunker.serialize(chunk) (256 tokens):\n",
      "'ABSTRACT\\nAccurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10%'\n",
      "\n",
      "=== 32 ===\n",
      "chunk.text (69 tokens):\n",
      "'behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.'\n",
      "chunker.serialize(chunk) (70 tokens):\n",
      "'ABSTRACT\\nbehind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.'\n",
      "\n",
      "=== 33 ===\n",
      "chunk.text (246 tokens):\n",
      "\"· Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ;\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD '22, August 14-18, 2022, Washington, DC, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\\nBirgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\\nChristoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\\nMichele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\"\n",
      "chunker.serialize(chunk) (249 tokens):\n",
      "\"CCS CONCEPTS\\n· Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ;\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD '22, August 14-18, 2022, Washington, DC, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\\nBirgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\\nChristoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\\nMichele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\"\n",
      "\n",
      "=== 34 ===\n",
      "chunk.text (40 tokens):\n",
      "'Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\\nPeter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com'\n",
      "chunker.serialize(chunk) (43 tokens):\n",
      "'CCS CONCEPTS\\nAhmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\\nPeter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com'\n",
      "\n",
      "=== 35 ===\n",
      "chunk.text (255 tokens):\n",
      "'Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large groundtruth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10%'\n",
      "chunker.serialize(chunk) (256 tokens):\n",
      "'ABSTRACT\\nAccurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large groundtruth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10%'\n",
      "\n",
      "=== 36 ===\n",
      "chunk.text (69 tokens):\n",
      "'behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.'\n",
      "chunker.serialize(chunk) (70 tokens):\n",
      "'ABSTRACT\\nbehind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.'\n",
      "\n",
      "=== 37 ===\n",
      "chunk.text (195 tokens):\n",
      "\"Æ Information systems → Document structure ; Æ Applied computing → Document analysis ; Æ Computing methodologies → Machine learning ; Computer vision ; Object detection ;\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\\nKDD '22, August 14-18, 2022, Washington, DC, USA ' 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\\nFigure 1: Four examples of complex page layouts across different document categories\"\n",
      "chunker.serialize(chunk) (198 tokens):\n",
      "\"CCS CONCEPTS\\nÆ Information systems → Document structure ; Æ Applied computing → Document analysis ; Æ Computing methodologies → Machine learning ; Computer vision ; Object detection ;\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\\nKDD '22, August 14-18, 2022, Washington, DC, USA ' 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\\nFigure 1: Four examples of complex page layouts across different document categories\"\n",
      "\n",
      "=== 38 ===\n",
      "chunk.text (17 tokens):\n",
      "'PDF document conversion, layout segmentation, object-detection, data set, Machine Learning'\n",
      "chunker.serialize(chunk) (19 tokens):\n",
      "'KEYWORDS\\nPDF document conversion, layout segmentation, object-detection, data set, Machine Learning'\n",
      "\n",
      "=== 39 ===\n",
      "chunk.text (151 tokens):\n",
      "\"Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\\nAGL Energy Limited  ABN 74 1\\n5 061 375\\nFigure 1: Four examples of complex page layouts across different document categories\"\n",
      "chunker.serialize(chunk) (156 tokens):\n",
      "\"ACM Reference Format:\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\\nAGL Energy Limited  ABN 74 1\\n5 061 375\\nFigure 1: Four examples of complex page layouts across different document categories\"\n",
      "\n",
      "=== 40 ===\n",
      "chunk.text (17 tokens):\n",
      "'PDF document conversion, layout segmentation, object-detection, data set, Machine Learning'\n",
      "chunker.serialize(chunk) (19 tokens):\n",
      "'KEYWORDS\\nPDF document conversion, layout segmentation, object-detection, data set, Machine Learning'\n",
      "\n",
      "=== 41 ===\n",
      "chunk.text (127 tokens):\n",
      "\"Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\\n1 INTRODUCTION\"\n",
      "chunker.serialize(chunk) (133 tokens):\n",
      "\"ACMReference Format:\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\\n1 INTRODUCTION\"\n",
      "\n",
      "=== 42 ===\n",
      "chunk.text (210 tokens):\n",
      "'Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).'\n",
      "chunker.serialize(chunk) (216 tokens):\n",
      "'ACMReference Format:\\nDespite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).'\n",
      "\n",
      "=== 43 ===\n",
      "chunk.text (188 tokens):\n",
      "\"KDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\nTable 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\"\n",
      "chunker.serialize(chunk) (194 tokens):\n",
      "\"ACMReference Format:\\nKDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\nTable 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\"\n",
      "\n",
      "=== 44 ===\n",
      "chunk.text (249 tokens):\n",
      "'Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All, human = 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83. Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All, MRCNN R50 R101 = 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5. Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All, FRCNN R101 = 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4. Caption Footnote Formula List-item Page-footer'\n",
      "chunker.serialize(chunk) (255 tokens):\n",
      "'ACMReference Format:\\nCaption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All, human = 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83. Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All, MRCNN R50 R101 = 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5. Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All, FRCNN R101 = 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4. Caption Footnote Formula List-item Page-footer'\n",
      "\n",
      "=== 45 ===\n",
      "chunk.text (55 tokens):\n",
      "'Page-header Picture Section-header Table Text Title All, YOLO v5x6 = 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8'\n",
      "chunker.serialize(chunk) (61 tokens):\n",
      "'ACMReference Format:\\nPage-header Picture Section-header Table Text Title All, YOLO v5x6 = 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8'\n",
      "\n",
      "=== 46 ===\n",
      "chunk.text (250 tokens):\n",
      "'to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s,'\n",
      "chunker.serialize(chunk) (256 tokens):\n",
      "'ACMReference Format:\\nto avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s,'\n",
      "\n",
      "=== 47 ===\n",
      "chunk.text (5 tokens):\n",
      "'depending on its complexity.'\n",
      "chunker.serialize(chunk) (11 tokens):\n",
      "'ACMReference Format:\\ndepending on its complexity.'\n",
      "\n",
      "=== 48 ===\n",
      "chunk.text (138 tokens):\n",
      "'The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\\nThird, Ccs label qu Oolines achienec Exanole'\n",
      "chunker.serialize(chunk) (140 tokens):\n",
      "'5 EXPERIMENTS\\nThe primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\\nThird, Ccs label qu Oolines achienec Exanole'\n",
      "\n",
      "=== 49 ===\n",
      "chunk.text (205 tokens):\n",
      "'chalenongayouls ground-vuth dawa such WC\\nFigure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\\npaper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\\nIn this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].'\n",
      "chunker.serialize(chunk) (206 tokens):\n",
      "'EXPERIMENTS\\nchalenongayouls ground-vuth dawa such WC\\nFigure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\\npaper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\\nIn this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].'\n",
      "\n",
      "=== 50 ===\n",
      "chunk.text (251 tokens):\n",
      "'In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a'\n",
      "chunker.serialize(chunk) (256 tokens):\n",
      "'Baselines for Object Detection\\nIn Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a'\n",
      "\n",
      "=== 51 ===\n",
      "chunk.text (223 tokens):\n",
      "\"document.\\nmak enbrel dacuont\\nFigure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\\nKDD '22, August 14-18, 2022, Washington, DC, USA\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\\nbetween pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.\\nof row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric\"\n",
      "chunker.serialize(chunk) (228 tokens):\n",
      "\"Baselines for Object Detection\\ndocument.\\nmak enbrel dacuont\\nFigure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\\nKDD '22, August 14-18, 2022, Washington, DC, USA\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\\nbetween pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.\\nof row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric\"\n",
      "\n",
      "=== 52 ===\n",
      "chunk.text (251 tokens):\n",
      "'Caption, Count.Count = 22524. Caption, % of Total.Train = 2.04. Caption, % of Total.Test = 1.77. Caption, % of Total.Val = 2.32. Caption, triple inter-annotator mAP @0.5-0.95 (%).All = 84-89. Caption, triple inter-annotator mAP @0.5-0.95 (%).Fin = 40-61. Caption, triple inter-annotator mAP @0.5-0.95 (%).Man = 86-92. Caption, triple inter-annotator mAP @0.5-0.95 (%).Sci = 94-99. Caption, triple inter-annotator mAP @0.5-0.95 (%).Law = 95-99. Caption, triple inter-annotator mAP @0.5-0.95 (%).Pat = 69-78. Caption, triple inter-annotator mAP @0.5-0.95 (%).Ten = n/a. Footnote, Count.Count'\n",
      "chunker.serialize(chunk) (256 tokens):\n",
      "'Baselines for Object Detection\\nCaption, Count.Count = 22524. Caption, % of Total.Train = 2.04. Caption, % of Total.Test = 1.77. Caption, % of Total.Val = 2.32. Caption, triple inter-annotator mAP @0.5-0.95 (%).All = 84-89. Caption, triple inter-annotator mAP @0.5-0.95 (%).Fin = 40-61. Caption, triple inter-annotator mAP @0.5-0.95 (%).Man = 86-92. Caption, triple inter-annotator mAP @0.5-0.95 (%).Sci = 94-99. Caption, triple inter-annotator mAP @0.5-0.95 (%).Law = 95-99. Caption, triple inter-annotator mAP @0.5-0.95 (%).Pat = 69-78. Caption, triple inter-annotator mAP @0.5-0.95 (%).Ten = n/a. Footnote, Count.Count'\n",
      "\n",
      "=== 53 ===\n",
      "chunk.text (250 tokens):\n",
      "'= 6318. Footnote, % of Total.Train = 0.60. Footnote, % of Total.Test = 0.31. Footnote, % of Total.Val = 0.58. Footnote, triple inter-annotator mAP @0.5-0.95 (%).All = 83-91. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Fin = n/a. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Man = 100. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Sci = 62-88. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Law = 85-94. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Pat = n/a. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Ten = 82-97. Formula, Count.Count = 25027. Formula, % of'\n",
      "chunker.serialize(chunk) (255 tokens):\n",
      "'Baselines for Object Detection\\n= 6318. Footnote, % of Total.Train = 0.60. Footnote, % of Total.Test = 0.31. Footnote, % of Total.Val = 0.58. Footnote, triple inter-annotator mAP @0.5-0.95 (%).All = 83-91. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Fin = n/a. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Man = 100. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Sci = 62-88. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Law = 85-94. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Pat = n/a. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Ten = 82-97. Formula, Count.Count = 25027. Formula, % of'\n",
      "\n",
      "=== 54 ===\n",
      "chunk.text (251 tokens):\n",
      "'Total.Train = 2.25. Formula, % of Total.Test = 1.90. Formula, % of Total.Val = 2.96. Formula, triple inter-annotator mAP @0.5-0.95 (%).All = 83-85. Formula, triple inter-annotator mAP @0.5-0.95 (%).Fin = n/a. Formula, triple inter-annotator mAP @0.5-0.95 (%).Man = n/a. Formula, triple inter-annotator mAP @0.5-0.95 (%).Sci = 84-87. Formula, triple inter-annotator mAP @0.5-0.95 (%).Law = 86-96. Formula, triple inter-annotator mAP @0.5-0.95 (%).Pat = n/a. Formula, triple inter-annotator mAP @0.5-0.95 (%).Ten = n/a. List-item, Count.Count = 185660. List-item, % of Total.Train = 17.19. List-item, %'\n",
      "chunker.serialize(chunk) (256 tokens):\n",
      "'Baselines for Object Detection\\nTotal.Train = 2.25. Formula, % of Total.Test = 1.90. Formula, % of Total.Val = 2.96. Formula, triple inter-annotator mAP @0.5-0.95 (%).All = 83-85. Formula, triple inter-annotator mAP @0.5-0.95 (%).Fin = n/a. Formula, triple inter-annotator mAP @0.5-0.95 (%).Man = n/a. Formula, triple inter-annotator mAP @0.5-0.95 (%).Sci = 84-87. Formula, triple inter-annotator mAP @0.5-0.95 (%).Law = 86-96. Formula, triple inter-annotator mAP @0.5-0.95 (%).Pat = n/a. Formula, triple inter-annotator mAP @0.5-0.95 (%).Ten = n/a. List-item, Count.Count = 185660. List-item, % of Total.Train = 17.19. List-item, %'\n",
      "\n",
      "=== 55 ===\n",
      "chunk.text (251 tokens):\n",
      "'of Total.Test = 13.34. List-item, % of Total.Val = 15.82. List-item, triple inter-annotator mAP @0.5-0.95 (%).All = 87-88. List-item, triple inter-annotator mAP @0.5-0.95 (%).Fin = 74-83. List-item, triple inter-annotator mAP @0.5-0.95 (%).Man = 90-92. List-item, triple inter-annotator mAP @0.5-0.95 (%).Sci = 97-97. List-item, triple inter-annotator mAP @0.5-0.95 (%).Law = 81-85. List-item, triple inter-annotator mAP @0.5-0.95 (%).Pat = 75-88. List-item, triple inter-annotator mAP @0.5-0.95 (%).Ten = 93-95. Page-footer, Count.Count = 70878. Page-footer, % of Total.Train ='\n",
      "chunker.serialize(chunk) (256 tokens):\n",
      "'Baselines for Object Detection\\nof Total.Test = 13.34. List-item, % of Total.Val = 15.82. List-item, triple inter-annotator mAP @0.5-0.95 (%).All = 87-88. List-item, triple inter-annotator mAP @0.5-0.95 (%).Fin = 74-83. List-item, triple inter-annotator mAP @0.5-0.95 (%).Man = 90-92. List-item, triple inter-annotator mAP @0.5-0.95 (%).Sci = 97-97. List-item, triple inter-annotator mAP @0.5-0.95 (%).Law = 81-85. List-item, triple inter-annotator mAP @0.5-0.95 (%).Pat = 75-88. List-item, triple inter-annotator mAP @0.5-0.95 (%).Ten = 93-95. Page-footer, Count.Count = 70878. Page-footer, % of Total.Train ='\n",
      "\n",
      "=== 56 ===\n",
      "chunk.text (251 tokens):\n",
      "'6.51. Page-footer, % of Total.Test = 5.58. Page-footer, % of Total.Val = 6.00. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).All = 93-94. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Fin = 88-90. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Man = 95-96. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Sci = 100. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Law = 92-97. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Pat = 100. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Ten = 96-98. Page-header, Count.Count = 58022.'\n",
      "chunker.serialize(chunk) (256 tokens):\n",
      "'Baselines for Object Detection\\n6.51. Page-footer, % of Total.Test = 5.58. Page-footer, % of Total.Val = 6.00. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).All = 93-94. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Fin = 88-90. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Man = 95-96. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Sci = 100. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Law = 92-97. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Pat = 100. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Ten = 96-98. Page-header, Count.Count = 58022.'\n",
      "\n",
      "=== 57 ===\n",
      "chunk.text (251 tokens):\n",
      "'Page-header, % of Total.Train = 5.10. Page-header, % of Total.Test = 6.70. Page-header, % of Total.Val = 5.06. Page-header, triple inter-annotator mAP @0.5-0.95 (%).All = 85-89. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Fin = 66-76. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Man = 90-94. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Sci = 98-100. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Law = 91-92. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Pat = 97-99. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Ten = 81-86. Picture, Count.Count ='\n",
      "chunker.serialize(chunk) (256 tokens):\n",
      "'Baselines for Object Detection\\nPage-header, % of Total.Train = 5.10. Page-header, % of Total.Test = 6.70. Page-header, % of Total.Val = 5.06. Page-header, triple inter-annotator mAP @0.5-0.95 (%).All = 85-89. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Fin = 66-76. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Man = 90-94. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Sci = 98-100. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Law = 91-92. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Pat = 97-99. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Ten = 81-86. Picture, Count.Count ='\n",
      "\n",
      "=== 58 ===\n",
      "chunk.text (249 tokens):\n",
      "'45976. Picture, % of Total.Train = 4.21. Picture, % of Total.Test = 2.78. Picture, % of Total.Val = 5.31. Picture, triple inter-annotator mAP @0.5-0.95 (%).All = 69-71. Picture, triple inter-annotator mAP @0.5-0.95 (%).Fin = 56-59. Picture, triple inter-annotator mAP @0.5-0.95 (%).Man = 82-86. Picture, triple inter-annotator mAP @0.5-0.95 (%).Sci = 69-82. Picture, triple inter-annotator mAP @0.5-0.95 (%).Law = 80-95. Picture, triple inter-annotator mAP @0.5-0.95 (%).Pat = 66-71. Picture, triple inter-annotator mAP @0.5-0.95 (%).Ten = 59-76. Section-header, Count.Count = 142884. Section-header, % of'\n",
      "chunker.serialize(chunk) (254 tokens):\n",
      "'Baselines for Object Detection\\n45976. Picture, % of Total.Train = 4.21. Picture, % of Total.Test = 2.78. Picture, % of Total.Val = 5.31. Picture, triple inter-annotator mAP @0.5-0.95 (%).All = 69-71. Picture, triple inter-annotator mAP @0.5-0.95 (%).Fin = 56-59. Picture, triple inter-annotator mAP @0.5-0.95 (%).Man = 82-86. Picture, triple inter-annotator mAP @0.5-0.95 (%).Sci = 69-82. Picture, triple inter-annotator mAP @0.5-0.95 (%).Law = 80-95. Picture, triple inter-annotator mAP @0.5-0.95 (%).Pat = 66-71. Picture, triple inter-annotator mAP @0.5-0.95 (%).Ten = 59-76. Section-header, Count.Count = 142884. Section-header, % of'\n",
      "\n",
      "=== 59 ===\n",
      "chunk.text (251 tokens):\n",
      "'Total.Train = 12.60. Section-header, % of Total.Test = 15.77. Section-header, % of Total.Val = 12.85. Section-header, triple inter-annotator mAP @0.5-0.95 (%).All = 83-84. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Fin = 76-81. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Man = 90-92. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Sci = 94-95. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Law = 87-94. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Pat = 69-73. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Ten = 78-86. Table, Count.Count = 34733. Table,'\n",
      "chunker.serialize(chunk) (256 tokens):\n",
      "'Baselines for Object Detection\\nTotal.Train = 12.60. Section-header, % of Total.Test = 15.77. Section-header, % of Total.Val = 12.85. Section-header, triple inter-annotator mAP @0.5-0.95 (%).All = 83-84. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Fin = 76-81. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Man = 90-92. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Sci = 94-95. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Law = 87-94. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Pat = 69-73. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Ten = 78-86. Table, Count.Count = 34733. Table,'\n",
      "\n",
      "=== 60 ===\n",
      "chunk.text (249 tokens):\n",
      "'% of Total.Train = 3.20. Table, % of Total.Test = 2.27. Table, % of Total.Val = 3.60. Table, triple inter-annotator mAP @0.5-0.95 (%).All = 77-81. Table, triple inter-annotator mAP @0.5-0.95 (%).Fin = 75-80. Table, triple inter-annotator mAP @0.5-0.95 (%).Man = 83-86. Table, triple inter-annotator mAP @0.5-0.95 (%).Sci = 98-99. Table, triple inter-annotator mAP @0.5-0.95 (%).Law = 58-80. Table, triple inter-annotator mAP @0.5-0.95 (%).Pat = 79-84. Table, triple inter-annotator mAP @0.5-0.95 (%).Ten = 70-85. Text, Count.Count = 510377. Text, % of Total.Train = 45.82. Text, % of'\n",
      "chunker.serialize(chunk) (254 tokens):\n",
      "'Baselines for Object Detection\\n% of Total.Train = 3.20. Table, % of Total.Test = 2.27. Table, % of Total.Val = 3.60. Table, triple inter-annotator mAP @0.5-0.95 (%).All = 77-81. Table, triple inter-annotator mAP @0.5-0.95 (%).Fin = 75-80. Table, triple inter-annotator mAP @0.5-0.95 (%).Man = 83-86. Table, triple inter-annotator mAP @0.5-0.95 (%).Sci = 98-99. Table, triple inter-annotator mAP @0.5-0.95 (%).Law = 58-80. Table, triple inter-annotator mAP @0.5-0.95 (%).Pat = 79-84. Table, triple inter-annotator mAP @0.5-0.95 (%).Ten = 70-85. Text, Count.Count = 510377. Text, % of Total.Train = 45.82. Text, % of'\n",
      "\n",
      "=== 61 ===\n",
      "chunk.text (251 tokens):\n",
      "'Total.Test = 49.28. Text, % of Total.Val = 45.00. Text, triple inter-annotator mAP @0.5-0.95 (%).All = 84-86. Text, triple inter-annotator mAP @0.5-0.95 (%).Fin = 81-86. Text, triple inter-annotator mAP @0.5-0.95 (%).Man = 88-93. Text, triple inter-annotator mAP @0.5-0.95 (%).Sci = 89-93. Text, triple inter-annotator mAP @0.5-0.95 (%).Law = 87-92. Text, triple inter-annotator mAP @0.5-0.95 (%).Pat = 71-79. Text, triple inter-annotator mAP @0.5-0.95 (%).Ten = 87-95. Title, Count.Count = 5071. Title, % of Total.Train = 0.47. Title, % of Total.Test = 0.30. Title, % of Total.Val ='\n",
      "chunker.serialize(chunk) (256 tokens):\n",
      "'Baselines for Object Detection\\nTotal.Test = 49.28. Text, % of Total.Val = 45.00. Text, triple inter-annotator mAP @0.5-0.95 (%).All = 84-86. Text, triple inter-annotator mAP @0.5-0.95 (%).Fin = 81-86. Text, triple inter-annotator mAP @0.5-0.95 (%).Man = 88-93. Text, triple inter-annotator mAP @0.5-0.95 (%).Sci = 89-93. Text, triple inter-annotator mAP @0.5-0.95 (%).Law = 87-92. Text, triple inter-annotator mAP @0.5-0.95 (%).Pat = 71-79. Text, triple inter-annotator mAP @0.5-0.95 (%).Ten = 87-95. Title, Count.Count = 5071. Title, % of Total.Train = 0.47. Title, % of Total.Test = 0.30. Title, % of Total.Val ='\n",
      "\n",
      "=== 62 ===\n",
      "chunk.text (249 tokens):\n",
      "'0.50. Title, triple inter-annotator mAP @0.5-0.95 (%).All = 60-72. Title, triple inter-annotator mAP @0.5-0.95 (%).Fin = 24-63. Title, triple inter-annotator mAP @0.5-0.95 (%).Man = 50-63. Title, triple inter-annotator mAP @0.5-0.95 (%).Sci = 94-100. Title, triple inter-annotator mAP @0.5-0.95 (%).Law = 82-96. Title, triple inter-annotator mAP @0.5-0.95 (%).Pat = 68-79. Title, triple inter-annotator mAP @0.5-0.95 (%).Ten = 24-56. Total, Count.Count = 1107470. Total, % of Total.Train = 941123. Total, % of Total.Test = 99816. Total, % of Total.Val = 66531. Total, triple inter-annotator mAP'\n",
      "chunker.serialize(chunk) (254 tokens):\n",
      "'Baselines for Object Detection\\n0.50. Title, triple inter-annotator mAP @0.5-0.95 (%).All = 60-72. Title, triple inter-annotator mAP @0.5-0.95 (%).Fin = 24-63. Title, triple inter-annotator mAP @0.5-0.95 (%).Man = 50-63. Title, triple inter-annotator mAP @0.5-0.95 (%).Sci = 94-100. Title, triple inter-annotator mAP @0.5-0.95 (%).Law = 82-96. Title, triple inter-annotator mAP @0.5-0.95 (%).Pat = 68-79. Title, triple inter-annotator mAP @0.5-0.95 (%).Ten = 24-56. Total, Count.Count = 1107470. Total, % of Total.Train = 941123. Total, % of Total.Test = 99816. Total, % of Total.Val = 66531. Total, triple inter-annotator mAP'\n",
      "\n",
      "=== 63 ===\n",
      "chunk.text (190 tokens):\n",
      "'@0.5-0.95 (%).All = 82-83. Total, triple inter-annotator mAP @0.5-0.95 (%).Fin = 71-74. Total, triple inter-annotator mAP @0.5-0.95 (%).Man = 79-81. Total, triple inter-annotator mAP @0.5-0.95 (%).Sci = 89-94. Total, triple inter-annotator mAP @0.5-0.95 (%).Law = 86-91. Total, triple inter-annotator mAP @0.5-0.95 (%).Pat = 71-76. Total, triple inter-annotator mAP @0.5-0.95 (%).Ten = 68-85\\ninclude publication repositories such as arXiv'\n",
      "chunker.serialize(chunk) (195 tokens):\n",
      "'Baselines for Object Detection\\n@0.5-0.95 (%).All = 82-83. Total, triple inter-annotator mAP @0.5-0.95 (%).Fin = 71-74. Total, triple inter-annotator mAP @0.5-0.95 (%).Man = 79-81. Total, triple inter-annotator mAP @0.5-0.95 (%).Sci = 89-94. Total, triple inter-annotator mAP @0.5-0.95 (%).Law = 86-91. Total, triple inter-annotator mAP @0.5-0.95 (%).Pat = 71-76. Total, triple inter-annotator mAP @0.5-0.95 (%).Ten = 68-85\\ninclude publication repositories such as arXiv'\n",
      "\n",
      "=== 64 ===\n",
      "chunk.text (85 tokens):\n",
      "'Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row \"Total\") in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-\\nannotated pages, from which we obtain accuracy ranges.'\n",
      "chunker.serialize(chunk) (90 tokens):\n",
      "'Baselines for Object Detection\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row \"Total\") in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-\\nannotated pages, from which we obtain accuracy ranges.'\n",
      "\n",
      "=== 65 ===\n",
      "chunk.text (204 tokens):\n",
      "'class label,  = Count. class label, %of Total = Train. class label, %of Total = Test. class label, %of Total = Val. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = All. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Fin. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Man. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Sci. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Law. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Pat. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Ten. Caption,'\n",
      "chunker.serialize(chunk) (209 tokens):\n",
      "'Baselines for Object Detection\\nclass label,  = Count. class label, %of Total = Train. class label, %of Total = Test. class label, %of Total = Val. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = All. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Fin. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Man. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Sci. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Law. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Pat. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Ten. Caption,'\n",
      "\n",
      "=== 66 ===\n",
      "chunk.text (222 tokens):\n",
      "'= 22524. Caption, %of Total = 2.04. Caption, %of Total = 1.77. Caption, %of Total = 2.32. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 84-89. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 40-61. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 86-92. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 94-99. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 95-99. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 69-78. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Footnote,'\n",
      "chunker.serialize(chunk) (227 tokens):\n",
      "'Baselines for Object Detection\\n= 22524. Caption, %of Total = 2.04. Caption, %of Total = 1.77. Caption, %of Total = 2.32. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 84-89. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 40-61. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 86-92. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 94-99. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 95-99. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 69-78. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Footnote,'\n",
      "\n",
      "=== 67 ===\n",
      "chunk.text (219 tokens):\n",
      "'= 6318. Footnote, %of Total = 0.60. Footnote, %of Total = 0.31. Footnote, %of Total = 0.58. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 83-91. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 100. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 62-88. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 85-94. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 82-97. Formula,'\n",
      "chunker.serialize(chunk) (224 tokens):\n",
      "'Baselines for Object Detection\\n= 6318. Footnote, %of Total = 0.60. Footnote, %of Total = 0.31. Footnote, %of Total = 0.58. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 83-91. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 100. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 62-88. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 85-94. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 82-97. Formula,'\n",
      "\n",
      "=== 68 ===\n",
      "chunk.text (213 tokens):\n",
      "'= 25027. Formula, %of Total = 2.25. Formula, %of Total = 1.90. Formula, %of Total = 2.96. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = 83-85. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = 84-87. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = 86-96. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. List-item,'\n",
      "chunker.serialize(chunk) (218 tokens):\n",
      "'Baselines for Object Detection\\n= 25027. Formula, %of Total = 2.25. Formula, %of Total = 1.90. Formula, %of Total = 2.96. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = 83-85. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = 84-87. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = 86-96. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. List-item,'\n",
      "\n",
      "=== 69 ===\n",
      "chunk.text (234 tokens):\n",
      "'= 185660. List-item, %of Total = 17.19. List-item, %of Total = 13.34. List-item, %of Total = 15.82. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 87-88. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 74-83. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 90-92. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 97-97. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 81-85. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 75-88. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 93-95. Page- footer,'\n",
      "chunker.serialize(chunk) (239 tokens):\n",
      "'Baselines for Object Detection\\n= 185660. List-item, %of Total = 17.19. List-item, %of Total = 13.34. List-item, %of Total = 15.82. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 87-88. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 74-83. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 90-92. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 97-97. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 81-85. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 75-88. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 93-95. Page- footer,'\n",
      "\n",
      "=== 70 ===\n",
      "chunk.text (243 tokens):\n",
      "'= 70878. Page- footer, %of Total = 6.51. Page- footer, %of Total = 5.58. Page- footer, %of Total = 6.00. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 93-94. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 88-90. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 95-96. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 100. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 92-97. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 100. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 96-98. Page- header offices,,'\n",
      "chunker.serialize(chunk) (248 tokens):\n",
      "'Baselines for Object Detection\\n= 70878. Page- footer, %of Total = 6.51. Page- footer, %of Total = 5.58. Page- footer, %of Total = 6.00. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 93-94. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 88-90. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 95-96. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 100. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 92-97. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 100. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 96-98. Page- header offices,,'\n",
      "\n",
      "=== 71 ===\n",
      "chunk.text (251 tokens):\n",
      "'= 58022. Page- header offices,, %of Total = 5.10. Page- header offices,, %of Total = 6.70. Page- header offices,, %of Total = 5.06. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 85-89. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 66-76. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 90-94. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 98-100. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 91-92. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 97-99. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 81-86. Picture,'\n",
      "chunker.serialize(chunk) (256 tokens):\n",
      "'Baselines for Object Detection\\n= 58022. Page- header offices,, %of Total = 5.10. Page- header offices,, %of Total = 6.70. Page- header offices,, %of Total = 5.06. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 85-89. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 66-76. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 90-94. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 98-100. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 91-92. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 97-99. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 81-86. Picture,'\n",
      "\n",
      "=== 72 ===\n",
      "chunk.text (216 tokens):\n",
      "'= 45976. Picture, %of Total = 4.21. Picture, %of Total = 2.78. Picture, %of Total = 5.31. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 69-71. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 56-59. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 82-86. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 69-82. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 80-95. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 66-71. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 59-76. Section- header not,'\n",
      "chunker.serialize(chunk) (221 tokens):\n",
      "'Baselines for Object Detection\\n= 45976. Picture, %of Total = 4.21. Picture, %of Total = 2.78. Picture, %of Total = 5.31. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 69-71. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 56-59. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 82-86. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 69-82. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 80-95. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 66-71. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 59-76. Section- header not,'\n",
      "\n",
      "=== 73 ===\n",
      "chunk.text (243 tokens):\n",
      "'= 142884. Section- header not, %of Total = 12.60. Section- header not, %of Total = 15.77. Section- header not, %of Total = 12.85. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 83-84. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 76-81. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 90-92. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 94-95. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 87-94. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 69-73. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 78-86. Table,'\n",
      "chunker.serialize(chunk) (248 tokens):\n",
      "'Baselines for Object Detection\\n= 142884. Section- header not, %of Total = 12.60. Section- header not, %of Total = 15.77. Section- header not, %of Total = 12.85. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 83-84. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 76-81. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 90-92. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 94-95. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 87-94. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 69-73. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 78-86. Table,'\n",
      "\n",
      "=== 74 ===\n",
      "chunk.text (212 tokens):\n",
      "'= 34733. Table, %of Total = 3.20. Table, %of Total = 2.27. Table, %of Total = 3.60. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 77-81. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 75-80. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 83-86. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 98-99. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 58-80. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 79-84. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 70-85. Text,'\n",
      "chunker.serialize(chunk) (217 tokens):\n",
      "'Baselines for Object Detection\\n= 34733. Table, %of Total = 3.20. Table, %of Total = 2.27. Table, %of Total = 3.60. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 77-81. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 75-80. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 83-86. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 98-99. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 58-80. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 79-84. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 70-85. Text,'\n",
      "\n",
      "=== 75 ===\n",
      "chunk.text (217 tokens):\n",
      "'= 510377. Text, %of Total = 45.82. Text, %of Total = 49.28. Text, %of Total = 45.00. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 84-86. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 81-86. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 88-93. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 89-93. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 87-92. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 71-79. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 87-95. Title [22], a,'\n",
      "chunker.serialize(chunk) (222 tokens):\n",
      "'Baselines for Object Detection\\n= 510377. Text, %of Total = 45.82. Text, %of Total = 49.28. Text, %of Total = 45.00. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 84-86. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 81-86. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 88-93. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 89-93. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 87-92. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 71-79. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 87-95. Title [22], a,'\n",
      "\n",
      "=== 76 ===\n",
      "chunk.text (245 tokens):\n",
      "'= 5071. Title [22], a, %of Total = 0.47. Title [22], a, %of Total = 0.30. Title [22], a, %of Total = 0.50. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 60-72. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 24-63. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 50-63. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 94-100. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 82-96. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 68-79. Title [22], a, triple inter- annotator mAP @'\n",
      "chunker.serialize(chunk) (250 tokens):\n",
      "'Baselines for Object Detection\\n= 5071. Title [22], a, %of Total = 0.47. Title [22], a, %of Total = 0.30. Title [22], a, %of Total = 0.50. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 60-72. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 24-63. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 50-63. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 94-100. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 82-96. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 68-79. Title [22], a, triple inter- annotator mAP @'\n",
      "\n",
      "=== 77 ===\n",
      "chunk.text (251 tokens):\n",
      "'0.5-0.95 (%) = 24-56. Total in-,\\n= 1107470. Total in-, %of Total = 941123. Total in-, %of Total = 99816. Total in-, %of Total = 66531. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 82-83. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 71-74. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 79-81. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 89-94. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 86-91. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 71-76. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 68-85\\n3\\n,'\n",
      "chunker.serialize(chunk) (256 tokens):\n",
      "'Baselines for Object Detection\\n0.5-0.95 (%) = 24-56. Total in-,\\n= 1107470. Total in-, %of Total = 941123. Total in-, %of Total = 99816. Total in-, %of Total = 66531. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 82-83. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 71-74. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 79-81. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 89-94. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 86-91. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 71-76. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 68-85\\n3\\n,'\n",
      "\n",
      "=== 78 ===\n",
      "chunk.text (139 tokens):\n",
      "'government offices,\\nWe reviewed the col-\\n,\\nPage-\\nTitle and\\n.\\npage. Specificity ensures that the choice of label is not ambiguous,\\nwe distributed the annotation workload and performed continuous be annotated. We refrained from class labels that are very specific\\nonly. For phases three and four, a group of 40 dedicated annotators while coverage ensures that all meaningful items on a page can\\nquality controls. Phase one and two required a small team of experts to a document category, such as\\nAbstract in the\\nScientific Articles were assembled and supervised.\\ncategory. We also avoided class labels that are tightly linked to the\\nPhase 1: Data selection and preparation.\\nOur inclusion cri-\\nAuthor\\nAffiliation'\n",
      "chunker.serialize(chunk) (144 tokens):\n",
      "'Baselines for Object Detection\\ngovernment offices,\\nWe reviewed the col-\\n,\\nPage-\\nTitle and\\n.\\npage. Specificity ensures that the choice of label is not ambiguous,\\nwe distributed the annotation workload and performed continuous be annotated. We refrained from class labels that are very specific\\nonly. For phases three and four, a group of 40 dedicated annotators while coverage ensures that all meaningful items on a page can\\nquality controls. Phase one and two required a small team of experts to a document category, such as\\nAbstract in the\\nScientific Articles were assembled and supervised.\\ncategory. We also avoided class labels that are tightly linked to the\\nPhase 1: Data selection and preparation.\\nOur inclusion cri-\\nAuthor\\nAffiliation'\n",
      "\n",
      "=== 79 ===\n",
      "chunk.text (191 tokens):\n",
      "\"teria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\\nsemantics of the text. Labels such as and\\n,\\nas seen\"\n",
      "chunker.serialize(chunk) (196 tokens):\n",
      "\"Baselines for Object Detection\\nteria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\\nsemantics of the text. Labels such as and\\n,\\nas seen\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunks_to_embed = []\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"=== {i} ===\")\n",
    "    txt_tokens = len(tokenizer.tokenize(chunk.text))\n",
    "    print(f\"chunk.text ({txt_tokens} tokens):\\n{chunk.text!r}\")\n",
    "\n",
    "    ser_txt = chunker.contextualize(chunk=chunk)\n",
    "    ser_tokens = len(tokenizer.tokenize(ser_txt))\n",
    "    print(f\"chunker.serialize(chunk) ({ser_tokens} tokens):\\n{ser_txt!r}\")\n",
    "    chunks_to_embed.append(ser_txt)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba0da036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class VectorDbEntity:\n",
    "    page_no: int\n",
    "    source: str\n",
    "    content: str\n",
    "    vector: list[float]\n",
    "\n",
    "\n",
    "def create_vector_db_entity(doc_chunk, vector):\n",
    "    meta = doc_chunk.meta\n",
    "\n",
    "    # Get source filename\n",
    "    filename = meta.origin.filename if meta.origin else None\n",
    "\n",
    "    # Collect all page numbers from doc_items' provenance\n",
    "    page = meta.doc_items[0].prov[0].page_no\n",
    "\n",
    "    return VectorDbEntity(\n",
    "        page_no=page,\n",
    "        source=filename,\n",
    "        content=chunker.contextualize(doc_chunk),\n",
    "        vector=vector\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3961f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "api_key = \"Ua6lkNDZiSdPYKEN2bCoEFoS4H3xBoQO\"\n",
    "model = \"mistral-embed\"\n",
    "\n",
    "mistral_client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f71e33f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_batch_response = mistral_client.embeddings.create(\n",
    "    model=model,\n",
    "    inputs=chunks_to_embed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c71530e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = [create_vector_db_entity(doc_chunk, vector.embedding) for doc_chunk, vector in zip(chunks, embeddings_batch_response.data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be5fcfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 0 ===\n",
      "page_no: 1\n",
      "source: 2408.09869v5.pdf\n",
      "content: Docling Technical Report\n",
      "Version 1.0\n",
      "Christoph Auer Maksym Lysak Ahmed Nassar Michele Dolfi Nikolaos Livathinos Panos Vagenas Cesar Berrospi Ramis Matteo Omenetti Fabian Lindlbauer Kasper Dinkla Lokesh Mishra Yusik Kim Shubham Gupta Rafael Teixeira de Lima Valery Weber Lucas Morin Ingmar Meijer Viktor Kuropiatnyk Peter W. J. Staar\n",
      "AI4K Group, IBM Research R¨ uschlikon, Switzerland\n",
      "vector: [-0.033050537109375, 0.0215606689453125, 0.00079345703125, 0.00910186767578125, 0.0276336669921875, 0.036041259765625, 0.03863525390625, -0.006908416748046875, -0.0311737060546875, -0.0134429931640625]…\n",
      "\n",
      "=== 1 ===\n",
      "page_no: 1\n",
      "source: 2408.09869v5.pdf\n",
      "content: Abstract\n",
      "This technical report introduces Docling , an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.\n",
      "vector: [-0.031524658203125, 0.0167083740234375, 0.0233154296875, 0.0079345703125, 0.040802001953125, 0.0186004638671875, 0.01425933837890625, -0.01071929931640625, -0.02001953125, 0.0157623291015625]…\n",
      "\n",
      "=== 2 ===\n",
      "page_no: 1\n",
      "source: 2408.09869v5.pdf\n",
      "content: 1 Introduction\n",
      "Converting PDF documents back into a machine-processable format has been a major challenge for decades due to their huge variability in formats, weak standardization and printing-optimized characteristic, which discards most structural features and metadata. With the advent of LLMs and popular application patterns such as retrieval-augmented generation (RAG), leveraging the rich content embedded in PDFs has become ever more relevant. In the past decade, several powerful document understanding solutions have emerged on the market, most of which are commercial software, cloud offerings [3] and most recently, multi-modal vision-language models. As of today, only a handful of open-source tools cover PDF conversion, leaving a significant feature and quality gap to proprietary solutions.\n",
      "With Docling , we open-source a very capable and efficient document conversion tool which builds on the powerful, specialized AI models and datasets for layout analysis and table structure recognition we developed and presented in the recent past [12, 13, 9]. Docling is designed as a simple, self-contained python library with permissive license, running entirely locally on commodity hardware. Its code architecture allows for easy extensibility and addition of new features and models.\n",
      "Here is what Docling delivers today:\n",
      "vector: [-0.03369140625, 0.024322509765625, 0.0234527587890625, 0.0220794677734375, 0.05413818359375, 0.009979248046875, 0.02557373046875, 0.005237579345703125, -0.004116058349609375, 0.016845703125]…\n",
      "\n",
      "=== 3 ===\n",
      "page_no: 2\n",
      "source: 2408.09869v5.pdf\n",
      "content: 1 Introduction\n",
      "· Converts PDF documents to JSON or Markdown format, stable and lightning fast\n",
      "· Understands detailed page layout, reading order, locates figures and recovers table structures\n",
      "· Extracts metadata from the document, such as title, authors, references and language\n",
      "· Optionally applies OCR, e.g. for scanned PDFs\n",
      "· Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\n",
      "· Can leverage different accelerators (GPU, MPS, etc).\n",
      "vector: [-0.0100860595703125, 0.03814697265625, 0.03057861328125, 0.0219268798828125, 0.038330078125, 0.024505615234375, 0.0215606689453125, 0.01142120361328125, 0.02044677734375, 0.0169525146484375]…\n",
      "\n",
      "=== 4 ===\n",
      "page_no: 2\n",
      "source: 2408.09869v5.pdf\n",
      "content: 2 Getting Started\n",
      "To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\n",
      "Docling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\n",
      "```\n",
      "from docling.document_converter import DocumentConverter Large\n",
      "```\n",
      "vector: [-0.0255126953125, 0.0267181396484375, 0.0155029296875, 0.0230865478515625, 0.041900634765625, 0.01456451416015625, 0.044036865234375, 0.0009775161743164062, 0.0029201507568359375, 0.030609130859375]…\n",
      "\n",
      "=== 5 ===\n",
      "page_no: 2\n",
      "source: 2408.09869v5.pdf\n",
      "content: 2 Getting Started\n",
      "```\n",
      "source = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Human -Annotated Dataset for Document -Layout Analysis [...]\"\n",
      "```\n",
      "Optionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\n",
      "vector: [-0.0210418701171875, 0.039947509765625, 0.0185089111328125, -0.0003476142883300781, 0.055389404296875, 0.034637451171875, 0.0218353271484375, 0.008026123046875, -0.00559234619140625, 0.03515625]…\n",
      "\n",
      "=== 6 ===\n",
      "page_no: 2\n",
      "source: 2408.09869v5.pdf\n",
      "content: 3 Processing pipeline\n",
      "Docling implements a linear pipeline of operations, which execute sequentially on each given document (see Fig. 1). Each document is first parsed by a PDF backend, which retrieves the programmatic text tokens, consisting of string content and its coordinates on the page, and also renders a bitmap image of each page to support downstream operations. Then, the standard model pipeline applies a sequence of AI models independently on every page in the document to extract features and content, such as layout and table structures. Finally, the results from all pages are aggregated and passed through a post-processing stage, which augments metadata, detects the document language, infers reading-order and eventually assembles a typed document object which can be serialized to JSON or Markdown.\n",
      "vector: [-0.028564453125, 0.031219482421875, 0.01453399658203125, 0.0130462646484375, 0.03302001953125, 0.01593017578125, 0.032196044921875, -0.00763702392578125, 0.003963470458984375, 0.0175933837890625]…\n",
      "\n",
      "=== 7 ===\n",
      "page_no: 2\n",
      "source: 2408.09869v5.pdf\n",
      "content: 3.1 PDF backends\n",
      "Two basic requirements to process PDF documents in our pipeline are a) to retrieve all text content and their geometric coordinates on each page and b) to render the visual representation of each page as it would appear in a PDF viewer. Both these requirements are encapsulated in Docling's PDF backend interface. While there are several open-source PDF parsing libraries available for python, we faced major obstacles with all of them for different reasons, among which were restrictive\n",
      "1 see huggingface.co/ds4sd/docling-models/\n",
      "Figure 1: Sketch of Docling's default processing pipeline. The inner part of the model pipeline is easily customizable and extensible.\n",
      "licensing (e.g. pymupdf [7]), poor speed or unrecoverable quality issues, such as merged text cells across far-apart text tokens or table columns (pypdfium, PyPDF) [15, 14].\n",
      "vector: [-0.02313232421875, 0.0158233642578125, 0.0283966064453125, 0.03436279296875, 0.04388427734375, 0.0014352798461914062, -0.0018596649169921875, 0.018707275390625, 0.015899658203125, 0.01947021484375]…\n",
      "\n",
      "=== 8 ===\n",
      "page_no: 3\n",
      "source: 2408.09869v5.pdf\n",
      "content: 3.1 PDF backends\n",
      "We therefore decided to provide multiple backend choices, and additionally open-source a custombuilt PDF parser, which is based on the low-level qpdf [4] library. It is made available in a separate package named docling-parse and powers the default PDF backend in Docling. As an alternative, we provide a PDF backend relying on pypdfium , which may be a safe backup choice in certain cases, e.g. if issues are seen with particular font encodings.\n",
      "vector: [-0.0266265869140625, 0.004505157470703125, 0.026275634765625, 0.0209197998046875, 0.03515625, 0.0192108154296875, -0.00951385498046875, 0.017669677734375, 0.0140380859375, 0.0190277099609375]…\n",
      "\n",
      "=== 9 ===\n",
      "page_no: 3\n",
      "source: 2408.09869v5.pdf\n",
      "content: 3.2 AI models\n",
      "As part of Docling, we initially release two highly capable AI models to the open-source community, which have been developed and published recently by our team. The first model is a layout analysis model, an accurate object-detector for page elements [13]. The second model is TableFormer [12, 9], a state-of-the-art table structure recognition model. We provide the pre-trained weights (hosted on huggingface) and a separate package for the inference code as docling-ibm-models . Both models are also powering the open-access deepsearch-experience, our cloud-native service for knowledge exploration tasks.\n",
      "vector: [-0.0311737060546875, 0.032806396484375, 0.0240325927734375, 0.004161834716796875, 0.007266998291015625, 0.00933837890625, 0.01551055908203125, -0.0020809173583984375, -0.00420379638671875, 0.0014314651489257812]…\n",
      "\n",
      "=== 10 ===\n",
      "page_no: 3\n",
      "source: 2408.09869v5.pdf\n",
      "content: Layout Analysis Model\n",
      "Our layout analysis model is an object-detector which predicts the bounding-boxes and classes of various elements on the image of a given page. Its architecture is derived from RT-DETR [16] and re-trained on DocLayNet [13], our popular human-annotated dataset for document-layout analysis, among other proprietary datasets. For inference, our implementation relies on the onnxruntime [5].\n",
      "The Docling pipeline feeds page images at 72 dpi resolution, which can be processed on a single CPU with sub-second latency. All predicted bounding-box proposals for document elements are post-processed to remove overlapping proposals based on confidence and size, and then intersected with the text tokens in the PDF to group them into meaningful and complete units such as paragraphs, section titles, list items, captions, figures or tables.\n",
      "vector: [-0.0184326171875, 0.0159149169921875, 0.0251617431640625, 0.004810333251953125, 0.038787841796875, -0.00525665283203125, 0.0192413330078125, -0.0250244140625, -0.01910400390625, 0.00873565673828125]…\n",
      "\n",
      "=== 11 ===\n",
      "page_no: 3\n",
      "source: 2408.09869v5.pdf\n",
      "content: Table Structure Recognition\n",
      "The TableFormer model [12], first published in 2022 and since refined with a custom structure token language [9], is a vision-transformer model for table structure recovery. It can predict the logical row and column structure of a given table based on an input image, and determine which table cells belong to column headers, row headers or the table body. Compared to earlier approaches, TableFormer handles many characteristics of tables, such as partial or no borderlines, empty cells, rows or columns, cell spans and hierarchy both on column-heading or row-heading level, tables with inconsistent indentation or alignment and other complexities. For inference, our implementation relies on PyTorch [2].\n",
      "The Docling pipeline feeds all table objects detected in the layout analysis to the TableFormer model, by providing an image-crop of the table and the included text cells. TableFormer structure predictions are matched back to the PDF cells in post-processing to avoid expensive re-transcription text in the table image. Typical tables require between 2 and 6 seconds to be processed on a standard CPU, strongly depending on the amount of included table cells.\n",
      "vector: [-0.01459503173828125, -0.0004725456237792969, 0.01235198974609375, -0.00736236572265625, 0.0289306640625, 0.022979736328125, 0.004425048828125, -0.01439666748046875, -0.002857208251953125, -0.01255035400390625]…\n",
      "\n",
      "=== 12 ===\n",
      "page_no: 4\n",
      "source: 2408.09869v5.pdf\n",
      "content: OCR\n",
      "Docling provides optional support for OCR, for example to cover scanned PDFs or content in bitmaps images embedded on a page. In our initial release, we rely on EasyOCR [1], a popular thirdparty OCR library with support for many languages. Docling, by default, feeds a high-resolution page image (216 dpi) to the OCR engine, to allow capturing small print detail in decent quality. While EasyOCR delivers reasonable transcription quality, we observe that it runs fairly slow on CPU (upwards of 30 seconds per page).\n",
      "We are actively seeking collaboration from the open-source community to extend Docling with additional OCR backends and speed improvements.\n",
      "vector: [-0.0022678375244140625, 0.0195465087890625, -0.005916595458984375, 0.0165863037109375, 0.0333251953125, 0.0218048095703125, 0.0264739990234375, -0.0208740234375, -0.0389404296875, 0.0039520263671875]…\n",
      "\n",
      "=== 13 ===\n",
      "page_no: 4\n",
      "source: 2408.09869v5.pdf\n",
      "content: 3.3 Assembly\n",
      "In the final pipeline stage, Docling assembles all prediction results produced on each page into a well-defined datatype that encapsulates a converted document, as defined in the auxiliary package docling-core . The generated document object is passed through a post-processing model which leverages several algorithms to augment features, such as detection of the document language, correcting the reading order, matching figures with captions and labelling metadata such as title, authors and references. The final output can then be serialized to JSON or transformed into a Markdown representation at the users request.\n",
      "vector: [-0.02703857421875, 0.037261962890625, 0.01666259765625, 0.01337432861328125, 0.0496826171875, 0.01198577880859375, 0.0299530029296875, -0.016143798828125, -0.0005025863647460938, 0.039154052734375]…\n",
      "\n",
      "=== 14 ===\n",
      "page_no: 4\n",
      "source: 2408.09869v5.pdf\n",
      "content: 3.4 Extensibility\n",
      "Docling provides a straight-forward interface to extend its capabilities, namely the model pipeline. A model pipeline constitutes the central part in the processing, following initial document parsing and preceding output assembly, and can be fully customized by sub-classing from an abstract baseclass ( BaseModelPipeline ) or cloning the default model pipeline. This effectively allows to fully customize the chain of models, add or replace models, and introduce additional pipeline configuration parameters. To use a custom model pipeline, the custom pipeline class to instantiate can be provided as an argument to the main document conversion methods. We invite everyone in the community to propose additional or alternative models and improvements.\n",
      "Implementations of model classes must satisfy the python Callable interface. The __call__ method must accept an iterator over page objects, and produce another iterator over the page objects which were augmented with the additional features predicted by the model, by extending the provided PagePredictions data model accordingly.\n",
      "vector: [-0.0258331298828125, 0.019744873046875, -0.0016803741455078125, 0.0047149658203125, -0.0064239501953125, 0.008392333984375, 0.007312774658203125, 0.01425933837890625, -0.007720947265625, 0.04632568359375]…\n",
      "\n",
      "=== 15 ===\n",
      "page_no: 4\n",
      "source: 2408.09869v5.pdf\n",
      "content: 4 Performance\n",
      "In this section, we establish some reference numbers for the processing speed of Docling and the resource budget it requires. All tests in this section are run with default options on our standard test set distributed with Docling, which consists of three papers from arXiv and two IBM Redbooks, with a total of 225 pages. Measurements were taken using both available PDF backends on two different hardware systems: one MacBook Pro M3 Max, and one bare-metal server running Ubuntu 20.04 LTS on an Intel Xeon E5-2690 CPU. For reproducibility, we fixed the thread budget (through setting OMP NUM THREADS environment variable ) once to 4 (Docling default) and once to 16 (equal to full core count on the test hardware). All results are shown in Table 1.\n",
      "If you need to run Docling in very low-resource environments, please consider configuring the pypdfium backend. While it is faster and more memory efficient than the default docling-parse backend, it will come at the expense of worse quality results, especially in table structure recovery.\n",
      "vector: [-0.04742431640625, 0.00948333740234375, 0.006908416748046875, 0.029541015625, 0.037109375, 0.020599365234375, 0.0124664306640625, -0.0112457275390625, 0.01117706298828125, 0.0204620361328125]…\n",
      "\n",
      "=== 16 ===\n",
      "page_no: 4\n",
      "source: 2408.09869v5.pdf\n",
      "content: 4 Performance\n",
      "Establishing GPU acceleration support for the AI models is currently work-in-progress and largely untested, but may work implicitly when CUDA is available and discovered by the onnxruntime and\n",
      "torch runtimes backing the Docling pipeline. We will deliver updates on this topic at in a future version of this report.\n",
      "\n",
      "Table 1: Runtime characteristics of Docling with the standard model pipeline and settings, on our test dataset of 225 pages, on two different systems. OCR is disabled. We show the time-to-solution (TTS), computed throughput in pages per second, and the peak memory used (resident set size) for both the Docling-native PDF backend and for the pypdfium backend, using 4 and 16 threads.\n",
      "vector: [-0.03558349609375, 0.01104736328125, 0.0209808349609375, 0.01739501953125, 0.045135498046875, 0.029083251953125, 0.008026123046875, 0.004390716552734375, 0.00957489013671875, 0.0020961761474609375]…\n",
      "\n",
      "=== 17 ===\n",
      "page_no: 5\n",
      "source: 2408.09869v5.pdf\n",
      "content: 4 Performance\n",
      "Apple M3 Max, Thread budget. = 4. Apple M3 Max, native backend.TTS = 177 s 167 s. Apple M3 Max, native backend.Pages/s = 1.27 1.34. Apple M3 Max, native backend.Mem = 6.20 GB. Apple M3 Max, pypdfium backend.TTS = 103 s 92 s. Apple M3 Max, pypdfium backend.Pages/s = 2.18 2.45. Apple M3 Max, pypdfium backend.Mem = 2.56 GB. (16 cores) Intel(R) Xeon E5-2690, Thread budget. = 16 4 16. (16 cores) Intel(R) Xeon E5-2690, native backend.TTS = 375 s 244 s. (16 cores) Intel(R) Xeon E5-2690, native backend.Pages/s = 0.60 0.92. (16 cores) Intel(R) Xeon E5-2690, native backend.Mem = 6.16 GB. (16 cores) Intel(R) Xeon E5-2690, pypdfium\n",
      "vector: [-0.023529052734375, 0.038604736328125, 0.027557373046875, 0.0167388916015625, 0.06158447265625, 0.03546142578125, 0.00225830078125, 0.010040283203125, 0.0245361328125, 0.0237579345703125]…\n",
      "\n",
      "=== 18 ===\n",
      "page_no: 5\n",
      "source: 2408.09869v5.pdf\n",
      "content: 4 Performance\n",
      "backend.TTS = 239 s 143 s. (16 cores) Intel(R) Xeon E5-2690, pypdfium backend.Pages/s = 0.94 1.57. (16 cores) Intel(R) Xeon E5-2690, pypdfium backend.Mem = 2.42 GB\n",
      "vector: [-0.055328369140625, 0.031219482421875, 0.017913818359375, 0.02587890625, 0.05279541015625, 0.0310211181640625, -0.01152801513671875, 0.0057373046875, 0.0185394287109375, 0.0250396728515625]…\n",
      "\n",
      "=== 19 ===\n",
      "page_no: 5\n",
      "source: 2408.09869v5.pdf\n",
      "content: 5 Applications\n",
      "Thanks to the high-quality, richly structured document conversion achieved by Docling, its output qualifies for numerous downstream applications. For example, Docling can provide a base for detailed enterprise document search, passage retrieval or classification use-cases, or support knowledge extraction pipelines, allowing specific treatment of different structures in the document, such as tables, figures, section structure or references. For popular generative AI application patterns, such as retrieval-augmented generation (RAG), we provide quackling , an open-source package which capitalizes on Docling's feature-rich document output to enable document-native optimized vector embedding and chunking. It plugs in seamlessly with LLM frameworks such as LlamaIndex [8]. Since Docling is fast, stable and cheap to run, it also makes for an excellent choice to build document-derived datasets. With its powerful table structure recognition, it provides significant benefit to automated knowledge-base construction [11, 10]. Docling is also integrated within the open IBM data prep kit [6], which implements scalable data transforms to build large-scale multi-modal training datasets.\n",
      "vector: [-0.0245361328125, 0.01226806640625, 0.020965576171875, 0.030548095703125, 0.040130615234375, 0.0037364959716796875, 0.0328369140625, 0.006946563720703125, -0.01446533203125, 0.0282745361328125]…\n",
      "\n",
      "=== 20 ===\n",
      "page_no: 5\n",
      "source: 2408.09869v5.pdf\n",
      "content: 6 Future work and contributions\n",
      "Docling is designed to allow easy extension of the model library and pipelines. In the future, we plan to extend Docling with several more models, such as a figure-classifier model, an equationrecognition model, a code-recognition model and more. This will help improve the quality of conversion for specific types of content, as well as augment extracted document metadata with additional information. Further investment into testing and optimizing GPU acceleration as well as improving the Docling-native PDF backend are on our roadmap, too.\n",
      "We encourage everyone to propose or implement additional features and models, and will gladly take your inputs and contributions under review . The codebase of Docling is open for use and contribution, under the MIT license agreement and in alignment with our contributing guidelines included in the Docling repository. If you use Docling in your projects, please consider citing this technical report.\n",
      "vector: [-0.0223541259765625, 0.019439697265625, 0.0131988525390625, 0.0180511474609375, 0.02386474609375, 0.003803253173828125, 0.03814697265625, -0.0257568359375, -0.0142059326171875, 0.01029205322265625]…\n",
      "\n",
      "=== 21 ===\n",
      "page_no: 5\n",
      "source: 2408.09869v5.pdf\n",
      "content: References\n",
      "[1] J. AI. Easyocr: Ready-to-use ocr with 80+ supported languages. https://github.com/ JaidedAI/EasyOCR , 2024. Version: 1.7.0.\n",
      "vector: [0.0026798248291015625, 0.050567626953125, 0.0179443359375, -0.00328826904296875, 0.01546478271484375, 0.048187255859375, 0.0227203369140625, 0.01580810546875, -0.00392913818359375, 0.01495361328125]…\n",
      "\n",
      "=== 22 ===\n",
      "page_no: 5\n",
      "source: 2408.09869v5.pdf\n",
      "content: References\n",
      "[2] J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski, G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong, M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang, Y. Lu, C. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, M. Suo, P. Tillet, E. Wang, X. Wang, W. Wen, S. Zhang, X. Zhao, K. Zhou, R. Zou, A. Mathews, G. Chanan, P. Wu, and S. Chintala. Pytorch 2: Faster\n",
      "vector: [-0.0268402099609375, 0.01198577880859375, 0.0460205078125, -0.000804901123046875, 0.04986572265625, 0.0102691650390625, -0.012054443359375, -0.0002932548522949219, -1.8477439880371094e-05, 0.0063018798828125]…\n",
      "\n",
      "=== 23 ===\n",
      "page_no: 6\n",
      "source: 2408.09869v5.pdf\n",
      "content: References\n",
      "machine learning through dynamic python bytecode transformation and graph compilation. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24) . ACM, 4 2024. doi: 10.1145/3620665.3640366. URL https://pytorch.org/assets/pytorch2-2.pdf .\n",
      "vector: [-0.03326416015625, 0.00415802001953125, 0.042755126953125, -0.000865936279296875, 0.03997802734375, 0.0318603515625, 0.0213775634765625, 0.0035877227783203125, 0.0004329681396484375, -0.00910186767578125]…\n",
      "\n",
      "=== 24 ===\n",
      "page_no: 6\n",
      "source: 2408.09869v5.pdf\n",
      "content: References\n",
      "[3] C. Auer, M. Dolfi, A. Carvalho, C. B. Ramis, and P. W. Staar. Delivering document conversion as a cloud service with high throughput and responsiveness. In 2022 IEEE 15th International Conference on Cloud Computing (CLOUD) , pages 363-373. IEEE, 2022.\n",
      "[4] J. Berkenbilt. Qpdf: A content-preserving pdf document transformer, 2024. URL https: //github.com/qpdf/qpdf .\n",
      "[5] O. R. developers. Onnx runtime. https://onnxruntime.ai/ , 2024. Version: 1.18.1.\n",
      "[6] IBM. Data Prep Kit: a community project to democratize and accelerate unstructured data preparation for LLM app developers, 2024. URL https://github.com/IBM/ data-prep-kit .\n",
      "[7] A. S. Inc. PyMuPDF, 2024. URL https://github.com/pymupdf/PyMuPDF .\n",
      "vector: [-0.032989501953125, 0.037200927734375, 0.03179931640625, 0.0026607513427734375, 0.039215087890625, 0.027252197265625, 0.0177459716796875, 0.007236480712890625, 0.0161590576171875, 0.0201873779296875]…\n",
      "\n",
      "=== 25 ===\n",
      "page_no: 6\n",
      "source: 2408.09869v5.pdf\n",
      "content: References\n",
      "[8] J. Liu. LlamaIndex, 11 2022. URL https://github.com/jerryjliu/llama_index .\n",
      "[9] M. Lysak, A. Nassar, N. Livathinos, C. Auer, and P. Staar. Optimized Table Tokenization for Table Structure Recognition. In Document Analysis and Recognition - ICDAR 2023: 17th International Conference, San Jos´ e, CA, USA, August 21-26, 2023, Proceedings, Part II , pages 37-50, Berlin, Heidelberg, Aug. 2023. Springer-Verlag. ISBN 978-3-031-41678-1. doi: 10. 1007/978-3-031-41679-8 3. URL https://doi.org/10.1007/978-3-031-41679-8_3 .\n",
      "vector: [-0.0213165283203125, 0.04449462890625, 0.0299835205078125, -0.01206207275390625, 0.0105438232421875, 0.020965576171875, -0.0023136138916015625, 0.0075531005859375, 0.01065826416015625, 0.0154571533203125]…\n",
      "\n",
      "=== 26 ===\n",
      "page_no: 6\n",
      "source: 2408.09869v5.pdf\n",
      "content: References\n",
      "[10] L. Mishra, S. Dhibi, Y. Kim, C. Berrospi Ramis, S. Gupta, M. Dolfi, and P. Staar. Statements: Universal information extraction from tables with large language models for ESG KPIs. In D. Stammbach, J. Ni, T. Schimanski, K. Dutia, A. Singh, J. Bingler, C. Christiaen, N. Kushwaha, V. Muccione, S. A. Vaghefi, and M. Leippold, editors, Proceedings of the 1st Workshop on Natural Language Processing Meets Climate Change (ClimateNLP 2024) , pages 193-214, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.climatenlp-1.15 .\n",
      "vector: [-0.0268707275390625, 0.055206298828125, 0.032440185546875, -0.001911163330078125, 0.0139923095703125, 0.0076446533203125, 0.00653076171875, -0.0177001953125, -0.00699615478515625, 0.028900146484375]…\n",
      "\n",
      "=== 27 ===\n",
      "page_no: 6\n",
      "source: 2408.09869v5.pdf\n",
      "content: References\n",
      "[11] L. Morin, V. Weber, G. I. Meijer, F. Yu, and P. W. J. Staar. Patcid: an open-access dataset of chemical structures in patent documents. Nature Communications , 15(1):6532, August 2024. ISSN 2041-1723. doi: 10.1038/s41467-024-50779-y. URL https://doi.org/10.1038/ s41467-024-50779-y .\n",
      "[12] A. Nassar, N. Livathinos, M. Lysak, and P. Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4614-4623, 2022.\n",
      "[13] B. Pfitzmann, C. Auer, M. Dolfi, A. S. Nassar, and P. Staar. Doclaynet: a large humanannotated dataset for document-layout segmentation. pages 3743-3751, 2022.\n",
      "vector: [-0.0222320556640625, 0.0221405029296875, 0.031402587890625, 0.0023708343505859375, 0.00576019287109375, 0.0023250579833984375, 0.024688720703125, 0.01088714599609375, 0.0152435302734375, 0.01043701171875]…\n",
      "\n",
      "=== 28 ===\n",
      "page_no: 6\n",
      "source: 2408.09869v5.pdf\n",
      "content: References\n",
      "[14] pypdf Maintainers. pypdf: A Pure-Python PDF Library, 2024. URL https://github.com/ py-pdf/pypdf .\n",
      "[15] P. Team. PyPDFium2: Python bindings for PDFium, 2024. URL https://github.com/ pypdfium2-team/pypdfium2 .\n",
      "[16] Y. Zhao, W. Lv, S. Xu, J. Wei, G. Wang, Q. Dang, Y. Liu, and J. Chen. Detrs beat yolos on real-time object detection, 2023.\n",
      "vector: [-0.049072265625, 0.01611328125, 0.0288543701171875, 0.007427215576171875, 0.02655029296875, 0.0236968994140625, 0.00350189208984375, 0.0110626220703125, 0.022857666015625, -0.0016393661499023438]…\n",
      "\n",
      "=== 29 ===\n",
      "page_no: 7\n",
      "source: 2408.09869v5.pdf\n",
      "content: Appendix\n",
      "In this section, we illustrate a few examples of Docling's output in Markdown and JSON.\n",
      "vector: [-0.0200958251953125, 0.03985595703125, 0.03436279296875, 0.0209503173828125, 0.0234222412109375, 0.033172607421875, 0.011199951171875, -0.016326904296875, -0.01042938232421875, 0.0309600830078125]…\n",
      "\n",
      "=== 30 ===\n",
      "page_no: 7\n",
      "source: 2408.09869v5.pdf\n",
      "content: DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\n",
      "Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\n",
      "Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\n",
      "Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\n",
      "Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\n",
      "Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\n",
      "vector: [-0.0231475830078125, 0.009063720703125, 0.01010894775390625, -0.0002701282501220703, 0.042938232421875, 0.0008625984191894531, 0.033599853515625, -0.02789306640625, -0.0221710205078125, -0.004253387451171875]…\n",
      "\n",
      "=== 31 ===\n",
      "page_no: 7\n",
      "source: 2408.09869v5.pdf\n",
      "content: ABSTRACT\n",
      "Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10%\n",
      "vector: [-0.03564453125, 0.0173492431640625, 0.0234375, 0.0007085800170898438, 0.05450439453125, 0.00867462158203125, 0.0159149169921875, -0.037353515625, -0.0253448486328125, 0.0169677734375]…\n",
      "\n",
      "=== 32 ===\n",
      "page_no: 7\n",
      "source: 2408.09869v5.pdf\n",
      "content: ABSTRACT\n",
      "behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\n",
      "vector: [-0.0367431640625, 0.0131378173828125, 0.020751953125, 0.0101470947265625, 0.03900146484375, 0.0015077590942382812, 0.022552490234375, -0.03924560546875, -0.0293121337890625, 0.0171356201171875]…\n",
      "\n",
      "=== 33 ===\n",
      "page_no: 7\n",
      "source: 2408.09869v5.pdf\n",
      "content: CCS CONCEPTS\n",
      "· Informationsystems → Documentstructure ; · Appliedcomputing → Document analysis ; · Computing methodologies → Machine learning Computer vision ; ; Object detection ;\n",
      "Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). KDD '22, August 14-18, 2022, Washington, DC, USA © 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\n",
      "Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com\n",
      "Christoph Auer IBM Research Rueschlikon, Switzerland cau@zurich.ibm.com\n",
      "Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com\n",
      "vector: [-0.0267181396484375, 0.0272979736328125, 0.0322265625, 0.0313720703125, 0.042388916015625, 0.0173492431640625, 0.045013427734375, -0.01096343994140625, -0.0088958740234375, -0.0035572052001953125]…\n",
      "\n",
      "=== 34 ===\n",
      "page_no: 7\n",
      "source: 2408.09869v5.pdf\n",
      "content: CCS CONCEPTS\n",
      "Ahmed S. Nassar IBM Research Rueschlikon, Switzerland ahn@zurich.ibm.com\n",
      "Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com\n",
      "vector: [-0.033538818359375, 0.0160675048828125, 0.0128936767578125, 0.0043182373046875, 0.0283203125, 0.016510009765625, 0.046234130859375, -0.0121917724609375, -0.00806427001953125, -0.0246429443359375]…\n",
      "\n",
      "=== 35 ===\n",
      "page_no: 7\n",
      "source: 2408.09869v5.pdf\n",
      "content: ABSTRACT\n",
      "Accurate document layout analysis is a key requirement for highquality PDF document conversion. With the recent availability of public, large groundtruth datasets such as PubLayNet and DocBank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXiv only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present DocLayNet , a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide variability in layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments, we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10%\n",
      "vector: [-0.0357666015625, 0.0172119140625, 0.0233001708984375, 0.00038051605224609375, 0.05517578125, 0.00875091552734375, 0.0164642333984375, -0.036895751953125, -0.0256805419921875, 0.0172119140625]…\n",
      "\n",
      "=== 36 ===\n",
      "page_no: 7\n",
      "source: 2408.09869v5.pdf\n",
      "content: ABSTRACT\n",
      "behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DocLayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis.\n",
      "vector: [-0.0367431640625, 0.0131378173828125, 0.020751953125, 0.0101470947265625, 0.03900146484375, 0.0015077590942382812, 0.022552490234375, -0.03924560546875, -0.0293121337890625, 0.0171356201171875]…\n",
      "\n",
      "=== 37 ===\n",
      "page_no: 7\n",
      "source: 2408.09869v5.pdf\n",
      "content: CCS CONCEPTS\n",
      "Æ Information systems → Document structure ; Æ Applied computing → Document analysis ; Æ Computing methodologies → Machine learning ; Computer vision ; Object detection ;\n",
      "Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA ' 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9385-0/22/08. https://doi.org/10.1145/3534678.3539043\n",
      "Figure 1: Four examples of complex page layouts across different document categories\n",
      "vector: [-0.036529541015625, 0.029327392578125, 0.02716064453125, 0.03350830078125, 0.037841796875, 0.022674560546875, 0.04534912109375, 0.0030689239501953125, -0.01090240478515625, 0.004009246826171875]…\n",
      "\n",
      "=== 38 ===\n",
      "page_no: 7\n",
      "source: 2408.09869v5.pdf\n",
      "content: KEYWORDS\n",
      "PDF document conversion, layout segmentation, object-detection, data set, Machine Learning\n",
      "vector: [-0.01727294921875, 0.0245819091796875, 0.024017333984375, 0.0163421630859375, 0.035247802734375, 0.0081024169921875, 0.027862548828125, -0.007282257080078125, -0.0015277862548828125, 0.0167694091796875]…\n",
      "\n",
      "=== 39 ===\n",
      "page_no: 7\n",
      "source: 2408.09869v5.pdf\n",
      "content: ACM Reference Format:\n",
      "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\n",
      "AGL Energy Limited  ABN 74 1\n",
      "5 061 375\n",
      "Figure 1: Four examples of complex page layouts across different document categories\n",
      "vector: [-0.0377197265625, 0.006107330322265625, 0.033294677734375, 0.00992584228515625, 0.0308380126953125, 0.0260772705078125, 0.030181884765625, -0.0107421875, -0.014434814453125, 0.00856781005859375]…\n",
      "\n",
      "=== 40 ===\n",
      "page_no: 7\n",
      "source: 2408.09869v5.pdf\n",
      "content: KEYWORDS\n",
      "PDF document conversion, layout segmentation, object-detection, data set, Machine Learning\n",
      "vector: [-0.01727294921875, 0.0245819091796875, 0.024017333984375, 0.0163421630859375, 0.035247802734375, 0.0081024169921875, 0.027862548828125, -0.007282257080078125, -0.0015277862548828125, 0.0167694091796875]…\n",
      "\n",
      "=== 41 ===\n",
      "page_no: 7\n",
      "source: 2408.09869v5.pdf\n",
      "content: ACMReference Format:\n",
      "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar. 2022. DocLayNet: A Large Human-Annotated Dataset for DocumentLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '22), August 14-18, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ 3534678.3539043\n",
      "1 INTRODUCTION\n",
      "vector: [-0.0372314453125, -0.00243377685546875, 0.0227203369140625, 0.00027489662170410156, 0.026641845703125, 0.0183563232421875, 0.038787841796875, -0.018707275390625, -0.007904052734375, 0.0011053085327148438]…\n",
      "\n",
      "=== 42 ===\n",
      "page_no: 7\n",
      "source: 2408.09869v5.pdf\n",
      "content: ACMReference Format:\n",
      "Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).\n",
      "vector: [-0.0272369384765625, 0.0268707275390625, 0.0204315185546875, 0.01084136962890625, 0.048370361328125, 0.019439697265625, 0.03277587890625, -0.009765625, -0.0177459716796875, 0.0084228515625]…\n",
      "\n",
      "=== 43 ===\n",
      "page_no: 8\n",
      "source: 2408.09869v5.pdf\n",
      "content: ACMReference Format:\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "Table 2: Prediction performance (mAP@0.5-0.95) of object detection networks on DocLayNet test set. The MRCNN (Mask R-CNN) and FRCNN (Faster R-CNN) models with ResNet-50 or ResNet-101 backbone were trained based on the network architectures from the detectron2 model zoo (Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN 3x), with default configurations. The YOLO implementation utilized was YOLOv5x6 [13]. All models were initialised using pre-trained weights from the COCO 2017 dataset.\n",
      "vector: [-0.038848876953125, -1.0848045349121094e-05, 0.02801513671875, 0.0155792236328125, 0.04931640625, 0.0227813720703125, 0.026824951171875, -0.03985595703125, -0.0081329345703125, 0.007099151611328125]…\n",
      "\n",
      "=== 44 ===\n",
      "page_no: 8\n",
      "source: 2408.09869v5.pdf\n",
      "content: ACMReference Format:\n",
      "Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All, human = 84-89 83-91 83-85 87-88 93-94 85-89 69-71 83-84 77-81 84-86 60-72 82-83. Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All, MRCNN R50 R101 = 68.4 71.5 70.9 71.8 60.1 63.4 81.2 80.8 61.6 59.3 71.9 70.0 71.7 72.7 67.6 69.3 82.2 82.9 84.6 85.8 76.7 80.4 72.4 73.5. Caption Footnote Formula List-item Page-footer Page-header Picture Section-header Table Text Title All, FRCNN R101 = 70.1 73.7 63.5 81.0 58.9 72.0 72.0 68.4 82.2 85.4 79.9 73.4. Caption Footnote Formula List-item Page-footer\n",
      "vector: [-0.03887939453125, 0.0224609375, 0.018768310546875, 0.020904541015625, 0.04180908203125, 0.0169219970703125, 0.0078277587890625, -0.0285797119140625, -0.005615234375, 0.00860595703125]…\n",
      "\n",
      "=== 45 ===\n",
      "page_no: 8\n",
      "source: 2408.09869v5.pdf\n",
      "content: ACMReference Format:\n",
      "Page-header Picture Section-header Table Text Title All, YOLO v5x6 = 77.7 77.2 66.2 86.2 61.1 67.9 77.1 74.6 86.3 88.1 82.7 76.8\n",
      "vector: [-0.0260467529296875, 0.0199737548828125, 0.031463623046875, -0.0005698204040527344, 0.060211181640625, 0.03472900390625, 0.0276031494140625, 0.001338958740234375, 0.0013866424560546875, -0.0012359619140625]…\n",
      "\n",
      "=== 46 ===\n",
      "page_no: 8\n",
      "source: 2408.09869v5.pdf\n",
      "content: ACMReference Format:\n",
      "to avoid this at any cost in order to have clear, unbiased baseline numbers for human document-layout annotation. Third, we introduced the feature of snapping boxes around text segments to obtain a pixel-accurate annotation and again reduce time and effort. The CCS annotation tool automatically shrinks every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, which excludes only Table and Picture . For the latter, we instructed annotation staff to minimise inclusion of surrounding whitespace while including all graphical lines. A downside of snapping boxes to enclosed text cells is that some wrongly parsed PDF pages cannot be annotated correctly and need to be skipped. Fourth, we established a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved. Example cases for this would be PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles. Such rejected pages are not contained in the final dataset. With all these measures in place, experienced annotation staff managed to annotate a single page in a typical timeframe of 20s to 60s,\n",
      "vector: [-0.03326416015625, 0.027557373046875, 0.0229339599609375, 0.007232666015625, 0.0430908203125, 0.00022292137145996094, 0.0291290283203125, -0.017608642578125, -0.01416778564453125, 0.0067901611328125]…\n",
      "\n",
      "=== 47 ===\n",
      "page_no: 8\n",
      "source: 2408.09869v5.pdf\n",
      "content: ACMReference Format:\n",
      "depending on its complexity.\n",
      "vector: [-0.044586181640625, 0.009429931640625, 0.022796630859375, -0.024139404296875, 0.0210418701171875, 0.01374053955078125, 0.03619384765625, -0.005573272705078125, -0.0165863037109375, -0.01123046875]…\n",
      "\n",
      "=== 48 ===\n",
      "page_no: 8\n",
      "source: 2408.09869v5.pdf\n",
      "content: 5 EXPERIMENTS\n",
      "The primary goal of DocLayNet is to obtain high-quality ML models capable of accurate document-layout analysis on a wide variety of challenging layouts. As discussed in Section 2, object detection models are currently the easiest to use, due to the standardisation of ground-truth data in COCO format [16] and the availability of general frameworks such as detectron2 [17]. Furthermore, baseline numbers in PubLayNet and DocBank were obtained using standard object detection models such as Mask R-CNN and Faster R-CNN. As such, we will relate to these object detection methods in this\n",
      "Third, Ccs label qu Oolines achienec Exanole\n",
      "vector: [-0.037567138671875, 0.0218048095703125, 0.0286865234375, 0.0028514862060546875, 0.05401611328125, -0.002452850341796875, 0.036895751953125, -0.02130126953125, -0.0177764892578125, 0.0027675628662109375]…\n",
      "\n",
      "=== 49 ===\n",
      "page_no: 8\n",
      "source: 2408.09869v5.pdf\n",
      "content: EXPERIMENTS\n",
      "chalenongayouls ground-vuth dawa such WC\n",
      "Figure 5: Prediction performance (mAP@0.5-0.95) of a Mask R-CNNnetworkwithResNet50backbonetrainedonincreasing fractions of the DocLayNet dataset. The learning curve flattens around the 80% mark, indicating that increasing the size of the DocLayNet dataset with similar data will not yield significantly better predictions.\n",
      "paper and leave the detailed evaluation of more recent methods mentioned in Section 2 for future work.\n",
      "In this section, we will present several aspects related to the performance of object detection models on DocLayNet. Similarly as in PubLayNet, we will evaluate the quality of their predictions using mean average precision (mAP) with 10 overlaps that range from 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are computed by leveraging the evaluation code provided by the COCO API [16].\n",
      "vector: [-0.04339599609375, 0.0258026123046875, 0.02777099609375, 0.018157958984375, 0.06768798828125, -0.0099029541015625, 0.031402587890625, -0.043701171875, -0.01035308837890625, 0.0100250244140625]…\n",
      "\n",
      "=== 50 ===\n",
      "page_no: 8\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "In Table 2, we present baseline experiments (given in mAP) on Mask R-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training and evaluation were performed on RGB images with dimensions of 1025 × 1025 pixels. For training, we only used one annotation in case of redundantly annotated pages. As one can observe, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages. This gives a good indication that the DocLayNet dataset poses a worthwhile challenge for the research community to close the gap between human recognition and ML approaches. It is interesting to see that Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text , Table and Picture . This is not entirely surprising, as Text , Table and Picture are abundant and the most visually distinctive in a\n",
      "vector: [-0.0189208984375, 0.034912109375, 0.01505279541015625, 0.01465606689453125, 0.06634521484375, 0.01032257080078125, 0.0235748291015625, -0.0253143310546875, -0.00592803955078125, 0.0294342041015625]…\n",
      "\n",
      "=== 51 ===\n",
      "page_no: 8\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "document.\n",
      "mak enbrel dacuont\n",
      "Figure 3: Page 6 of the DocLayNet paper. If recognized, metadata such as authors are appearing first under the title. Elements recognized as page headers or footers are suppressed in Markdown to deliver uninterrupted content in reading order. Tables are inserted in reading order. The paragraph in '5. Experiments' wrapping over the column end is broken up in two and interrupted by the table.\n",
      "KDD '22, August 14-18, 2022, Washington, DC, USA\n",
      "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\n",
      "Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\n",
      "between pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.\n",
      "of row 'Total') in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric\n",
      "vector: [-0.0271453857421875, 0.02227783203125, 0.0169677734375, 0.0224151611328125, 0.04779052734375, 0.00034332275390625, 0.034210205078125, -0.021392822265625, -0.009185791015625, 0.0305328369140625]…\n",
      "\n",
      "=== 52 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "Caption, Count.Count = 22524. Caption, % of Total.Train = 2.04. Caption, % of Total.Test = 1.77. Caption, % of Total.Val = 2.32. Caption, triple inter-annotator mAP @0.5-0.95 (%).All = 84-89. Caption, triple inter-annotator mAP @0.5-0.95 (%).Fin = 40-61. Caption, triple inter-annotator mAP @0.5-0.95 (%).Man = 86-92. Caption, triple inter-annotator mAP @0.5-0.95 (%).Sci = 94-99. Caption, triple inter-annotator mAP @0.5-0.95 (%).Law = 95-99. Caption, triple inter-annotator mAP @0.5-0.95 (%).Pat = 69-78. Caption, triple inter-annotator mAP @0.5-0.95 (%).Ten = n/a. Footnote, Count.Count\n",
      "vector: [-0.032379150390625, 0.04290771484375, 0.0201873779296875, 0.02008056640625, 0.0576171875, 0.004047393798828125, 0.0265960693359375, -0.03070068359375, -0.0120391845703125, 0.0218658447265625]…\n",
      "\n",
      "=== 53 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "= 6318. Footnote, % of Total.Train = 0.60. Footnote, % of Total.Test = 0.31. Footnote, % of Total.Val = 0.58. Footnote, triple inter-annotator mAP @0.5-0.95 (%).All = 83-91. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Fin = n/a. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Man = 100. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Sci = 62-88. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Law = 85-94. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Pat = n/a. Footnote, triple inter-annotator mAP @0.5-0.95 (%).Ten = 82-97. Formula, Count.Count = 25027. Formula, % of\n",
      "vector: [-0.032012939453125, 0.040283203125, 0.020782470703125, 0.0128326416015625, 0.04833984375, 0.020355224609375, 0.032012939453125, -0.0224761962890625, -0.00879669189453125, 0.031585693359375]…\n",
      "\n",
      "=== 54 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "Total.Train = 2.25. Formula, % of Total.Test = 1.90. Formula, % of Total.Val = 2.96. Formula, triple inter-annotator mAP @0.5-0.95 (%).All = 83-85. Formula, triple inter-annotator mAP @0.5-0.95 (%).Fin = n/a. Formula, triple inter-annotator mAP @0.5-0.95 (%).Man = n/a. Formula, triple inter-annotator mAP @0.5-0.95 (%).Sci = 84-87. Formula, triple inter-annotator mAP @0.5-0.95 (%).Law = 86-96. Formula, triple inter-annotator mAP @0.5-0.95 (%).Pat = n/a. Formula, triple inter-annotator mAP @0.5-0.95 (%).Ten = n/a. List-item, Count.Count = 185660. List-item, % of Total.Train = 17.19. List-item, %\n",
      "vector: [-0.0374755859375, 0.0380859375, 0.016357421875, 0.01348876953125, 0.04779052734375, 0.015472412109375, 0.033721923828125, -0.021820068359375, -0.007785797119140625, 0.02618408203125]…\n",
      "\n",
      "=== 55 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "of Total.Test = 13.34. List-item, % of Total.Val = 15.82. List-item, triple inter-annotator mAP @0.5-0.95 (%).All = 87-88. List-item, triple inter-annotator mAP @0.5-0.95 (%).Fin = 74-83. List-item, triple inter-annotator mAP @0.5-0.95 (%).Man = 90-92. List-item, triple inter-annotator mAP @0.5-0.95 (%).Sci = 97-97. List-item, triple inter-annotator mAP @0.5-0.95 (%).Law = 81-85. List-item, triple inter-annotator mAP @0.5-0.95 (%).Pat = 75-88. List-item, triple inter-annotator mAP @0.5-0.95 (%).Ten = 93-95. Page-footer, Count.Count = 70878. Page-footer, % of Total.Train =\n",
      "vector: [-0.033447265625, 0.037841796875, 0.0211334228515625, 0.0110931396484375, 0.042510986328125, 0.017303466796875, 0.032501220703125, -0.0182342529296875, -0.01410675048828125, 0.0287933349609375]…\n",
      "\n",
      "=== 56 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "6.51. Page-footer, % of Total.Test = 5.58. Page-footer, % of Total.Val = 6.00. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).All = 93-94. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Fin = 88-90. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Man = 95-96. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Sci = 100. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Law = 92-97. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Pat = 100. Page-footer, triple inter-annotator mAP @0.5-0.95 (%).Ten = 96-98. Page-header, Count.Count = 58022.\n",
      "vector: [-0.025970458984375, 0.03961181640625, 0.0251007080078125, 0.0236968994140625, 0.0543212890625, 0.02001953125, 0.0276947021484375, -0.01568603515625, -0.00540924072265625, 0.034423828125]…\n",
      "\n",
      "=== 57 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "Page-header, % of Total.Train = 5.10. Page-header, % of Total.Test = 6.70. Page-header, % of Total.Val = 5.06. Page-header, triple inter-annotator mAP @0.5-0.95 (%).All = 85-89. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Fin = 66-76. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Man = 90-94. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Sci = 98-100. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Law = 91-92. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Pat = 97-99. Page-header, triple inter-annotator mAP @0.5-0.95 (%).Ten = 81-86. Picture, Count.Count =\n",
      "vector: [-0.033172607421875, 0.038238525390625, 0.0267486572265625, 0.0208892822265625, 0.053497314453125, 0.00962066650390625, 0.03802490234375, -0.01934814453125, -0.01177215576171875, 0.0227813720703125]…\n",
      "\n",
      "=== 58 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "45976. Picture, % of Total.Train = 4.21. Picture, % of Total.Test = 2.78. Picture, % of Total.Val = 5.31. Picture, triple inter-annotator mAP @0.5-0.95 (%).All = 69-71. Picture, triple inter-annotator mAP @0.5-0.95 (%).Fin = 56-59. Picture, triple inter-annotator mAP @0.5-0.95 (%).Man = 82-86. Picture, triple inter-annotator mAP @0.5-0.95 (%).Sci = 69-82. Picture, triple inter-annotator mAP @0.5-0.95 (%).Law = 80-95. Picture, triple inter-annotator mAP @0.5-0.95 (%).Pat = 66-71. Picture, triple inter-annotator mAP @0.5-0.95 (%).Ten = 59-76. Section-header, Count.Count = 142884. Section-header, % of\n",
      "vector: [-0.02392578125, 0.039642333984375, 0.019195556640625, 0.01158905029296875, 0.045318603515625, 0.0171051025390625, 0.03924560546875, -0.024017333984375, -0.0004818439483642578, 0.018463134765625]…\n",
      "\n",
      "=== 59 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "Total.Train = 12.60. Section-header, % of Total.Test = 15.77. Section-header, % of Total.Val = 12.85. Section-header, triple inter-annotator mAP @0.5-0.95 (%).All = 83-84. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Fin = 76-81. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Man = 90-92. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Sci = 94-95. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Law = 87-94. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Pat = 69-73. Section-header, triple inter-annotator mAP @0.5-0.95 (%).Ten = 78-86. Table, Count.Count = 34733. Table,\n",
      "vector: [-0.0294036865234375, 0.0289764404296875, 0.0207977294921875, 0.007678985595703125, 0.048126220703125, 0.01312255859375, 0.032684326171875, -0.0226593017578125, -0.00925445556640625, 0.027008056640625]…\n",
      "\n",
      "=== 60 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "% of Total.Train = 3.20. Table, % of Total.Test = 2.27. Table, % of Total.Val = 3.60. Table, triple inter-annotator mAP @0.5-0.95 (%).All = 77-81. Table, triple inter-annotator mAP @0.5-0.95 (%).Fin = 75-80. Table, triple inter-annotator mAP @0.5-0.95 (%).Man = 83-86. Table, triple inter-annotator mAP @0.5-0.95 (%).Sci = 98-99. Table, triple inter-annotator mAP @0.5-0.95 (%).Law = 58-80. Table, triple inter-annotator mAP @0.5-0.95 (%).Pat = 79-84. Table, triple inter-annotator mAP @0.5-0.95 (%).Ten = 70-85. Text, Count.Count = 510377. Text, % of Total.Train = 45.82. Text, % of\n",
      "vector: [-0.025390625, 0.04193115234375, 0.0216522216796875, 0.01042938232421875, 0.04852294921875, 0.00963592529296875, 0.0275421142578125, -0.032867431640625, -0.00838470458984375, 0.021759033203125]…\n",
      "\n",
      "=== 61 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "Total.Test = 49.28. Text, % of Total.Val = 45.00. Text, triple inter-annotator mAP @0.5-0.95 (%).All = 84-86. Text, triple inter-annotator mAP @0.5-0.95 (%).Fin = 81-86. Text, triple inter-annotator mAP @0.5-0.95 (%).Man = 88-93. Text, triple inter-annotator mAP @0.5-0.95 (%).Sci = 89-93. Text, triple inter-annotator mAP @0.5-0.95 (%).Law = 87-92. Text, triple inter-annotator mAP @0.5-0.95 (%).Pat = 71-79. Text, triple inter-annotator mAP @0.5-0.95 (%).Ten = 87-95. Title, Count.Count = 5071. Title, % of Total.Train = 0.47. Title, % of Total.Test = 0.30. Title, % of Total.Val =\n",
      "vector: [-0.0282135009765625, 0.048004150390625, 0.016845703125, 0.01390838623046875, 0.048980712890625, 0.015380859375, 0.03546142578125, -0.0286102294921875, -0.006561279296875, 0.02703857421875]…\n",
      "\n",
      "=== 62 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "0.50. Title, triple inter-annotator mAP @0.5-0.95 (%).All = 60-72. Title, triple inter-annotator mAP @0.5-0.95 (%).Fin = 24-63. Title, triple inter-annotator mAP @0.5-0.95 (%).Man = 50-63. Title, triple inter-annotator mAP @0.5-0.95 (%).Sci = 94-100. Title, triple inter-annotator mAP @0.5-0.95 (%).Law = 82-96. Title, triple inter-annotator mAP @0.5-0.95 (%).Pat = 68-79. Title, triple inter-annotator mAP @0.5-0.95 (%).Ten = 24-56. Total, Count.Count = 1107470. Total, % of Total.Train = 941123. Total, % of Total.Test = 99816. Total, % of Total.Val = 66531. Total, triple inter-annotator mAP\n",
      "vector: [-0.0294189453125, 0.042388916015625, 0.0216064453125, 0.00568389892578125, 0.045684814453125, 0.01116180419921875, 0.035400390625, -0.028594970703125, -0.005039215087890625, 0.0300445556640625]…\n",
      "\n",
      "=== 63 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "@0.5-0.95 (%).All = 82-83. Total, triple inter-annotator mAP @0.5-0.95 (%).Fin = 71-74. Total, triple inter-annotator mAP @0.5-0.95 (%).Man = 79-81. Total, triple inter-annotator mAP @0.5-0.95 (%).Sci = 89-94. Total, triple inter-annotator mAP @0.5-0.95 (%).Law = 86-91. Total, triple inter-annotator mAP @0.5-0.95 (%).Pat = 71-76. Total, triple inter-annotator mAP @0.5-0.95 (%).Ten = 68-85\n",
      "include publication repositories such as arXiv\n",
      "vector: [-0.0282745361328125, 0.050506591796875, 0.0255889892578125, 0.0061798095703125, 0.033477783203125, 0.0129547119140625, 0.029571533203125, -0.0225677490234375, -0.0045623779296875, 0.02655029296875]…\n",
      "\n",
      "=== 64 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "Table 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as % of row \"Total\") in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-\n",
      "annotated pages, from which we obtain accuracy ranges.\n",
      "vector: [-0.0220947265625, 0.0238494873046875, 0.0177764892578125, 0.006061553955078125, 0.032470703125, 0.0084228515625, 0.0281524658203125, -0.027618408203125, -0.0029468536376953125, 0.03448486328125]…\n",
      "\n",
      "=== 65 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "class label,  = Count. class label, %of Total = Train. class label, %of Total = Test. class label, %of Total = Val. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = All. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Fin. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Man. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Sci. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Law. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Pat. class label, triple inter- annotator mAP @ 0.5-0.95 (%) = Ten. Caption,\n",
      "vector: [-0.02191162109375, 0.039794921875, 0.020294189453125, 0.014373779296875, 0.047821044921875, 0.01377105712890625, 0.031158447265625, -0.03436279296875, -0.01085662841796875, 0.026123046875]…\n",
      "\n",
      "=== 66 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "= 22524. Caption, %of Total = 2.04. Caption, %of Total = 1.77. Caption, %of Total = 2.32. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 84-89. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 40-61. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 86-92. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 94-99. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 95-99. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = 69-78. Caption, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Footnote,\n",
      "vector: [-0.027984619140625, 0.046051025390625, 0.0210723876953125, 0.0277557373046875, 0.05596923828125, 0.006450653076171875, 0.0203857421875, -0.03778076171875, -0.016357421875, 0.0185394287109375]…\n",
      "\n",
      "=== 67 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "= 6318. Footnote, %of Total = 0.60. Footnote, %of Total = 0.31. Footnote, %of Total = 0.58. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 83-91. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 100. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 62-88. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 85-94. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Footnote, triple inter- annotator mAP @ 0.5-0.95 (%) = 82-97. Formula,\n",
      "vector: [-0.033294677734375, 0.0377197265625, 0.01885986328125, 0.015350341796875, 0.047821044921875, 0.01629638671875, 0.0254364013671875, -0.0235595703125, -0.0069427490234375, 0.0304718017578125]…\n",
      "\n",
      "=== 68 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "= 25027. Formula, %of Total = 2.25. Formula, %of Total = 1.90. Formula, %of Total = 2.96. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = 83-85. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = 84-87. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = 86-96. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. Formula, triple inter- annotator mAP @ 0.5-0.95 (%) = n/a. List-item,\n",
      "vector: [-0.03515625, 0.042572021484375, 0.027099609375, 0.013885498046875, 0.04681396484375, 0.016021728515625, 0.030914306640625, -0.0188140869140625, -0.00963592529296875, 0.0276641845703125]…\n",
      "\n",
      "=== 69 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "= 185660. List-item, %of Total = 17.19. List-item, %of Total = 13.34. List-item, %of Total = 15.82. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 87-88. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 74-83. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 90-92. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 97-97. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 81-85. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 75-88. List-item, triple inter- annotator mAP @ 0.5-0.95 (%) = 93-95. Page- footer,\n",
      "vector: [-0.031341552734375, 0.03857421875, 0.02001953125, 0.01200103759765625, 0.041717529296875, 0.0133209228515625, 0.0284576416015625, -0.0180816650390625, -0.00958251953125, 0.02447509765625]…\n",
      "\n",
      "=== 70 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "= 70878. Page- footer, %of Total = 6.51. Page- footer, %of Total = 5.58. Page- footer, %of Total = 6.00. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 93-94. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 88-90. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 95-96. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 100. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 92-97. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 100. Page- footer, triple inter- annotator mAP @ 0.5-0.95 (%) = 96-98. Page- header offices,,\n",
      "vector: [-0.0343017578125, 0.037689208984375, 0.0228424072265625, 0.02294921875, 0.049774169921875, 0.0115966796875, 0.02886962890625, -0.020782470703125, -0.01099395751953125, 0.02935791015625]…\n",
      "\n",
      "=== 71 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "= 58022. Page- header offices,, %of Total = 5.10. Page- header offices,, %of Total = 6.70. Page- header offices,, %of Total = 5.06. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 85-89. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 66-76. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 90-94. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 98-100. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 91-92. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 97-99. Page- header offices,, triple inter- annotator mAP @ 0.5-0.95 (%) = 81-86. Picture,\n",
      "vector: [-0.033355712890625, 0.0399169921875, 0.02325439453125, 0.01444244384765625, 0.049774169921875, 0.005840301513671875, 0.03826904296875, -0.0197296142578125, -0.00798797607421875, 0.029937744140625]…\n",
      "\n",
      "=== 72 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "= 45976. Picture, %of Total = 4.21. Picture, %of Total = 2.78. Picture, %of Total = 5.31. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 69-71. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 56-59. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 82-86. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 69-82. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 80-95. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 66-71. Picture, triple inter- annotator mAP @ 0.5-0.95 (%) = 59-76. Section- header not,\n",
      "vector: [-0.0284423828125, 0.041717529296875, 0.0186004638671875, 0.0187225341796875, 0.043365478515625, 0.01480865478515625, 0.032470703125, -0.0240478515625, -0.0035400390625, 0.018951416015625]…\n",
      "\n",
      "=== 73 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "= 142884. Section- header not, %of Total = 12.60. Section- header not, %of Total = 15.77. Section- header not, %of Total = 12.85. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 83-84. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 76-81. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 90-92. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 94-95. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 87-94. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 69-73. Section- header not, triple inter- annotator mAP @ 0.5-0.95 (%) = 78-86. Table,\n",
      "vector: [-0.032989501953125, 0.032989501953125, 0.024810791015625, 0.015655517578125, 0.046478271484375, 0.00626373291015625, 0.0299835205078125, -0.024932861328125, -0.0055084228515625, 0.0276947021484375]…\n",
      "\n",
      "=== 74 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "= 34733. Table, %of Total = 3.20. Table, %of Total = 2.27. Table, %of Total = 3.60. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 77-81. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 75-80. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 83-86. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 98-99. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 58-80. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 79-84. Table, triple inter- annotator mAP @ 0.5-0.95 (%) = 70-85. Text,\n",
      "vector: [-0.0260772705078125, 0.0430908203125, 0.0181121826171875, 0.012603759765625, 0.0426025390625, 0.011688232421875, 0.02081298828125, -0.0274200439453125, -0.00315093994140625, 0.0268096923828125]…\n",
      "\n",
      "=== 75 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "= 510377. Text, %of Total = 45.82. Text, %of Total = 49.28. Text, %of Total = 45.00. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 84-86. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 81-86. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 88-93. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 89-93. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 87-92. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 71-79. Text, triple inter- annotator mAP @ 0.5-0.95 (%) = 87-95. Title [22], a,\n",
      "vector: [-0.0240631103515625, 0.04833984375, 0.01715087890625, 0.0193328857421875, 0.054779052734375, 0.002590179443359375, 0.0322265625, -0.032470703125, -0.006214141845703125, 0.0208282470703125]…\n",
      "\n",
      "=== 76 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "= 5071. Title [22], a, %of Total = 0.47. Title [22], a, %of Total = 0.30. Title [22], a, %of Total = 0.50. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 60-72. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 24-63. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 50-63. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 94-100. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 82-96. Title [22], a, triple inter- annotator mAP @ 0.5-0.95 (%) = 68-79. Title [22], a, triple inter- annotator mAP @\n",
      "vector: [-0.0386962890625, 0.0452880859375, 0.0170745849609375, 0.017303466796875, 0.04779052734375, 0.007167816162109375, 0.028106689453125, -0.02435302734375, -0.00682830810546875, 0.0295867919921875]…\n",
      "\n",
      "=== 77 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "0.5-0.95 (%) = 24-56. Total in-,\n",
      "= 1107470. Total in-, %of Total = 941123. Total in-, %of Total = 99816. Total in-, %of Total = 66531. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 82-83. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 71-74. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 79-81. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 89-94. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 86-91. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 71-76. Total in-, triple inter- annotator mAP @ 0.5-0.95 (%) = 68-85\n",
      "3\n",
      ",\n",
      "vector: [-0.03033447265625, 0.034698486328125, 0.020843505859375, 0.007476806640625, 0.048675537109375, 0.00960540771484375, 0.0281524658203125, -0.028594970703125, -0.00070953369140625, 0.028594970703125]…\n",
      "\n",
      "=== 78 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "government offices,\n",
      "We reviewed the col-\n",
      ",\n",
      "Page-\n",
      "Title and\n",
      ".\n",
      "page. Specificity ensures that the choice of label is not ambiguous,\n",
      "we distributed the annotation workload and performed continuous be annotated. We refrained from class labels that are very specific\n",
      "only. For phases three and four, a group of 40 dedicated annotators while coverage ensures that all meaningful items on a page can\n",
      "quality controls. Phase one and two required a small team of experts to a document category, such as\n",
      "Abstract in the\n",
      "Scientific Articles were assembled and supervised.\n",
      "category. We also avoided class labels that are tightly linked to the\n",
      "Phase 1: Data selection and preparation.\n",
      "Our inclusion cri-\n",
      "Author\n",
      "Affiliation\n",
      "vector: [-0.0249176025390625, 0.045440673828125, 0.035003662109375, 0.00698089599609375, 0.055511474609375, -4.9591064453125e-05, 0.03753662109375, -0.022552490234375, -0.020355224609375, 0.038482666015625]…\n",
      "\n",
      "=== 79 ===\n",
      "page_no: 9\n",
      "source: 2408.09869v5.pdf\n",
      "content: Baselines for Object Detection\n",
      "teria for documents were described in Section 3. A large effort went into ensuring that all documents are free to use. The data sources in DocBank, are often only distinguishable by discriminating on 3 https://arxiv.org/ Figure 4: Table 1 from the DocLayNet paper in the original PDF (A), as rendered Markdown (B) and in JSON representation (C). Spanning table cells, such as the multi-column header 'triple interannotator mAP@0.5-0.95 (%)', is repeated for each column in the Markdown representation (B), which guarantees that every data point can be traced back to row and column headings only by its grid coordinates in the table. In the JSON representation, the span information is reflected in the fields of each table cell (C).\n",
      "semantics of the text. Labels such as and\n",
      ",\n",
      "as seen\n",
      "vector: [-0.0123748779296875, 0.031829833984375, 0.024749755859375, 0.018798828125, 0.030517578125, 0.0075836181640625, 0.029022216796875, -0.039642333984375, 0.0034885406494140625, 0.03070068359375]…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(payload):\n",
    "    print(f\"=== {i} ===\")\n",
    "    print(f\"page_no: {item.page_no}\")\n",
    "    print(f\"source: {item.source}\")\n",
    "    print(f\"content: {item.content}\")\n",
    "    print(f\"vector: {item.vector[:10]}…\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import client library\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "qdrant_client = QdrantClient(\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd222f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not qdrant_client.collection_exists(\"test_collection\"):\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=\"test_collection\",\n",
    "        vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n",
    "    )ó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7ee4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "def upload_entities(entities: list[VectorDbEntity], collection_name: str):\n",
    "    points = []\n",
    "    for entity in entities:\n",
    "        point_id = str(uuid.uuid4())  # Generate a random UUID as a string\n",
    "\n",
    "        points.append(PointStruct(\n",
    "            id=point_id,\n",
    "            vector=entity.vector,\n",
    "            payload={\n",
    "                \"page_no\": entity.page_no,\n",
    "                \"source\": entity.source,\n",
    "                \"content\": entity.content,\n",
    "            }\n",
    "        ))\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=points\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bdded658",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_entities(payload, \"test_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe59735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(text_to_embed: str):\n",
    "    response = mistral_client.embeddings.create(\n",
    "        model=model,\n",
    "        inputs=[text_to_embed],\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08079fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serach_vector_db(query: str, collection_name: str):\n",
    "    query_vector = create_embedding(query)\n",
    "    search_results = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        with_payload=True,\n",
    "        limit=5,\n",
    "    ).points\n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2322a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_search_results = serach_vector_db(\"How to create markdown documents?\", \"test_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0659ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780d9fe5-3b4d-4117-ab9c-fd87b71596ae\n",
      "8ccaea95-45da-4361-8aec-5292d89d0197\n",
      "e5ed761d-6b37-411d-a4c4-b916d0f9a269\n",
      "387b4f9b-a4aa-4ae0-abd7-40e54719e93a\n",
      "c43d13ce-a893-450a-8dfc-834f133cf91d\n"
     ]
    }
   ],
   "source": [
    "for result in vector_search_results:\n",
    "    print(result.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6119003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypothetical_document(query, desired_length=1000):\n",
    "    \"\"\"\n",
    "    Generate a hypothetical document that answers the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        desired_length (int): Target length of the hypothetical document\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated hypothetical document\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the model on how to generate the document\n",
    "    system_prompt = f\"\"\"You are an expert document creator. \n",
    "    Given a question, generate a detailed document that would directly answer this question.\n",
    "    The document should be approximately {desired_length} characters long and provide an in-depth, \n",
    "    informative answer to the question. Write as if this document is from an authoritative source\n",
    "    on the subject. Include specific details, facts, and explanations.\n",
    "    Do not mention that this is a hypothetical document - just write the content directly.\"\"\"\n",
    "\n",
    "    # Define the user prompt with the query\n",
    "    user_prompt = f\"Question: {query}\\n\\nGenerate a document that fully answers this question:\"\n",
    "    \n",
    "    # Make a request to the MistralAI API to generate the hypothetical document\n",
    "    response = mistral_client.chat.complete(\n",
    "        model=\"mistral-small-latest\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with the query\n",
    "        ],\n",
    "        temperature=0.1  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated document content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "28846398",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_doc = generate_hypothetical_document(\"How to create markdown documents?\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "896f6ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Creating Markdown Documents**\n",
      "\n",
      "Markdown is a lightweight markup language designed for easy formatting of text using a plain-text editor. To create a Markdown document, follow these steps and conventions:\n",
      "\n",
      "1. **File Extension**: Save your file with a `.md` or `.markdown` extension, e.g., `document.md`.\n",
      "\n",
      "2. **Headings**: Use the `#` symbol to create headings. The number of `#` symbols denotes the heading level (e.g., `#` for `<h1>`, `##` for `<h2>`, etc.).\n",
      "\n",
      "3. **Emphasis**: Use `*` or `_` for italics (e.g., `*italic*` or `_italic_`) and `**` or `__` for bold (e.g., `**bold**` or `__bold__`).\n",
      "\n",
      "4. **Lists**: Create unordered lists using `*` or `-` (e.g., `* Item 1`, `- Item 2`). For ordered lists, use numbers followed by a period (e.g., `1. First item`, `2. Second item`).\n",
      "\n",
      "5. **Links**: Create hyperlinks using `[link text](URL)` (e.g., `[Google](https://www.google.com)`).\n",
      "\n",
      "6. **Images**: Insert images using `![alt text](image URL)` (e.g., `![Logo](https://example.com/logo.png)`).\n",
      "\n",
      "7. **Blockquotes**: Use `>` to create blockquotes (e.g., `> This is a blockquote`).\n",
      "\n",
      "8. **Code**: Use backticks (`` ` ``) for inline code (e.g., `` `code` ``) and triple backticks (`` ``` ``) for code blocks (e.g., `` ```code``` ``).\n",
      "\n",
      "9. **Horizontal Rules**: Use three or more `*`, `-`, or `_` on a line by themselves to create a horizontal rule.\n",
      "\n",
      "10. **Tables**: Create tables using pipes (`|`) and hyphens (`-`) for headers (e.g., `| Header 1 | Header 2 |`, `|----------|----------|`).\n",
      "\n",
      "11. **Escaping Characters**: Use a backslash (`\\`) to escape characters that have special meanings in Markdown (e.g., `\\*` for an asterisk).\n",
      "\n",
      "To view your Markdown document with proper formatting, use a Markdown viewer or editor, such as Visual Studio Code, Typora, or online tools like Dillinger or StackEdit. These tools provide real-time rendering and additional features to enhance your Markdown experience.\n"
     ]
    }
   ],
   "source": [
    "print(hyde_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "353a5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_search_results = serach_vector_db(hyde_doc, \"test_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5c3bf412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload: Appendix\n",
      "In this section, we illustrate a few examples of Docling's output in Markdown and JSON.\n",
      "\n",
      "Payload: 2 Getting Started\n",
      "```\n",
      "source = \"https://arxiv.org/pdf/2206.01062\" # PDF path or URL converter = DocumentConverter() result = converter.convert_single(source) print(result.render_as_markdown()) # output: \"## DocLayNet: A Human -Annotated Dataset for Document -Layout Analysis [...]\"\n",
      "```\n",
      "Optionally, you can configure custom pipeline features and runtime options, such as turning on or off features (e.g. OCR, table structure recognition), enforcing limits on the input document size, and defining the budget of CPU threads. Advanced usage examples and options are documented in the README file. Docling also provides a Dockerfile to demonstrate how to install and run it inside a container.\n",
      "\n",
      "Payload: 2 Getting Started\n",
      "To use Docling, you can simply install the docling package from PyPI. Documentation and examples are available in our GitHub repository at github.com/DS4SD/docling. All required model assets 1 are downloaded to a local huggingface datasets cache on first use, unless you choose to pre-install the model assets in advance.\n",
      "Docling provides an easy code interface to convert PDF documents from file system, URLs or binary streams, and retrieve the output in either JSON or Markdown format. For convenience, separate methods are offered to convert single documents or batches of documents. A basic usage example is illustrated below. Further examples are available in the Doclign code repository.\n",
      "```\n",
      "from docling.document_converter import DocumentConverter Large\n",
      "```\n",
      "\n",
      "Payload: 1 Introduction\n",
      "· Converts PDF documents to JSON or Markdown format, stable and lightning fast\n",
      "· Understands detailed page layout, reading order, locates figures and recovers table structures\n",
      "· Extracts metadata from the document, such as title, authors, references and language\n",
      "· Optionally applies OCR, e.g. for scanned PDFs\n",
      "· Can be configured to be optimal for batch-mode (i.e high throughput, low time-to-solution) or interactive mode (compromise on efficiency, low time-to-solution)\n",
      "· Can leverage different accelerators (GPU, MPS, etc).\n",
      "\n",
      "Payload: ACMReference Format:\n",
      "Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1-4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1. Figure 2: Title page of the DocLayNet paper (arxiv.org/pdf/2206.01062) - left PDF, right rendered Markdown. If recognized, metadata such as authors are appearing first under the title. Text content inside figures is currently dropped, the caption is retained and linked to the figure in the JSON representation (not shown).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in vector_search_results:\n",
    "    print(f\"Payload: {result.payload['content']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e055a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RetrieverResults:\n",
    "    source: str\n",
    "    content: str\n",
    "    score: float\n",
    "    page_no: int"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
